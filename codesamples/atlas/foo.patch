commit 74117a82a90a5fd7f7142e2e87bfc8a2b3f42dc0
Author: Shivaji Dutta <dutta_shivaji@yahoo.co.uk>
Date:   Wed Aug 5 16:44:46 2015 -0700

    added support for importmysql

diff --git a/codesamples/atlas/TestHierarchy.json b/codesamples/atlas/TestHierarchy.json
index 99a5324..c3b0d67 100644
--- a/codesamples/atlas/TestHierarchy.json
+++ b/codesamples/atlas/TestHierarchy.json
@@ -3,23 +3,14 @@
   "Products": {
     "Vehicles": {
     	"Entities1":[
-      {"type": "hive_table", "name": "hortontimesheet"},
-     {"type": "hive_table", "name": "hortondrivers"}
+      {"type": "Table", "name": "DRIVERS"},
+     {"type": "Table", "name": "TIMESHEET"},
+     {"type": "hive_table", "name": "test.drivers@atlasdemo"}
     ]
   },
   "Parts": "done",
-  "TruckParts": {
-  	"Entities2": [
-  	 {"type": "hive_table", "name": "hortondrivers25"},
-  	 {"type": "hive_table", "name": "hortondrivers26"}
-  	]
-
-  },
-  "AutoParts": {
-"Entities3": [
-  	 {"type": "hive_table", "name": "hortondrivers23"},
-  	 {"type": "hive_table", "name": "hortondrivers24"}
-  	]
+  "TruckParts": "NONE",
+  "AutoParts": "NONE"
   }
 }
 
diff --git a/codesamples/atlas/pom.xml b/codesamples/atlas/pom.xml
index a39aa75..33138cf 100644
--- a/codesamples/atlas/pom.xml
+++ b/codesamples/atlas/pom.xml
@@ -263,7 +263,7 @@
           </descriptorRefs>
           <archive>
             <manifest>
-              <mainClass>com.atlas.cli.AtlasCLI</mainClass>
+              <mainClass>com.hortonworks.atlas.cli.AtlasCLI</mainClass>
             </manifest>
           </archive>
         </configuration>
diff --git a/codesamples/atlas/src/main/java/com/atlas/cli/AtlasCLI.java b/codesamples/atlas/src/main/java/com/atlas/cli/AtlasCLI.java
deleted file mode 100644
index ab96616..0000000
--- a/codesamples/atlas/src/main/java/com/atlas/cli/AtlasCLI.java
+++ /dev/null
@@ -1,701 +0,0 @@
-package com.atlas.cli;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.ListIterator;
-
-import org.apache.atlas.AtlasServiceException;
-import org.apache.atlas.typesystem.Referenceable;
-import org.apache.atlas.typesystem.Struct;
-import org.apache.atlas.typesystem.json.InstanceSerialization;
-import org.apache.atlas.typesystem.persistence.Id;
-import org.apache.commons.cli.CommandLine;
-import org.apache.commons.cli.CommandLineParser;
-import org.apache.commons.cli.GnuParser;
-import org.apache.commons.cli.HelpFormatter;
-import org.apache.commons.cli.OptionBuilder;
-import org.apache.commons.cli.Options;
-import org.apache.commons.cli.ParseException;
-import org.apache.commons.cli.PosixParser;
-import org.codehaus.jackson.JsonParseException;
-import org.codehaus.jettison.json.JSONException;
-import org.codehaus.jettison.json.JSONObject;
-import org.apache.atlas.AtlasClient;
-
-import com.atlas.client.AtlasEntityConnector;
-import com.atlas.client.AtlasEntityCreator;
-import com.atlas.client.AtlasEntitySearch;
-import com.atlas.client.AtlasTypeDefCreator;
-import com.atlas.client.JsonHierarchy;
-import com.atlas.client.NewAtlasClient;
-import com.atlas.client.Taxonomy;
-import com.google.common.collect.ImmutableList;
-import com.hortonworks.atlas.adapter.AtlasTableInterface;
-import com.hortonworks.atlas.adapter.EntityModel;
-import com.hortonworks.atlas.adapter.TupleModel;
-
-/**
- * 
- * @author sdutta
- *
- */
-public class AtlasCLI {
-
-	String baseurl;
-	Options opt = null;
-	AtlasClient aClient = null;
-	String action = null;
-
-	{
-		System.setProperty("atlas.conf", "conf");
-	}
-
-	/**
-	 * 
-	 * @param args
-	 */
-	public static void main(String[] args) {
-
-		try {
-			@SuppressWarnings("unused")
-			AtlasCLI cli = new AtlasCLI(args);
-		} catch (ParseException e) {
-			e.printStackTrace();
-		}
-
-	}
-
-	/**
-	 * 
-	 * @param args
-	 * @throws ParseException
-	 * @throws Exception
-	 */
-
-	@SuppressWarnings("static-access")
-	AtlasCLI(String args[]) throws ParseException {
-
-		CommandLineParser parser = new GnuParser();
-
-		opt = new Options();
-
-		opt.addOption(OptionBuilder
-				.withLongOpt(AtlasCLIOptions.action)
-				.withDescription(
-						"action you want to perform [search|createSimpleType|createDataSetType|"
-								+ "createProcessType|createSimpleEntity|createDataSetEntity|createProcessEntity"
-								+ "|createtrait|loadtraithierarchy|importmysql]")
-				.hasArg().withArgName("action").create()
-
-		);
-
-		opt.addOption(OptionBuilder.withLongOpt(AtlasCLIOptions.url)
-				.withDescription("Url for the atlas host http://host:21000")
-				.hasArg().withArgName("URL").create()
-
-		);
-
-		opt.addOption(OptionBuilder
-				.withLongOpt(AtlasCLIOptions.type)
-				.withDescription(
-						"String describing the type of the object. You can find by querying the list - http://host:21000/api/atlas/types?type=CLASS")
-				.hasArg().withArgName("type").create());
-
-		opt.addOption(OptionBuilder.withLongOpt(AtlasCLIOptions.name)
-				.withDescription("name of type or entity").hasArg()
-				.withArgName("name").create());
-
-		opt.addOption(OptionBuilder.withLongOpt(AtlasCLIOptions.description)
-				.withDescription("description of type or entity").hasArg()
-				.withArgName("name").create());
-
-		opt.addOption(OptionBuilder.withLongOpt(AtlasCLIOptions.inp_type)
-				.withDescription("name of type for input to a lineage")
-				.hasArg().withArgName("inp_type").create()
-
-		);
-
-		opt.addOption(OptionBuilder.withLongOpt(AtlasCLIOptions.inp_value)
-				.withDescription("value for input to a lineage").hasArg()
-				.withArgName("inp_value").create());
-
-		opt.addOption(OptionBuilder.withLongOpt(AtlasCLIOptions.out_type)
-				.withDescription("name of output to a lineage").hasArg()
-				.withArgName("out_type").create());
-
-		opt.addOption(OptionBuilder.withLongOpt(AtlasCLIOptions.out_value)
-				.withDescription("value for output to a lineage").hasArg()
-				.withArgName("out_value").create());
-
-		opt.addOption(OptionBuilder.withLongOpt(AtlasCLIOptions.traitTypename)
-				.withDescription("value for trait type").hasArg()
-				.withArgName(AtlasCLIOptions.traitTypename).create());
-
-		opt.addOption(OptionBuilder.withLongOpt(AtlasCLIOptions.traitnames)
-				.withDescription("name of the trait").hasArg()
-				.withArgName(AtlasCLIOptions.traitnames).create());
-
-		opt.addOption(OptionBuilder
-				.withLongOpt(AtlasCLIOptions.parentTraitName)
-				.withDescription("value of parent trait ").hasArg()
-				.withArgName(AtlasCLIOptions.parentTraitName).create());
-
-		opt.addOption(OptionBuilder.withLongOpt(AtlasCLIOptions.supertype)
-				.withDescription("Super type").hasArg()
-				.withArgName(AtlasCLIOptions.supertype).create());
-
-		opt.addOption(OptionBuilder.withLongOpt(AtlasCLIOptions.username)
-				.withDescription("mysql username").hasArg()
-				.withArgName(AtlasCLIOptions.username).create());
-		opt.addOption(OptionBuilder.withLongOpt(AtlasCLIOptions.password)
-				.withDescription("mysql password").hasArg()
-				.withArgName(AtlasCLIOptions.password).create());
-
-		opt.addOption(OptionBuilder
-				.withLongOpt(AtlasCLIOptions.mysqlhost)
-				.withDescription(
-						"mysql host. It assumes mysql is running on port 3306")
-				.hasArg().withArgName(AtlasCLIOptions.mysqlhost).create());
-
-		opt.addOption(OptionBuilder.withLongOpt(AtlasCLIOptions.dbname)
-				.withDescription("mysql db").hasArg()
-				.withArgName(AtlasCLIOptions.dbname).create());
-
-		opt.addOption(OptionBuilder.withLongOpt(AtlasCLIOptions.filepath)
-				.withDescription("json filename. The complete filepath ")
-				.hasArg().withArgName(AtlasCLIOptions.filepath).create());
-
-		opt.addOption(AtlasCLIOptions.listtype, false, "display all types");
-
-		opt.addOption("help", false, "requesting help");
-
-		HelpFormatter formatter = new HelpFormatter();
-
-		CommandLine line = parser.parse(opt, args);
-
-		if (line.hasOption("help") || args.length < 1) {
-			formatter.printHelp("atlasclient", opt);
-		}
-
-		if (line.hasOption(AtlasCLIOptions.url)) {
-			baseurl = line.getOptionValue(AtlasCLIOptions.url);
-
-			this.aClient = new AtlasClient(baseurl);
-
-		} else {
-			System.err.println("url is a mandatory field");
-			formatter.printHelp("atlasclient", opt);
-			System.exit(1);
-		}
-
-		// This will list you list all types
-		if (line.hasOption(AtlasCLIOptions.listtype)) {
-
-			this.listTypes();
-
-		}
-
-		if (line.hasOption(AtlasCLIOptions.action)) {
-
-			this.action = line.getOptionValue(AtlasCLIOptions.action);
-
-			String name = line.getOptionValue(AtlasCLIOptions.name);
-
-			if (AtlasCLIOptions.search.equalsIgnoreCase(this.action)) {
-				searchEntities(line);
-
-			} else if (AtlasCLIOptions.createSimpleType
-					.equalsIgnoreCase(this.action)) {
-
-				this.createSimpleType(line);
-
-			} else if (AtlasCLIOptions.createDataSetType
-					.equalsIgnoreCase(this.action)) {
-
-				this.createDataSetType(line);
-
-			} else if (AtlasCLIOptions.createProcessType
-					.equalsIgnoreCase(this.action)) {
-
-				this.createProcessType(line);
-
-			} else if (AtlasCLIOptions.createSimpleEntity
-					.equalsIgnoreCase(this.action)) {
-
-				this.createSimpleEntity(line);
-
-			} else if (AtlasCLIOptions.createDataSetEntity
-					.equalsIgnoreCase(this.action)) {
-
-				this.createDataSetEntity(line);
-
-			} else if (AtlasCLIOptions.createProcessEntity
-					.equalsIgnoreCase(this.action)) {
-
-				this.createProcessEntity(line);
-
-			} else if (AtlasCLIOptions.createrait.equalsIgnoreCase(this.action)) {
-
-				this.createTraitType(line);
-
-			} else if (AtlasCLIOptions.importMySqlTables
-					.equalsIgnoreCase(this.action)) {
-
-				try {
-					this.importMysqlTables(line);
-				} catch (Exception e) {
-
-					e.printStackTrace();
-				}
-
-			} else if (AtlasCLIOptions.loadtraithierarchy
-					.equalsIgnoreCase(this.action)) {
-				this.loadHierarchy(line);
-
-			}
-
-			else {
-				formatter.printHelp("Usage:", opt);
-			}
-
-		}
-
-	}
-
-	/**
-	 * This method invoked the AtlasEntity Search
-	 * 
-	 * @param line
-	 */
-	private void searchEntities(CommandLine line) {
-
-		try {
-
-			AtlasEntitySearch aES = new AtlasEntitySearch(baseurl);
-			String type_name = line.getOptionValue(AtlasCLIOptions.type);
-			String value = line.getOptionValue(AtlasCLIOptions.name);
-			Referenceable ref = aES.getReferenceByName(type_name, value);
-
-			String entityJSON = InstanceSerialization.toJson(ref, true);
-
-			System.out.println("Search Result:");
-			System.out.println(entityJSON);
-
-		} catch (Exception e) {
-			e.printStackTrace();
-			System.exit(1);
-		}
-
-	}
-
-	/**
-	 * 
-	 * @param line
-	 */
-	private void createSimpleType(CommandLine line) {
-
-		try {
-
-			AtlasTypeDefCreator ad = new AtlasTypeDefCreator(baseurl);
-
-			String classtypename = line.getOptionValue(AtlasCLIOptions.type);
-			String traitname = line.getOptionValue(AtlasCLIOptions.traitnames);
-			String parentype = line.getOptionValue(AtlasCLIOptions.supertype);
-
-			String typeJson = ad.assembleSimpleType(traitname, classtypename,
-					parentype);
-
-			JSONObject createType = this.aClient.createType(typeJson);
-
-			System.out.println(createType.toString());
-
-		} catch (Exception e) {
-			e.printStackTrace();
-		}
-
-	}
-
-	/**
-	 * 
-	 * @param typename
-	 */
-	private void createTraitType(CommandLine line) {
-
-		try {
-
-			String traittype = line
-					.getOptionValue(AtlasCLIOptions.traitTypename);
-			String parenttrait = line
-					.getOptionValue(AtlasCLIOptions.parentTraitName);
-
-			Taxonomy tx = new Taxonomy();
-
-			String traitJson = tx.createTraitTypes(traittype, parenttrait);
-
-			this.aClient.createType(traitJson);
-
-			System.out.println("Trait Created with name :" + traittype);
-
-		} catch (Exception e) {
-			e.printStackTrace();
-		}
-
-	}
-
-	/**
-	 * This create a Process Type
-	 * 
-	 * @param processName
-	 */
-	private void createProcessType(CommandLine line) {
-		try {
-			AtlasTypeDefCreator ad = new AtlasTypeDefCreator(baseurl);
-			String typename = line.getOptionValue(AtlasCLIOptions.type);
-			String typeJson = ad.assembleProcessType(null, typename);
-			aClient.createType(typeJson);
-
-		} catch (Exception e) {
-			e.printStackTrace();
-		}
-
-	}
-
-	/**
-	 * 
-	 * @param dataSetTypeName
-	 */
-	private void createDataSetType(CommandLine line) {
-		try {
-			AtlasTypeDefCreator ad = new AtlasTypeDefCreator(baseurl);
-			String typename = line.getOptionValue(AtlasCLIOptions.type);
-
-			String typeJson = ad.assembleDataSetType(null, typename);
-
-			System.out.println(typeJson);
-			aClient.createType(typeJson);
-
-		} catch (Exception e) {
-			e.printStackTrace();
-		}
-
-	}
-
-	/**
-	 * 
-	 * @param type
-	 * @param name
-	 * @param description
-	 */
-	private void createSimpleEntity(CommandLine line) {
-
-		AtlasEntityCreator aec = new AtlasEntityCreator(baseurl);
-		String typename = line.getOptionValue(AtlasCLIOptions.type);
-		String name = line.getOptionValue(AtlasCLIOptions.name);
-		String description = line.getOptionValue(AtlasCLIOptions.description);
-
-		if (description == null || "".equals(description)) {
-			description = "This is is entity of type :" + typename
-					+ " with name:" + name;
-		}
-
-		try {
-			Referenceable createuniveralEntity = aec.createRefObject(typename,
-					name, description);
-			aec.createEntity(createuniveralEntity);
-
-		} catch (Exception e) {
-
-			e.printStackTrace();
-		}
-
-	}
-
-	/**
-	 * 
-	 * 
-	 */
-	private void createDataSetEntity(CommandLine line) {
-		AtlasEntityCreator aec = new AtlasEntityCreator(baseurl);
-		try {
-
-			String type = line.getOptionValue(AtlasCLIOptions.type);
-			String name = line.getOptionValue(AtlasCLIOptions.name);
-			String description = line
-					.getOptionValue(AtlasCLIOptions.description);
-
-			if (description == null || "".equals(description)) {
-				description = "This is is entity of type :" + type
-						+ " with name:" + name;
-			}
-
-			String traitnames = line.getOptionValue(AtlasCLIOptions.traitnames);
-
-			// TODO
-			// This needs to be replaced by columns
-
-			List<Referenceable> timeDimColumns = ImmutableList.of(
-					aec.rawColumn("time_id", "int", "time id"),
-					aec.rawColumn("dayOfYear", "int", "day Of Year"),
-					aec.rawColumn("weekDay", "int", "week Day"));
-
-			Referenceable referenceable;
-			if (traitnames != null)
-				referenceable = new Referenceable(type, traitnames);
-			else
-				referenceable = new Referenceable(type);
-			referenceable.set("name", name);
-			referenceable.set("description", description);
-			referenceable.set("createTime", System.currentTimeMillis());
-			referenceable.set("lastAccessTime", System.currentTimeMillis());
-
-			referenceable.set("columns", timeDimColumns);
-
-			aec.createEntity(referenceable);
-
-		} catch (Exception e) {
-			// TODO Auto-generated catch block
-			e.printStackTrace();
-		}
-	}
-
-	/**
-	 * This creates a process Entity
-	 * 
-	 */
-	private void createProcessEntity(CommandLine line) {
-
-		String type = line.getOptionValue(AtlasCLIOptions.type);
-		String description = line.getOptionValue(AtlasCLIOptions.description);
-		String name = line.getOptionValue(AtlasCLIOptions.name);
-
-		if (description == null || "".equals(description)) {
-			description = "This is is entity of type :" + type;
-		}
-
-		String inp_type_name = line.getOptionValue(AtlasCLIOptions.inp_type);
-		String inp_value = line.getOptionValue(AtlasCLIOptions.inp_value);
-		String out_type_name = line.getOptionValue(AtlasCLIOptions.out_type);
-		String out_value = line.getOptionValue(AtlasCLIOptions.out_value);
-		String traitname = line.getOptionValue(AtlasCLIOptions.traitnames);
-
-		AtlasEntitySearch aES = new AtlasEntitySearch(baseurl);
-		AtlasEntityConnector aec = new AtlasEntityConnector();
-		Referenceable inpref;
-		try {
-
-			inpref = aES.getReferenceByName(inp_type_name, inp_value);
-
-			Referenceable outref = aES.getReferenceByName(out_type_name,
-					out_value);
-
-			Referenceable proc = aec.loadProcess(type, name, description,
-					ImmutableList.of(inpref.getId()),
-					ImmutableList.of(outref.getId()), traitname);
-
-			createEntity(proc);
-			System.out.println(" Lineage formed in Atlas with name " + name
-					+ " of type " + type);
-		} catch (Exception e) {
-			// TODO Auto-generated catch block
-			e.printStackTrace();
-		}
-
-	}
-
-	/**
-	 * This method lists all types in the Atlas
-	 */
-	public void listTypes() {
-
-		List<String> lt;
-		try {
-			lt = aClient.listTypes();
-
-			ListIterator<String> lstItr = lt.listIterator();
-
-			System.out.println("Listing all Types in atlas: ");
-
-			while (lstItr.hasNext()) {
-
-				System.out.println(lstItr.next());
-
-			}
-		} catch (AtlasServiceException e) {
-			// TODO Auto-generated catch block
-			e.printStackTrace();
-		}
-
-	}
-
-	/**
-	 * This method imports all the table metadata from mysqltable and loads it
-	 * to Atlas
-	 * 
-	 * @param line
-	 * @throws Exception
-	 */
-	public void importMysqlTables(CommandLine line) throws Exception {
-
-		String host = line.getOptionValue(AtlasCLIOptions.mysqlhost);
-		String db = line.getOptionValue(AtlasCLIOptions.dbname);
-		String user = line.getOptionValue(AtlasCLIOptions.username);
-		String password = line.getOptionValue(AtlasCLIOptions.password);
-		String url = line.getOptionValue(AtlasCLIOptions.url);
-
-		AtlasTableInterface atlasTabIn = new AtlasTableInterface(url, host, db,
-				user, password);
-	}
-
-	/**
-	 * 
-	 * @param line
-	 */
-	public void loadHierarchy(CommandLine line) {
-
-		String path = line.getOptionValue(AtlasCLIOptions.filepath);
-		JsonHierarchy jsn;
-
-		try {
-
-			jsn = new JsonHierarchy();
-			jsn.parseJSON(path);
-
-			ArrayList<EntityModel> emList = jsn.getEmList();
-			ArrayList<TupleModel> tmModel = jsn.getTmapList();
-
-			/**
-			 * Create the Traits
-			 */
-
-			ListIterator<TupleModel> arMdl = tmModel.listIterator();
-			TupleModel tpM = null;
-			String trait = null;
-			String supertrait = null;
-			Taxonomy tx1 = new Taxonomy();
-
-			while (arMdl.hasNext()) {
-
-				tpM = arMdl.next();
-				trait = tpM.getCurrnode();
-				supertrait = tpM.getParentnode();
-
-				String traitJson = tx1.createTraitTypes(trait, supertrait);
-				System.out.println(trait + " created..");
-
-				try {
-
-					this.aClient.createType(traitJson);
-
-				} catch (AtlasServiceException e) {
-					// TODO Auto-generated catch block
-					e.printStackTrace();
-				}
-
-			}
-
-			/**
-			 * Create Entities
-			 */
-
-			ListIterator<EntityModel> lsiEM = emList.listIterator();
-			EntityModel em = null;
-			String type = null;
-			String name = null;
-			trait = null;
-			Referenceable ref;
-			String Id;
-			Struct stc;
-			AtlasEntitySearch aES;
-			String type_name;
-			String value;
-
-			while (lsiEM.hasNext()) {
-
-				em = lsiEM.next();
-				name = em.getName();
-				trait = em.getParent();
-				this.searchEntities(line);
-
-				aES = new AtlasEntitySearch(baseurl);
-				type_name = line.getOptionValue(AtlasCLIOptions.type);
-				value = line.getOptionValue(AtlasCLIOptions.name);
-
-				try {
-					ref = aES.getReferenceByName(type_name, value);
-					if (ref != null) {
-						Id = ref.getId()._getId();
-						stc = new Struct(trait);
-						this.addTrait(Id, stc);
-						
-						System.out.println(String.format("Trait %s update to entity %s", trait, value));
-					}else {
-					
-						System.out.println(String.format("Entity %s, not found in Atlas. Trait not added", trait, value));
-					}
-
-				} catch (Exception e) {
-					// TODO Auto-generated catch block
-					e.printStackTrace();
-				}
-
-			}
-
-		} catch (JsonParseException e) {
-			// TODO Auto-generated catch block
-			e.printStackTrace();
-		} catch (IOException e) {
-			// TODO Auto-generated catch block
-			e.printStackTrace();
-		}
-	}
-
-	/**
-	 * 
-	 * This is a generic method of creating entities of any class
-	 * 
-	 * @throws JSONException
-	 * @throws AtlasServiceException
-	 * @throws com.atlas.client.AtlasServiceException
-	 * 
-	 */
-	public Id createEntity(Referenceable ref) throws JSONException,
-			AtlasServiceException, AtlasServiceException {
-
-		String typename = ref.getTypeName();
-
-		String entityJSON = InstanceSerialization.toJson(ref, true);
-
-		System.out.println("Submitting new entity= " + entityJSON);
-
-		JSONObject jsonObject = aClient.createEntity(entityJSON);
-
-		String guid = jsonObject.getString(AtlasClient.GUID);
-
-		System.out.println("created instance for type " + typename + ", guid: "
-				+ guid);
-
-		// return the Id for created instance with guid
-
-		return new Id(guid, ref.getId().getVersion(), ref.getTypeName());
-
-	}
-
-	/**
-	 * 
-	 * @param guid
-	 * @param trait
-	 * @throws Exception
-	 */
-	private void addTrait(String guid, Struct trait) throws Exception {
-		NewAtlasClient newatlasClient = new NewAtlasClient(this.baseurl);
-		// String guid = referenceable.getId()._getId();
-		List<String> existTraits = newatlasClient.getTraitNames(guid);
-
-		if (existTraits == null || !existTraits.contains(trait.typeName)) {
-			String traitJson = InstanceSerialization.toJson(trait, true);
-			newatlasClient.addTrait(guid, traitJson);
-		}
-	}
-
-}
diff --git a/codesamples/atlas/src/main/java/com/atlas/cli/AtlasCLIOptions.java b/codesamples/atlas/src/main/java/com/atlas/cli/AtlasCLIOptions.java
deleted file mode 100644
index b7c9c26..0000000
--- a/codesamples/atlas/src/main/java/com/atlas/cli/AtlasCLIOptions.java
+++ /dev/null
@@ -1,49 +0,0 @@
-package com.atlas.cli;
-
-public class AtlasCLIOptions {
-
-	
-	public static String url = "url";
-	public static String action = "c";
-	
-	
-	public static String type = "type";
-	public static String name = "name";
-	public static String description = "description";
-	public static String supertype = "parenttype";
-	public static String listtype = "listtype";
-	
-	public static String inp_type = "inptype";
-
-	public static String inp_value = "inpvalue";
-
-	public static String out_type = "outtype";
-
-	public static String out_value = "outvalue";
-	
-	public static String traitnames = "traitnames";
-	public static String traitTypename = "traittype";
-	public static String parentTraitName = "parenttrait";
-	public static String filepath = "";
-	
-	
-	
-	public static String search = "search";
-	public static String createSimpleType= "createSimpleType";
-	public static String createDataSetType= "createDatasetType";
-	public static String createProcessType= "createProcessType";
-	public static String createSimpleEntity = "createSimpleEntity";
-	public static String createDataSetEntity = "createDataSetEntity";
-	public static String createProcessEntity = "createProcessEntity";
-	public static String bindProcess = "bindprocess";
-	public static String createrait = "createtrait";
-	public static String loadtraithierarchy = "loadtraithierarchy";
-	
-	public static String importMySqlTables = "importmysql";
-	public static String dbname = "db";
-	public static String username = "username";
-	public static String password = "password";
-	public static String mysqlhost = "mysqlhost";
-	
-	
-}
diff --git a/codesamples/atlas/src/main/java/com/atlas/client/AtlasEntityConnector.java b/codesamples/atlas/src/main/java/com/atlas/client/AtlasEntityConnector.java
deleted file mode 100644
index c85ab6d..0000000
--- a/codesamples/atlas/src/main/java/com/atlas/client/AtlasEntityConnector.java
+++ /dev/null
@@ -1,118 +0,0 @@
-package com.atlas.client;
-
-import java.util.List;
-
-import org.apache.atlas.AtlasClient;
-import org.apache.atlas.AtlasServiceException;
-import org.apache.atlas.typesystem.Referenceable;
-import org.apache.atlas.typesystem.json.InstanceSerialization;
-import org.apache.atlas.typesystem.persistence.Id;
-import org.apache.atlas.typesystem.types.DataTypes;
-import org.codehaus.jettison.json.JSONException;
-import org.codehaus.jettison.json.JSONObject;
-
-import com.google.common.collect.ImmutableList;
-
-public class AtlasEntityConnector {
-
-
-	{
-		System.setProperty("atlas.conf", "./conf");
-	}
-	
-	private  AtlasClient ac = null;
-	
-	
-	public static void main(String[] args) throws Exception {
-	
-		if(args.length < 0)
-			throw new Exception("Please pass the atlas base url");
-		
-		String baseurl = args[0];
-		String inp_type_name = args[1];
-		String inp_value = args[2];
-		
-		String out_type_name = args[3];
-		String out_value = args[4];
-		
-		System.out.println(baseurl);
-		System.out.println(inp_type_name);
-		System.out.println(inp_value);
-		
-		
-		System.out.println(" Baseurl" + baseurl);
-		AtlasEntitySearch aES = new AtlasEntitySearch(baseurl);
-		AtlasEntityConnector aec = new AtlasEntityConnector();
-		aec.ac = aES.getAtlasClient();
-		
-		try {
-			
-			Referenceable inpref = aES.getReferenceByName(inp_type_name, inp_value);
-			
-			Referenceable outref = aES.getReferenceByName(out_type_name, out_value);
-		
-			Referenceable proc = aec.loadProcess(AtlasTypeDefCreator.Type_New_Life, "AsteroidConnector", "This  connects 2 Asteroids", ImmutableList.of(inpref.getId()), ImmutableList.of(outref.getId()), "Red");
-			
-			aec.createEntity(proc);
-			
-			System.out.println(" The 2 objects are connected");
-			
-			
-		} catch (Exception e) {
-			// TODO Auto-generated catch block
-			e.printStackTrace();
-		}
-	}
-
-	
-	 /*
-	  * 
-	  * 
-	  */
-	public Referenceable loadProcess(String type, String name, String description, List<Id> inputTables, List<Id> outputTables,
-           String... traitNames)
-   throws Exception {
-       Referenceable referenceable = new Referenceable(type, traitNames);
-       // super type attributes
-       referenceable.set("name", name);
-       referenceable.set("description", description);
-       referenceable.set("inputs", inputTables);
-       referenceable.set("outputs", outputTables);
-       referenceable.set("userName", "sdutta");
-       referenceable.set("startTime", System.currentTimeMillis());
-       referenceable.set("endTime", System.currentTimeMillis());	
-       
-       return referenceable;
-
-	}
-	
-	
-	/**
-	 * 
-	 * This is a generic method of creating entities of any class
-	 * @throws JSONException 
-	 * @throws AtlasServiceException 
-	 * 
-	 */
-	public Id createEntity(Referenceable ref ) throws JSONException, AtlasServiceException{
-		
-		String typename = ref.getTypeName(); 
-		
-		String entityJSON = InstanceSerialization.toJson(ref, true);
-		
-		System.out.println("Submitting new entity= " + entityJSON);
-        
-        JSONObject jsonObject = ac.createEntity(entityJSON);
-       
-        String guid = jsonObject.getString(AtlasClient.GUID);
-        
-        System.out.println("created instance for type " + typename + ", guid: " + guid);
-
-        // return the Id for created instance with guid
-       
-        return new Id(guid, ref.getId().getVersion(), ref.getTypeName());
-		
-		
-	}
-
-}
diff --git a/codesamples/atlas/src/main/java/com/atlas/client/AtlasEntityCreator.java b/codesamples/atlas/src/main/java/com/atlas/client/AtlasEntityCreator.java
deleted file mode 100644
index e989e79..0000000
--- a/codesamples/atlas/src/main/java/com/atlas/client/AtlasEntityCreator.java
+++ /dev/null
@@ -1,253 +0,0 @@
-package com.atlas.client;
-
-import java.util.List;
-
-import org.apache.atlas.AtlasClient;
-import org.apache.atlas.AtlasServiceException;
-import org.apache.atlas.typesystem.Referenceable;
-import org.apache.atlas.typesystem.json.InstanceSerialization;
-import org.apache.atlas.typesystem.persistence.Id;
-import org.apache.atlas.typesystem.types.AttributeDefinition;
-import org.apache.atlas.typesystem.types.ClassType;
-import org.apache.atlas.typesystem.types.DataTypes;
-import org.apache.atlas.typesystem.types.HierarchicalTypeDefinition;
-import org.apache.atlas.typesystem.types.IDataType;
-import org.apache.atlas.typesystem.types.Multiplicity;
-import org.apache.atlas.typesystem.types.utils.TypesUtil;
-import org.codehaus.jettison.json.JSONArray;
-import org.codehaus.jettison.json.JSONException;
-import org.codehaus.jettison.json.JSONObject;
-
-import com.google.common.base.Preconditions;
-import com.google.common.collect.ImmutableList;
-
-
-/**
- * This is a sample class to instantiate Entities for existing entities
- * 
- * @author sdutta
- *
- */
-public class AtlasEntityCreator {
-
-	{
-		System.setProperty("atlas.conf", "conf");
-	}
-	
-	private  AtlasClient ac = null;
-	
-	
-	/**
-	 * 
-	 * @param args
-	 * @throws Exception
-	 */
-	public static void main(String[] args) throws Exception {
-		// TODO Auto-generated method stub
-
-		if(args.length < 0)
-			throw new Exception("Please pass the atlas base url");
-		
-		String baseurl = args[0];
-		
-		System.out.println(" Baseurl" + baseurl);
-		AtlasEntityCreator aEC = new AtlasEntityCreator(baseurl);
-		//aEC.defineSimpleEntities();
-		
-		aEC.defineDataSetEntities();
-		
-		aEC.destroy();
-		
-	}
-	
-	
-	
-	/**
-	 * We created 3 types in the previous example
-	 * @throws Exception 
-	 * 
-	 * 
-	
-	private void defineSimpleEntities() throws Exception {
-		
-		Referenceable createuniveralEntity = this.createRefObjectWithTraits(AtlasTypeDefCreator.Type_GOD, "Level2_Prod", "Level 2 type company", "Product");
-		
-		//Referenceable creategeneralEntity = createRefObjectWithTraits(AtlasTypeDefCreator.Type_Planets, "Level2", "Level2 of type Prioduct","Product");
-         
-		Id universalObjId = this.createEntity(createuniveralEntity);
-		
-		//Id generalObjId = this.createEntity(creategeneralEntity);
-		
-		
-	} */
-	
-	 
-	
-	/**
-	 * 
-	 * @throws Exception
-	 */
-	private void defineDataSetEntities() throws Exception {
-
-		List<Referenceable> timeDimColumns = ImmutableList
-                .of(rawColumn("time_id", "int", "time id"), rawColumn("dayOfYear", "int", "day Of Year"),
-                        rawColumn("weekDay", "int", "week Day"));
-		
-		  Id Aster1 =
-	                createDataSetType("Aster1", "customer dimension table", timeDimColumns, 1000, "1 lightyear",  
-	                        "White");
-		
-		  List<Referenceable> spaceDimColumns = ImmutableList
-	                .of(rawColumn("space_id", "int", "space id"), rawColumn("timeOfYear", "int", "time Of Year"),
-	                        rawColumn("weekofDay", "int", "week Day"));
-			
-			  Id Aster2 =
-		                createDataSetType("Aster2", "customer dimension table", timeDimColumns, 1000, "1 lightyear",  
-		                        "White");
-			
-			 
-		
-		
-	}
-	 
-	
-	/**
-	 * 
-	 * @param name
-	 * @param description
-	 * @param columns
-	 * @param traitNames
-	 * @return
-	 * @throws Exception
-	 */
-	 
-	public Id createDataSetType(String name, String description, List<Referenceable> columns, int speed, String dist, String... traitNames) throws Exception {
-	        Referenceable referenceable = new Referenceable(AtlasTypeDefCreator.Type_Asteroids, traitNames);
-	        referenceable.set("name", name);
-	        referenceable.set("description", description);
-	        referenceable.set("createTime", System.currentTimeMillis());
-	        referenceable.set("lastAccessTime", System.currentTimeMillis());
-	        referenceable.set("speed", speed);
-	        referenceable.set("distance_frm_Earth", dist);
-	
-	        referenceable.set("columns", columns);
-
-	        return createEntity(referenceable);
-	    }
-	 
-	 
-	
-	/**
-	 * This method creates a simple entities
-	 * 
-	 * @param name
-	 * @param comment
-	 * @return
-	 * @throws Exception
-	 */
-	 public Referenceable createRefObject(String type, String name, String description, String... traits)
-	    	    throws Exception {
-	    	        Referenceable referenceable = new Referenceable(type, traits);
-	    	        referenceable.set("name", name);
-	    	        referenceable.set("description", description);
-	    	        referenceable.set("createTime", System.currentTimeMillis());
-	    	        referenceable.set("lastAccessTime", System.currentTimeMillis());
-	    	        	    	    
-	    	        return referenceable;
-	    	    }
-	 
-	 
-	 /**
-		 * This method creates a simple entities
-		 * 
-		 * @param name
-		 * @param comment
-		 * @return
-		 * @throws Exception
-		 */
-		 public Referenceable createRefObjectWithTraits(String type, String name, String description,String... traits)
-		    	    throws Exception {
-		    	        Referenceable referenceable = new Referenceable(type, traits);
-		    	        referenceable.set("name", name);
-		    	        referenceable.set("description", description);
-		    	        	    	    
-		    	        return referenceable;
-		    	    }
-	
-	  
-	/**
-	 * 
-	 * @param baseurl
-	 */
-	public  AtlasEntityCreator(String baseurl){
-		
-		System.out.println("Creating Client Connection" + baseurl);
-		ac = new AtlasClient(baseurl);
-		System.out.println("Client Object returned");
-	
-		
-	}
-	
-	
-
-	
-	/**
-	 * 
-	 * This is a generic method of creating entities of any class
-	 * @throws JSONException 
-	 * @throws AtlasServiceException 
-	 * 
-	 */
-	public Id createEntity(Referenceable ref ) throws JSONException, AtlasServiceException{
-		
-		String typename = ref.getTypeName(); 
-		
-		String entityJSON = InstanceSerialization.toJson(ref, true);
-		
-		System.out.println("Submitting new entity= " + entityJSON);
-        
-        JSONObject jsonObject = ac.createEntity(entityJSON);
-       
-        String guid = jsonObject.getString(AtlasClient.GUID);
-        
-        System.out.println("created instance for type " + typename + ", guid: " + guid);
-
-        // return the Id for created instance with guid
-       
-        return 
-        		new Id(guid, ref.getId().getVersion(), ref.getTypeName());
-		
-		
-	}
-	
-	
-	/**
-	 * Creates a rowCloumn
-	 * @param name
-	 * @param dataType
-	 * @param comment
-	 * @param traitNames
-	 * @return
-	 * @throws Exception
-	 */
-	public Referenceable rawColumn(String name, String dataType, String comment, String... traitNames) throws Exception {
-	        Referenceable referenceable = new Referenceable(AtlasTypeDefCreator.COLUMN_TYPE, traitNames);
-	        referenceable.set("name", name);
-	        referenceable.set("dataType", dataType);
-	        referenceable.set("comment", comment);
-
-	        return referenceable;
-	    }
-	  
-	
-	/**
-	 * 
-	 * 
-	 */
-	public void destroy() {
-		
-		ac = null;
-	}
-
-	
-}
diff --git a/codesamples/atlas/src/main/java/com/atlas/client/AtlasEntitySearch.java b/codesamples/atlas/src/main/java/com/atlas/client/AtlasEntitySearch.java
deleted file mode 100644
index 53dcc8b..0000000
--- a/codesamples/atlas/src/main/java/com/atlas/client/AtlasEntitySearch.java
+++ /dev/null
@@ -1,130 +0,0 @@
-package com.atlas.client;
-
-import java.util.List;
-
-import org.apache.atlas.AtlasClient;
-import org.apache.atlas.typesystem.Referenceable;
-import org.apache.atlas.typesystem.json.InstanceSerialization;
-import org.apache.atlas.typesystem.persistence.Id;
-import org.codehaus.jettison.json.JSONArray;
-import org.codehaus.jettison.json.JSONObject;
-
-
-
-/**
- * This code shows an example of searching for Entities by name
- * @author sdutta
- *
- */
-public class AtlasEntitySearch {
-
-	{
-		System.setProperty("atlas.conf", "./conf");
-	}
-
-	private AtlasClient ac = null;
-
- 
-	/**
-	 * 
-	 * @param args
-	 * @throws Exception
-	 */
-	public static void main(String[] args) throws Exception {
-
-		if (args.length < 0)
-			throw new Exception("Please pass the atlas base url");
-
-		String baseurl = args[0];
-		String type_name = args[1];
-		String value = args[2];
-
-		System.out.println(baseurl);
-		System.out.println(type_name);
-		System.out.println(value);
-
-		System.out.println(" Baseurl" + baseurl);
-		AtlasEntitySearch aES = new AtlasEntitySearch(baseurl);
-
-		try {
-
-			Referenceable ref = aES.getReferenceByName(type_name, value);
-
-			String entityJSON = InstanceSerialization.toJson(ref, true);
-
-			System.out.println(entityJSON);
-
-		} catch (Exception e) {
-			// TODO Auto-generated catch block
-			e.printStackTrace();
-		}
-
-		// aEC.destroy();
-
-	}
-
-	/**
-	 * 
-	 * @param baseurl
-	 */
-	public AtlasEntitySearch(String baseurl) {
-
-		System.out.println("Creating Client Connection" + baseurl);
-		ac = new AtlasClient(baseurl);
-		System.out.println("Client Object returned");
-
-	}
-
-	/**
-	 * 
-	 * @param typeName
-	 * @param value
-	 * @return
-	 * @throws Exception
-	 */
-	public Referenceable getReferenceByName(String typeName, String value)
-			throws Exception {
-
-		System.out.println(String.format("Getting reference for Entity %s",
-				value));
-
-		String dslQuery = String.format("%s where %s = '%s'", typeName, "name",
-				value);
-
-		return getEntityReferenceFromDSL(typeName, dslQuery);
-	}
-
-	/*
-	 * This will get an entity Object
-	 */
-	private Referenceable getEntityReferenceFromDSL(String typeName,
-			String dslQuery) throws Exception {
-
-		AtlasClient dgiClient = ac;
-
-		JSONArray results = dgiClient.searchByDSL(dslQuery);
-		if (results.length() == 0) {
-			return null;
-		} else {
-			String guid;
-			JSONObject row = results.getJSONObject(0);
-
-			if (row.has("$id$")) {
-				guid = row.getJSONObject("$id$").getString("id");
-			} else {
-				guid = row.getJSONObject("_col_0").getString("id");
-			}
-
-			return new Referenceable(guid, typeName, null);
-		}
-	}
-
-	/*
-	 * 
-	 */
-	public AtlasClient getAtlasClient() {
-
-		return ac;
-	}
-
-}
diff --git a/codesamples/atlas/src/main/java/com/atlas/client/AtlasServiceException.java b/codesamples/atlas/src/main/java/com/atlas/client/AtlasServiceException.java
deleted file mode 100644
index a7f4f3e..0000000
--- a/codesamples/atlas/src/main/java/com/atlas/client/AtlasServiceException.java
+++ /dev/null
@@ -1,32 +0,0 @@
-package com.atlas.client;
-
-import com.sun.jersey.api.client.ClientResponse;
-import org.codehaus.jettison.json.JSONException;
-
-public class AtlasServiceException extends Exception {
-    private ClientResponse.Status status;
-
-    public AtlasServiceException(NewAtlasClient.API api, Exception e) {
-        super("Metadata service API " + api + " failed", e);
-    }
-
-    public AtlasServiceException(NewAtlasClient.API api, ClientResponse response) {
-        super("Metadata service API " + api + " failed with status " +
-                response.getClientResponseStatus().getStatusCode() + "(" +
-                response.getClientResponseStatus().getReasonPhrase() + ") Response Body (" +
-                response.getEntity(String.class) + ")");
-        this.status = response.getClientResponseStatus();
-    }
-
-    public AtlasServiceException(Exception e) {
-        super(e);
-    }
-
-    public AtlasServiceException(String message, Exception e) {
-        super(message, e);
-    }
-
-    public ClientResponse.Status getStatus() {
-        return status;
-    }
-}
\ No newline at end of file
diff --git a/codesamples/atlas/src/main/java/com/atlas/client/AtlasTypeDefCreator.java b/codesamples/atlas/src/main/java/com/atlas/client/AtlasTypeDefCreator.java
deleted file mode 100644
index c01a249..0000000
--- a/codesamples/atlas/src/main/java/com/atlas/client/AtlasTypeDefCreator.java
+++ /dev/null
@@ -1,352 +0,0 @@
-package com.atlas.client;
-
-import java.util.List;
-import java.util.ListIterator;
-
-import org.apache.atlas.AtlasClient;
-import org.apache.atlas.typesystem.TypesDef;
-import org.apache.atlas.typesystem.json.TypesSerialization;
-import org.apache.atlas.typesystem.types.AttributeDefinition;
-import org.apache.atlas.typesystem.types.ClassType;
-import org.apache.atlas.typesystem.types.DataTypes;
-import org.apache.atlas.typesystem.types.EnumTypeDefinition;
-import org.apache.atlas.typesystem.types.HierarchicalTypeDefinition;
-import org.apache.atlas.typesystem.types.IDataType;
-import org.apache.atlas.typesystem.types.Multiplicity;
-import org.apache.atlas.typesystem.types.StructTypeDefinition;
-import org.apache.atlas.typesystem.types.TraitType;
-import org.apache.atlas.typesystem.types.TypeUtils;
-import org.apache.atlas.typesystem.types.utils.TypesUtil;
-import org.apache.commons.math3.util.MultidimensionalCounter.Iterator;
-
-import com.google.common.base.Preconditions;
-import com.google.common.collect.ImmutableList;
-
-import org.codehaus.jettison.json.JSONArray;
-import org.codehaus.jettison.json.JSONException;
-import org.codehaus.jettison.json.JSONObject;
-import org.apache.atlas.AtlasClient;
-import org.apache.atlas.AtlasServiceException;
-
-/*
- * This is a class to create Type Definitions
- * This is a simple class
- * @author - Shivaji Dutta
- */
- 
-public class AtlasTypeDefCreator {
-
-	{
-		System.setProperty("atlas.conf", "conf");
-	}
-
-	private String traitName = "Green";
-
-	private AtlasClient ac = null;
-
-	private ImmutableList enumType = null;
-	private ImmutableList structType = null;
-	private ImmutableList classType = null;
-	private ImmutableList traitType = null;
-
-	/**
-	 * This assembles the type
-	 * If trait name is passed it creates a simple trait with the class name
-	 * if not, just the class type is created
-	 * 
-	 * @return
-	 * @throws Exception 
-	 */
-
-	public String assembleSimpleType(String traitName, String ClassTypeName, String parenttype) throws Exception {
-		
-
-		System.out.print(traitName+  ClassTypeName + parenttype);
-		
-		TypesDef tdef = TypeUtils.getTypesDef(ImmutableList.<EnumTypeDefinition> of(),
-				ImmutableList.<StructTypeDefinition> of(),
-				this.createTraitType(traitName),
-				this.createClassType(ClassTypeName, parenttype));
-		
-		
-
-		return TypesSerialization.toJson(tdef);
-	}
-
-	/**
-	 * This register the Process type
-	 * 
-	 * @return s
-	 */
-
-	public String assembleProcessType(String traitName, String ClassTypeName) {
-		TypesDef tdef = null;
-
-		tdef = TypeUtils.getTypesDef(ImmutableList.<EnumTypeDefinition> of(),
-				ImmutableList.<StructTypeDefinition> of(),
-				this.createTraitType(traitName),
-				ImmutableList.of(this.createProcessTypeByName(ClassTypeName)));
-
-		return TypesSerialization.toJson(tdef);
-	}
-
-	public String assembleDataSetType(String traitName, String ClassTypeName) {
-		TypesDef tdef = null;
-
-		tdef = TypeUtils.getTypesDef(ImmutableList.<EnumTypeDefinition> of(),
-				ImmutableList.<StructTypeDefinition> of(),
-				this.createTraitType(traitName),
-				ImmutableList.of(this.createDataSetTypeByName(ClassTypeName)));
-
-		return TypesSerialization.toJson(tdef);
-	}
-
-	/**
-	 * 
-	 * @param baseurl
-	 * @throws AtlasServiceException
-	 */
-	public AtlasTypeDefCreator(String baseurl) throws AtlasServiceException {
-
-		//System.out.println("Creating Client Connection" + baseurl);
-		ac = new AtlasClient(baseurl);
-		//System.out.println("Client Object returned");
-
-	}
-
-	AttributeDefinition attrDef(String name, IDataType dT, Multiplicity m,
-			boolean isComposite, String reverseAttributeName) {
-		Preconditions.checkNotNull(name);
-		Preconditions.checkNotNull(dT);
-		return new AttributeDefinition(name, dT.getName(), m, isComposite,
-				reverseAttributeName);
-	}
-
-	AttributeDefinition attrDef(String name, IDataType dT) {
-		return attrDef(name, dT, Multiplicity.OPTIONAL, false, null);
-	}
-
-	AttributeDefinition attrDef(String name, IDataType dT, Multiplicity m) {
-		return attrDef(name, dT, m, false, null);
-	}
-
-	//public static String Type_GOD = "GOD_Type";
-	//public static String Type_Planets = "Planet_Type";
-	//public static String Type_Forces = "Force_Type";
-	public static String Type_New_Life = "New_Life_Type";
-	public static String Type_Asteroids = "Asteroid_Type";
-	public static final String COLUMN_TYPE = "Column";
-
-	/**
-	 * This returns an ImmutableList of the type being created
-	 * 
-	 * @param typeName
-	 * @return
-	 * @throws Exception 
-	 */
-	@SuppressWarnings({ "unused", "unchecked" })
-	public ImmutableList<HierarchicalTypeDefinition<ClassType>> createClassType(
-			String typeName, String parenttype) throws Exception {
-
-		if(typeName == null ){
-			throw new Exception ("type or typename cannot be null");
-		}
-		
-		System.out.print("class type name" + typeName);
-		HierarchicalTypeDefinition<ClassType> genericType =  null;
-		if(parenttype != null)
-		   {
-			genericType = TypesUtil
-				.createClassTypeDef(typeName, ImmutableList.of(parenttype),
-						attrDef("name", DataTypes.STRING_TYPE),
-						attrDef("description", DataTypes.STRING_TYPE),
-						attrDef("createTime", DataTypes.INT_TYPE),
-						attrDef("lastAccessTime", DataTypes.INT_TYPE)
-						
-						);
-		
-		   } else{
-			   genericType = TypesUtil
-						.createClassTypeDef(typeName, null,
-								attrDef("name", DataTypes.STRING_TYPE),
-								attrDef("description", DataTypes.STRING_TYPE),
-								attrDef("createTime", DataTypes.INT_TYPE),
-								attrDef("lastAccessTime", DataTypes.INT_TYPE));
-				
-		   }
-		
-		this.classType = ImmutableList.of(genericType);
-		return this.classType;
-	}
-
-	/**
-	 * This lets you create Class Type
-	 * 
-	 * @return
-	 */
-	public HierarchicalTypeDefinition<ClassType> createProcessTypes() {
-
-		return TypesUtil.createClassTypeDef(this.Type_New_Life,
-				ImmutableList.of("Process"),
-				attrDef("userName", DataTypes.STRING_TYPE),
-				attrDef("startTime", DataTypes.INT_TYPE),
-				attrDef("endTime", DataTypes.INT_TYPE));
-
-	}
-
-	/**
-	 * This method helps you to create types
-	 * 
-	 * @param typeName
-	 * @return
-	 */
-	public HierarchicalTypeDefinition<ClassType> createProcessTypeByName(
-			String typeName) {
-
-		return TypesUtil.createClassTypeDef(typeName,
-				ImmutableList.of("Process"),
-				attrDef("userName", DataTypes.STRING_TYPE),
-				attrDef("startTime", DataTypes.INT_TYPE),
-				attrDef("endTime", DataTypes.INT_TYPE));
-
-	}
-
-	/**
-	 * 
-	 * @param typeName
-	 * @return
-	 */
-	public HierarchicalTypeDefinition<ClassType> createDataSetTypeByName(
-			String typeName) {
-
-		return TypesUtil.createClassTypeDef(
-				typeName,
-				ImmutableList.of("DataSet"),
-				attrDef("createTime", DataTypes.INT_TYPE),
-				attrDef("lastAccessTime", DataTypes.INT_TYPE),
-				new AttributeDefinition("columns", DataTypes
-						.arrayTypeName(COLUMN_TYPE), Multiplicity.COLLECTION,
-						true, null));
-
-	}
-
-	/**
-	 * This method helps for creating traits
-	 * 
-	 * @param trait
-	 * @return
-	 */
-	public ImmutableList<HierarchicalTypeDefinition<TraitType>> createTraitType(
-			String trait) {
-
-		if (trait != null)
-			return ImmutableList.of(TypesUtil.createTraitTypeDef(
-					trait, null));
-		else
-			return ImmutableList
-					.<HierarchicalTypeDefinition<TraitType>> of();
-
-		
-	}
-	
-	
-
-	/**
-	 * 
-	 * @param args
-	 * @throws Exception
-	 
-	public static void main(String[] args) throws Exception {
-
-		if (args.length < 0)
-			throw new Exception("Please pass the atlas base url");
-
-		String baseurl = args[0];
-
-		System.out.println(" Baseurl" + baseurl);
-		AtlasTypeDefCreator ad = new AtlasTypeDefCreator(baseurl);
-		ad.traitName = args[1];
-		// ad.registerTypes();
-
-	}
-	
-	*/
-
-	/**
-	 * This is for registering types
-	 * 
-	 * 
-	 * public void registerTypes() throws org.apache.atlas.AtlasServiceException
-	 * {
-	 * 
-	 * System.out.println("Registering Types"); ac.createType(assembleTypes());
-	 * System.out.println("Done Creating Types"); }
-	 */
-
-	/**
-	 * This is a sample code with hard coded values
-	 * 
-	 * @deprecated
-	 * @return
-	 * 
-	 *         ImmutableList<HierarchicalTypeDefinition<ClassType>>
-	 *         createClassTypes() {
-	 * 
-	 *         HierarchicalTypeDefinition<ClassType> UniversalTypes =
-	 * 
-	 *         TypesUtil.createClassTypeDef(this.Type_GOD, null, attrDef("name",
-	 *         DataTypes.STRING_TYPE), attrDef("description",
-	 *         DataTypes.STRING_TYPE));
-	 * 
-	 *         HierarchicalTypeDefinition<ClassType> GeneralTypes =
-	 * 
-	 *         TypesUtil.createClassTypeDef(this.Type_Planets, null,
-	 *         attrDef("name", DataTypes.STRING_TYPE), attrDef("description",
-	 *         DataTypes.STRING_TYPE));
-	 * 
-	 *         HierarchicalTypeDefinition<ClassType> ConnectorTypes =
-	 * 
-	 *         TypesUtil.createClassTypeDef(this.Type_Forces, null,
-	 *         attrDef("name", DataTypes.STRING_TYPE), attrDef("description",
-	 *         DataTypes.STRING_TYPE));
-	 * 
-	 *         HierarchicalTypeDefinition<ClassType> processtype = this
-	 *         .createProcessTypes();
-	 * 
-	 * 
-	 *         HierarchicalTypeDefinition<ClassType> columnClsDef = TypesUtil
-	 *         .createClassTypeDef(COLUMN_TYPE, null, attrDef("name",
-	 *         DataTypes.STRING_TYPE), attrDef("dataType",
-	 *         DataTypes.STRING_TYPE), attrDef("comment",
-	 *         DataTypes.STRING_TYPE));
-	 * 
-	 *         HierarchicalTypeDefinition<ClassType> asteroidDef = TypesUtil
-	 *         .createClassTypeDef( this.Type_Asteroids,
-	 *         ImmutableList.of("DataSet"), attrDef("createTime",
-	 *         DataTypes.INT_TYPE), attrDef("lastAccessTime",
-	 *         DataTypes.INT_TYPE), attrDef("speed", DataTypes.INT_TYPE),
-	 *         attrDef("distance_frm_Earth", DataTypes.STRING_TYPE), new
-	 *         AttributeDefinition("columns", DataTypes
-	 *         .arrayTypeName(COLUMN_TYPE), Multiplicity.COLLECTION, true,
-	 *         null));
-	 * 
-	 *         return // ImmutableList.of(UniversalTypes, GeneralTypes,
-	 *         ConnectorTypes); ImmutableList.of(columnClsDef, asteroidDef);
-	 * 
-	 *         }
-	 
-	 
-	 * The JSON Type String for the TypeDef
-	 * 
-	 * public String assembleTypes() {
-	 * 
-	 * TypesDef tdef = TypeUtils.getTypesDef( ImmutableList.<EnumTypeDefinition>
-	 * of(), ImmutableList.<StructTypeDefinition> of(),
-	 * this.createTraitType(traitName), this.createClassTypes());
-	 * 
-	 * return TypesSerialization.toJson(tdef);
-	 * 
-	 * }
-	 */
-
-}
diff --git a/codesamples/atlas/src/main/java/com/atlas/client/DemoClass.java b/codesamples/atlas/src/main/java/com/atlas/client/DemoClass.java
deleted file mode 100644
index 91303e3..0000000
--- a/codesamples/atlas/src/main/java/com/atlas/client/DemoClass.java
+++ /dev/null
@@ -1,333 +0,0 @@
-package com.atlas.client;
-import com.google.common.base.Preconditions;
-import com.google.common.collect.ImmutableList;
-import com.sun.jersey.api.client.Client;
-import com.sun.jersey.api.client.ClientResponse;
-import com.sun.jersey.api.client.WebResource;
-import com.sun.jersey.api.client.config.DefaultClientConfig;
-import com.sun.jersey.client.urlconnection.URLConnectionClientHandler;
-import org.apache.atlas.AtlasClient;
-
-import org.apache.atlas.security.SecureClientUtils;
-import org.apache.atlas.typesystem.Referenceable;
-import org.apache.atlas.typesystem.json.InstanceSerialization;
-import org.apache.atlas.typesystem.json.TypesSerialization;
-import org.apache.atlas.typesystem.persistence.Id;
-import org.apache.atlas.typesystem.types.*;
-import org.apache.atlas.typesystem.types.utils.TypesUtil;
-import org.apache.atlas.typesystem.TypesDef;
-import org.apache.commons.configuration.PropertiesConfiguration;
-import org.codehaus.jettison.json.JSONArray;
-import org.codehaus.jettison.json.JSONException;
-import org.codehaus.jettison.json.JSONObject;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import javax.ws.rs.HttpMethod;
-import javax.ws.rs.core.MediaType;
-import javax.ws.rs.core.Response;
-import javax.ws.rs.core.UriBuilder;
-
-import java.util.ArrayList;
-import java.util.List;
-
-
-
-/**
- * This is for loading data
- * @author sdutta
- *
- */
-public class DemoClass {
-
-	static Logger logger = LoggerFactory.getLogger(DemoClass.class);
-	
-	 	private static final String TRANSPORT_TYPE = "AeroPlane";
-	    private static final String CARRIER_TYPE = "Manufacturing";
-	    private static final String ROUTE_TYPE = "Air";
-	    private static final String MOTION_TYPE = "Speed";
-	    
-	//    private static final String LOAD_PROCESS_TYPE = "LoadProcess";
-	//    private static final String STORAGE_DESC_TYPE = "StorageDesc";
-	
-	public static void main(String[] args) throws Exception{
-		// TODO Auto-generated method stub
-		
-		if(args.length < 1)
-		{	
-			throw new Exception("Please provide the DGI host url");
-		}
-		
-		System.setProperty("atlas.conf", "/Users/sdutta/Applications/conf");
-		
-		String baseUrl = getServerUrl(args);
-		
-		DemoClass dc = new DemoClass(baseUrl);
-		 dc.createTypes();
-		 
-		 // Shows how to create types in Atlas for your meta model
-	        dc.createTypes();
-
-	        // Shows how to create entities (instances) for the added types in Atlas
-	        dc.createEntities();
-
-	        // Shows some search queries using DSL based on types
-	        //dc.search();
-		
-	}
-	
-	private static final String[] TYPES =
-        {TRANSPORT_TYPE, CARRIER_TYPE, ROUTE_TYPE, MOTION_TYPE,};
-	
-	private final AtlasClient metadataServiceClient;
-	
-	public DemoClass(String baseurl){
-		
-		this.metadataServiceClient = new AtlasClient(baseurl);
-	}
-	
-
-    void createTypes() throws Exception {
-        TypesDef typesDef = createTypeDefinitions();
-
-        String typesAsJSON = TypesSerialization.toJson(typesDef);
-        
-        System.out.println("typesAsJSON = " + typesAsJSON);
-        
-       metadataServiceClient.createType(typesAsJSON);
-
-        // verify types created
-        verifyTypesCreated();
-    }
-    
-    
-    /*
-     * This API will list the types on the system
-     */
-    private void verifyTypesCreated() throws Exception {
-        List<String> types = metadataServiceClient.listTypes();
-        for (String type : TYPES) {
-        	System.out.println(type);
-            assert types.contains(type);
-        }
-    }
-    
-    
-    
-    TypesDef createTypeDefinitions() throws Exception {
-    	
-    	
-        HierarchicalTypeDefinition<ClassType> transportClsDef = TypesUtil
-                .createClassTypeDef(this.TRANSPORT_TYPE, null, attrDef("name", DataTypes.STRING_TYPE),
-                        attrDef("description", DataTypes.STRING_TYPE), attrDef("locationUri", DataTypes.STRING_TYPE),
-                        attrDef("owner", DataTypes.STRING_TYPE), attrDef("createTime", DataTypes.INT_TYPE));
-
-        HierarchicalTypeDefinition<ClassType> carrierClsDef = TypesUtil
-                .createClassTypeDef(this.CARRIER_TYPE, null, 
-                		attrDef("name", DataTypes.STRING_TYPE),
-                        attrDef("location", DataTypes.STRING_TYPE), 
-                        attrDef("country", DataTypes.STRING_TYPE),
-                        attrDef("CEO", DataTypes.STRING_TYPE)
-                       );
-
-        HierarchicalTypeDefinition<ClassType> routeClsDef = TypesUtil
-                .createClassTypeDef(this.ROUTE_TYPE, null, 
-                		attrDef("name", DataTypes.STRING_TYPE),
-                        attrDef("route_id", DataTypes.STRING_TYPE), 
-                        attrDef("comment", DataTypes.STRING_TYPE));
-
-        HierarchicalTypeDefinition<ClassType> motionClsDef = TypesUtil
-                .createClassTypeDef(this.MOTION_TYPE, null, 
-                		attrDef("rating", DataTypes.STRING_TYPE),
-                        attrDef("metrics", DataTypes.STRING_TYPE), 
-                        attrDef("comment", DataTypes.STRING_TYPE));
-
-       
-        HierarchicalTypeDefinition<TraitType> dimTraitDef = TypesUtil.createTraitTypeDef("Dimension", null);
-
-        HierarchicalTypeDefinition<TraitType> factTraitDef = TypesUtil.createTraitTypeDef("Fact", null);
-
-        HierarchicalTypeDefinition<TraitType> piiTraitDef = TypesUtil.createTraitTypeDef("PII", null);
-
-        HierarchicalTypeDefinition<TraitType> metricTraitDef = TypesUtil.createTraitTypeDef("Metric", null);
-
-        HierarchicalTypeDefinition<TraitType> etlTraitDef = TypesUtil.createTraitTypeDef("ETL", null);
-
-        /**
-         * List of 
-         */
-        return TypeUtils.getTypesDef(ImmutableList.<EnumTypeDefinition>of(), 
-        		ImmutableList.<StructTypeDefinition>of(),
-        
-        		ImmutableList.<HierarchicalTypeDefinition<TraitType>>of(),
-        		//ImmutableList.of(dimTraitDef, factTraitDef, piiTraitDef, metricTraitDef, etlTraitDef),
-                
-                ImmutableList.of(transportClsDef, carrierClsDef, routeClsDef, motionClsDef));
-    }
-
-    AttributeDefinition attrDef(String name, IDataType dT) {
-        return attrDef(name, dT, Multiplicity.OPTIONAL, false, null);
-    }
-    
-    AttributeDefinition attrDef(String name, IDataType dT, Multiplicity m) {
-        return attrDef(name, dT, m, false, null);
-    }
-
-    AttributeDefinition attrDef(String name, IDataType dT, Multiplicity m, boolean isComposite,
-            String reverseAttributeName) {
-        Preconditions.checkNotNull(name);
-        Preconditions.checkNotNull(dT);
-        return new AttributeDefinition(name, dT.getName(), m, isComposite, reverseAttributeName);
-    }
-    
-    
-    /**
-     * This creates a new Client
-     * @param referenceable
-     * @return
-     * @throws Exception
-     */
-    private Id createInstance(Referenceable referenceable) throws Exception {
-     
-    	
-    	String typeName = referenceable.getTypeName();
-
-        String entityJSON = InstanceSerialization.toJson(referenceable, true);
-        
-        System.out.println("Submitting new entity= " + entityJSON);
-        
-        JSONObject jsonObject = metadataServiceClient.createEntity(entityJSON);
-        String guid = jsonObject.getString(AtlasClient.GUID);
-        
-        System.out.println("created instance for type " + typeName + ", guid: " + guid);
-
-        // return the Id for created instance with guid
-        return new Id(guid, referenceable.getId().getVersion(), referenceable.getTypeName());
-    }
-    
-    
-    /**
-     * Create Entities for the type definitions.
-     * Types can be class, struct or a Java cas
-     * @throws Exception
-     */
-    
-    void createEntities() throws Exception {
-
-    	Id boeingDB = transport("Boeing 747", "Best Plane in the United States", "James McNeary", "http://wwww.boeing.com");
-    	 
-        Referenceable carrier =
-                carrier("United Airlines", "San Francisco", "USA",
-                        "James McNeary");
-       
-
-       Id carrierId = this.createInstance(carrier);
-    }
-    
-    Id transport(String name, String description, String owner, String locationUri, String... traitNames)
-    	    throws Exception {
-    	        Referenceable referenceable = new Referenceable(this.TRANSPORT_TYPE, traitNames);
-    	        referenceable.set("name", name);
-    	        referenceable.set("description", description);
-    	        referenceable.set("owner", owner);
-    	        referenceable.set("locationUri", locationUri);
-    	        referenceable.set("createTime", System.currentTimeMillis());
-
-    	        return createInstance(referenceable);
-    	    }
-    
-    
-  
-
-    	    Referenceable carrier(String name, String location, String country, String CEO)
-    	    throws Exception {
-    	        Referenceable referenceable = new Referenceable(this.CARRIER_TYPE);
-    	        referenceable.set("name", name);
-    	        referenceable.set("location", location);
-    	        referenceable.set("country", country);
-    	        referenceable.set("CEO", CEO);
-
-    	        return referenceable;
-    	    }
-
-    	    Referenceable route(String name, String route_id, String comment ) throws Exception {
-    	        Referenceable referenceable = new Referenceable(this.ROUTE_TYPE);
-    	        referenceable.set("name", name);
-    	        referenceable.set("dataType", route_id);
-    	        referenceable.set("comment", comment);
-
-    	        return referenceable;
-    	    }
-    	    
-    	    
-    	    Referenceable motion(String rating, String metrics, String comment ) throws Exception {
-    	        Referenceable referenceable = new Referenceable(this.MOTION_TYPE);
-    	        referenceable.set("rating", rating);
-    	        referenceable.set("metrics", metrics);
-    	        referenceable.set("comment", comment);
-
-    	        return referenceable;
-    	    }
-    	    
-    	    
-
-    	  /*  Id table(String name, String description, Id dbId, Referenceable sd, String owner, String tableType,
-    	            List<Referenceable> columns, String... traitNames) throws Exception {
-    	    	
-    	        Referenceable referenceable = new Referenceable(TABLE_TYPE, traitNames);
-    	        referenceable.set("name", name);
-    	        referenceable.set("description", description);
-    	        referenceable.set("owner", owner);
-    	        referenceable.set("tableType", tableType);
-    	        referenceable.set("createTime", System.currentTimeMillis());
-    	        referenceable.set("lastAccessTime", System.currentTimeMillis());
-    	        referenceable.set("retention", System.currentTimeMillis());
-    	        referenceable.set("db", dbId);
-    	        referenceable.set("sd", sd);
-    	        referenceable.set("columns", columns);
-
-    	        return createInstance(referenceable);
-    	    }
-
-    	    Id loadProcess(String name, String description, String user, List<Id> inputTables, List<Id> outputTables,
-    	            String queryText, String queryPlan, String queryId, String queryGraph, String... traitNames)
-    	    throws Exception {
-    	        Referenceable referenceable = new Referenceable(LOAD_PROCESS_TYPE, traitNames);
-    	        // super type attributes
-    	        referenceable.set("name", name);
-    	        referenceable.set("description", description);
-    	        referenceable.set("inputs", inputTables);
-    	        referenceable.set("outputs", outputTables);
-
-    	        referenceable.set("user", user);
-    	        referenceable.set("startTime", System.currentTimeMillis());
-    	        referenceable.set("endTime", System.currentTimeMillis() + 10000);
-
-    	        referenceable.set("queryText", queryText);
-    	        referenceable.set("queryPlan", queryPlan);
-    	        referenceable.set("queryId", queryId);
-    	        referenceable.set("queryGraph", queryGraph);
-
-    	        return createInstance(referenceable);
-    	    }
-    */
-    
-	
-	/*
-	 * This function gets the data
-	 */
-	 static String getServerUrl(String[] args) {
-	        String baseUrl = "http://http://atlas-partner-demo01.cloud.hortonworks.com:21000";
-	        if (args.length > 0) {
-	            baseUrl = args[0];
-	        }
-
-	        System.out.println(baseUrl);
-	        return baseUrl;
-	    }
-	 
-	 
-	 
-	 
-
-}
diff --git a/codesamples/atlas/src/main/java/com/atlas/client/Hierarchy.java b/codesamples/atlas/src/main/java/com/atlas/client/Hierarchy.java
deleted file mode 100644
index 8bc4b80..0000000
--- a/codesamples/atlas/src/main/java/com/atlas/client/Hierarchy.java
+++ /dev/null
@@ -1,8 +0,0 @@
-package com.atlas.client;
-
-public interface Hierarchy<T> {
-	
-	public void parse() throws Exception;
-	
-
-}
diff --git a/codesamples/atlas/src/main/java/com/atlas/client/HierarchyFactory.java b/codesamples/atlas/src/main/java/com/atlas/client/HierarchyFactory.java
deleted file mode 100644
index 2a6c221..0000000
--- a/codesamples/atlas/src/main/java/com/atlas/client/HierarchyFactory.java
+++ /dev/null
@@ -1,10 +0,0 @@
-package com.atlas.client;
-
-public class HierarchyFactory {
-
-	public static void main(String[] args) {
-		// TODO Auto-generated method stub
-
-	}
-
-}
diff --git a/codesamples/atlas/src/main/java/com/atlas/client/HiveMetaDataGenerator.java b/codesamples/atlas/src/main/java/com/atlas/client/HiveMetaDataGenerator.java
deleted file mode 100644
index 495cea2..0000000
--- a/codesamples/atlas/src/main/java/com/atlas/client/HiveMetaDataGenerator.java
+++ /dev/null
@@ -1,563 +0,0 @@
-package com.atlas.client;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-
-import org.apache.atlas.AtlasClient;
-import org.apache.atlas.AtlasServiceException;
-import org.apache.atlas.hive.model.HiveDataModelGenerator;
-import org.apache.atlas.hive.model.HiveDataTypes;
-import org.apache.atlas.typesystem.Referenceable;
-import org.apache.atlas.typesystem.Struct;
-import org.apache.atlas.typesystem.json.InstanceSerialization;
-import org.apache.atlas.typesystem.persistence.Id;
-import org.apache.commons.lang.StringEscapeUtils;
-import org.apache.commons.lang.StringUtils;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.metastore.api.Database;
-import org.apache.hadoop.hive.metastore.api.FieldSchema;
-import org.apache.hadoop.hive.metastore.api.Index;
-import org.apache.hadoop.hive.metastore.api.Order;
-import org.apache.hadoop.hive.metastore.api.SerDeInfo;
-import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
-import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;
-import org.apache.hadoop.hive.ql.metadata.Hive;
-import org.apache.hadoop.hive.ql.metadata.Partition;
-import org.apache.hadoop.hive.ql.metadata.Table;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.codehaus.jettison.json.JSONArray;
-import org.codehaus.jettison.json.JSONException;
-import org.codehaus.jettison.json.JSONObject;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import com.google.common.collect.ImmutableList;
-
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Set;
-
-/**
- * A Bridge Utility that imports metadata from the Hive Meta Store
- * and registers then in Atlas.
- */
-public class HiveMetaDataGenerator {
-	
-	{
-		System.setProperty("atlas.conf", "/Users/sdutta/Applications/conf");
-	}
-	
-	
-    private static final String DEFAULT_DGI_URL = "http://localhost:21000/";
-    
-    public static final String DEFAULT_CLUSTER_NAME = "primary";
-    private static String clusterName = "atlasdemo";
-
-    public static final String DGI_URL_PROPERTY = "hive.hook.dgi.url";
-
-    private static final Logger LOG = LoggerFactory.getLogger(HiveMetaDataGenerator.class);
-
-    private final Hive hiveClient = null;
-    private  AtlasClient atlasClient;
-    private static String databasename = null;
-    private static String tablename = null;
-    
-    public static void main(String[] args) throws Exception {
-    	
-    	clusterName = args[1];
-    	String baseurl = args[0];
-    	databasename = args[2];
-    	tablename = args[3];
-    	
-    	
-    	
-    	HiveMetaDataGenerator hmg = new HiveMetaDataGenerator(baseurl);
-    	
-    	Referenceable db = hmg.registerDatabase(databasename, clusterName);
-    	hmg.registerTable(db, databasename, tablename);
-    	
-    }
-
-    /**
-     * 
-     * @param baseurl
-     */
-    public HiveMetaDataGenerator(String baseurl) {
-    	
-    	atlasClient = new AtlasClient(baseurl);
-    
-    }
-    
-    
-    public AtlasClient getAtlasClient() {
-        return atlasClient;
-    }
-
-
-      
-
-    public Referenceable registerDatabase(String databaseName, String clusterName) throws Exception {
-        Referenceable dbRef = getDatabaseReference(databaseName, clusterName);
-        
-        if (dbRef == null) {
-            LOG.info("Importing objects from databaseName : " + databaseName);
-            //Database hiveDB = hiveClient.getDatabase(databaseName);
-
-            dbRef = new Referenceable(HiveDataTypes.HIVE_DB.getName());
-            dbRef.set(HiveDataModelGenerator.NAME, databaseName);
-            dbRef.set(HiveDataModelGenerator.CLUSTER_NAME, clusterName);
-            dbRef.set("description", "this is a default database");
-            dbRef.set("locationUri", "/hive/default");
-            dbRef.set("parameters", "key1=name1,key2=name2");
-            dbRef.set("ownerName", "Hortonworks");
-            dbRef = createInstance(dbRef);
-            
-            
-        } else {
-            LOG.info("Database {} is already registered with id {}", databaseName, dbRef.getId().id);
-        }
-        
-        return dbRef;
-    }
-
-    public Referenceable createInstance(Referenceable referenceable) throws Exception {
-        String typeName = referenceable.getTypeName();
-        LOG.debug("creating instance of type " + typeName);
-
-        String entityJSON = InstanceSerialization.toJson(referenceable, true);
-        LOG.debug("Submitting new entity {} = {}", referenceable.getTypeName(), entityJSON);
-        JSONObject jsonObject = atlasClient.createEntity(entityJSON);
-        String guid = jsonObject.getString(AtlasClient.GUID);
-        LOG.debug("created instance for type " + typeName + ", guid: " + guid);
-
-        return new Referenceable(guid, referenceable.getTypeName(), null);
-    }
-
-   
-
-    /**
-     * Gets reference for the database
-     *
-     *
-     * @param databaseName  database Name
-     * @param clusterName    cluster name
-     * @return Reference for database if exists, else null
-     * @throws Exception
-     */
-    private Referenceable getDatabaseReference(String databaseName, String clusterName) throws Exception {
-        LOG.debug("Getting reference for database {}", databaseName);
-        String typeName = HiveDataTypes.HIVE_DB.getName();
-
-        String dslQuery = String.format("%s where %s = '%s' and %s = '%s'", typeName, HiveDataModelGenerator.NAME,
-                databaseName.toLowerCase(), HiveDataModelGenerator.CLUSTER_NAME, clusterName);
-        return getEntityReferenceFromDSL(typeName, dslQuery);
-    }
-
-    public Referenceable getProcessReference(String queryStr) throws Exception {
-        LOG.debug("Getting reference for process with query {}", queryStr);
-        String typeName = HiveDataTypes.HIVE_PROCESS.getName();
-
-        //todo enable DSL
-        //        String dslQuery = String.format("%s where queryText = \"%s\"", typeName, queryStr);
-        //        return getEntityReferenceFromDSL(typeName, dslQuery);
-
-        String gremlinQuery =
-                String.format("g.V.has('__typeName', '%s').has('%s.queryText', \"%s\").toList()", typeName, typeName,
-                        StringEscapeUtils.escapeJava(queryStr));
-        return getEntityReferenceFromGremlin(typeName, gremlinQuery);
-    }
-
-    private Referenceable getEntityReferenceFromDSL(String typeName, String dslQuery) throws Exception {
-        AtlasClient dgiClient = getAtlasClient();
-        JSONArray results = dgiClient.searchByDSL(dslQuery);
-        if (results.length() == 0) {
-            return null;
-        } else {
-            String guid;
-            JSONObject row = results.getJSONObject(0);
-            if (row.has("$id$")) {
-                guid = row.getJSONObject("$id$").getString("id");
-            } else {
-                guid = row.getJSONObject("_col_0").getString("id");
-            }
-            return new Referenceable(guid, typeName, null);
-        }
-    }
-
-    public static String getTableName(String clusterName, String dbName, String tableName) {
-        return String.format("%s.%s@%s", dbName.toLowerCase(), tableName.toLowerCase(), clusterName);
-    }
-
-    /**
-     * Gets reference for the table
-     *
-     * @param dbName database name
-     * @param tableName table name
-     * @return table reference if exists, else null
-     * @throws Exception
-     */
-    private Referenceable getTableReference(String dbName, String tableName) throws Exception {
-        LOG.debug("Getting reference for table {}.{}", dbName, tableName);
-
-        String typeName = HiveDataTypes.HIVE_TABLE.getName();
-        String entityName = getTableName(clusterName, dbName, tableName);
-        String dslQuery = String.format("%s as t where name = '%s'", typeName, entityName);
-        return getEntityReferenceFromDSL(typeName, dslQuery);
-    }
-
-    private Referenceable getEntityReferenceFromGremlin(String typeName, String gremlinQuery)
-    throws AtlasServiceException, JSONException {
-        AtlasClient client = getAtlasClient();
-        JSONObject response = client.searchByGremlin(gremlinQuery);
-        JSONArray results = response.getJSONArray(AtlasClient.RESULTS);
-        if (results.length() == 0) {
-            return null;
-        }
-        String guid = results.getJSONObject(0).getString("__guid");
-        return new Referenceable(guid, typeName, null);
-    }
-
-    private Referenceable getPartitionReference(String dbName, String tableName, List<String> values) throws Exception {
-        String valuesStr = "['" + StringUtils.join(values, "', '") + "']";
-        LOG.debug("Getting reference for partition for {}.{} with values {}", dbName, tableName, valuesStr);
-        String typeName = HiveDataTypes.HIVE_PARTITION.getName();
-
-        //todo replace gremlin with DSL
-        //        String dslQuery = String.format("%s as p where values = %s, tableName where name = '%s', "
-        //                        + "dbName where name = '%s' and clusterName = '%s' select p", typeName, valuesStr,
-        // tableName,
-        //                dbName, clusterName);
-
-        String datasetType = AtlasClient.DATA_SET_SUPER_TYPE;
-        String tableEntityName = getTableName(clusterName, dbName, tableName);
-
-        String gremlinQuery = String.format("g.V.has('__typeName', '%s').has('%s.values', %s).as('p')."
-                        + "out('__%s.table').has('%s.name', '%s').back('p').toList()", typeName, typeName, valuesStr,
-                typeName, datasetType, tableEntityName);
-
-        return getEntityReferenceFromGremlin(typeName, gremlinQuery);
-    }
-
-    private Referenceable getSDForTable(String dbName, String tableName) throws Exception {
-        Referenceable tableRef = getTableReference(dbName, tableName);
-        if (tableRef == null) {
-            throw new IllegalArgumentException("Table " + dbName + "." + tableName + " doesn't exist");
-        }
-
-        AtlasClient dgiClient = getAtlasClient();
-        Referenceable tableInstance = dgiClient.getEntity(tableRef.getId().id);
-        Id sdId = (Id) tableInstance.get("sd");
-        return new Referenceable(sdId.id, sdId.getTypeName(), null);
-    }
-
-    /**
-     * 
-     * @param dbName
-     * @param tableName
-     * @return
-     * @throws Exception
-     */
-    public Referenceable registerTable(String dbName, String tableName) throws Exception {
-        Referenceable dbReferenceable = registerDatabase(dbName, clusterName);
-        return registerTable(dbReferenceable, dbName, tableName);
-    }
-
-    
-    /**
-     * 
-     * @param dbReference
-     * @param dbName
-     * @param tableName
-     * @return
-     * @throws Exception
-     */
-    public Referenceable registerTable(Referenceable dbReference, String dbName, String tableName) throws Exception {
-        LOG.info("Attempting to register table [" + tableName + "]");
-        Referenceable tableRef = getTableReference(dbName, tableName);
-        
-        if (tableRef == null) {
-            LOG.info("Importing objects from " + dbName + "." + tableName);
-
-            //Table hiveTable = hiveClient.getTable(dbName, tableName);
-
-            tableRef = new Referenceable(HiveDataTypes.HIVE_TABLE.getName());
-            tableRef.set(HiveDataModelGenerator.NAME,
-                    getTableName(clusterName, dbName, tableName));
-            
-            tableRef.set(HiveDataModelGenerator.TABLE_NAME,tableName.toLowerCase());
-            tableRef.set("owner", "Hortonworks");
-
-            tableRef.set("createTime", System.currentTimeMillis());
-            tableRef.set("lastAccessTime",System.currentTimeMillis());
-            tableRef.set("retention", System.currentTimeMillis());
-
-            tableRef.set(HiveDataModelGenerator.COMMENT, "This is loaded by Sqoop job");
-
-            // add reference to the database
-            tableRef.set(HiveDataModelGenerator.DB, dbReference);
-            
-            List<Referenceable> timeDimColumns = ImmutableList
-                    .of(rawColumn("driver_id", "String", "Driver Id"), rawColumn("driver_name", "String", "Driver Name"),
-                            rawColumn("certified", "String", "certified_Y/N","PII"), rawColumn("wageplan", "String", "hours of weekly"));
-            
-            
-            tableRef.set("columns", timeDimColumns);
-            
-            // add reference to the StorageDescriptor
-            //StorageDescriptor storageDesc = hiveTable.getSd();
-            //Referenceable sdReferenceable = fillStorageDescStruct(storageDesc, colList);
-            //tableRef.set("sd", sdReferenceable);
-
-            // add reference to the Partition Keys
-            //List<Referenceable> partKeys = getColumns(hiveTable.getPartitionKeys());
-            //tableRef.set("partitionKeys", partKeys);
-
-           // tableRef.set("parameters", "params");
-
-            
-            tableRef.set("viewOriginalText", "Original text");
-           
-
-            
-           tableRef.set("viewExpandedText", "Expanded Text");
-            
-
-            tableRef.set("tableType", "Sqoop generated table");
-            tableRef.set("temporary", "false");
-
-
-            tableRef = createInstance(tableRef);
-            
-        } else {
-            LOG.info("Table {}.{} is already registered with id {}", dbName, tableName, tableRef.getId().id);
-        }
-        return tableRef;
-    }
-
-    
-    /**
-     * 
-     * @param db
-     * @param tableName
-     * @param tableReferenceable
-     * @param sdReferenceable
-     * @throws Exception
-     */
-    private void registerPartitions(String db, String tableName, Referenceable tableReferenceable,
-            Referenceable sdReferenceable) throws Exception {
-        Set<Partition> tableParts = hiveClient.getAllPartitionsOf(new Table(Table.getEmptyTable(db, tableName)));
-
-        if (tableParts.size() > 0) {
-            for (Partition hivePart : tableParts) {
-                registerPartition(hivePart, tableReferenceable, sdReferenceable);
-            }
-        }
-    }
-
-    /**
-     * 
-     * @param partition
-     * @return
-     * @throws Exception
-     */
-    public Referenceable registerPartition(Partition partition) throws Exception {
-        String dbName = partition.getTable().getDbName();
-        String tableName = partition.getTable().getTableName();
-        Referenceable tableRef = registerTable(dbName, tableName);
-        Referenceable sdRef = getSDForTable(dbName, tableName);
-        return registerPartition(partition, tableRef, sdRef);
-    }
-
-    private Referenceable registerPartition(Partition hivePart, Referenceable tableReferenceable,
-            Referenceable sdReferenceable) throws Exception {
-        LOG.info("Registering partition for {} with values {}", tableReferenceable,
-                StringUtils.join(hivePart.getValues(), ","));
-        String dbName = hivePart.getTable().getDbName();
-        String tableName = hivePart.getTable().getTableName();
-
-        Referenceable partRef = getPartitionReference(dbName, tableName, hivePart.getValues());
-        if (partRef == null) {
-            partRef = new Referenceable(HiveDataTypes.HIVE_PARTITION.getName());
-            partRef.set("values", hivePart.getValues());
-
-            partRef.set(HiveDataModelGenerator.TABLE, tableReferenceable);
-
-            //todo fix
-            partRef.set("createTime", hivePart.getLastAccessTime());
-            partRef.set("lastAccessTime", hivePart.getLastAccessTime());
-
-            // sdStruct = fillStorageDescStruct(hivePart.getSd());
-            // Instead of creating copies of the sdstruct for partitions we are reusing existing
-            // ones will fix to identify partitions with differing schema.
-            partRef.set("sd", sdReferenceable);
-
-            partRef.set("parameters", hivePart.getParameters());
-            partRef = createInstance(partRef);
-        } else {
-            LOG.info("Partition {}.{} with values {} is already registered with id {}", dbName, tableName,
-                    StringUtils.join(hivePart.getValues(), ","), partRef.getId().id);
-        }
-        return partRef;
-    }
-
-    private void importIndexes(String db, String table, Referenceable dbReferenceable, Referenceable tableReferenceable)
-    throws Exception {
-        List<Index> indexes = hiveClient.getIndexes(db, table, Short.MAX_VALUE);
-        if (indexes.size() > 0) {
-            for (Index index : indexes) {
-                importIndex(index, dbReferenceable, tableReferenceable);
-            }
-        }
-    }
-
-    //todo should be idempotent
-    private void importIndex(Index index, Referenceable dbReferenceable, Referenceable tableReferenceable)
-            throws Exception {
-        LOG.info("Importing index {} for {}.{}", index.getIndexName(), dbReferenceable, tableReferenceable);
-        Referenceable indexRef = new Referenceable(HiveDataTypes.HIVE_INDEX.getName());
-
-        indexRef.set(HiveDataModelGenerator.NAME, index.getIndexName());
-        indexRef.set("indexHandlerClass", index.getIndexHandlerClass());
-
-        indexRef.set(HiveDataModelGenerator.DB, dbReferenceable);
-
-        indexRef.set("createTime", index.getCreateTime());
-        indexRef.set("lastAccessTime", index.getLastAccessTime());
-        indexRef.set("origTable", index.getOrigTableName());
-        indexRef.set("indexTable", index.getIndexTableName());
-
-        Referenceable sdReferenceable = fillStorageDescStruct(index.getSd(), null);
-        indexRef.set("sd", sdReferenceable);
-
-        indexRef.set("parameters", index.getParameters());
-
-        tableReferenceable.set("deferredRebuild", index.isDeferredRebuild());
-
-        createInstance(indexRef);
-    }
-
-    private Referenceable fillStorageDescStruct(StorageDescriptor storageDesc, List<Referenceable> colList)
-    throws Exception {
-        LOG.debug("Filling storage descriptor information for " + storageDesc);
-
-        Referenceable sdReferenceable = new Referenceable(HiveDataTypes.HIVE_STORAGEDESC.getName());
-
-        SerDeInfo serdeInfo = storageDesc.getSerdeInfo();
-        LOG.debug("serdeInfo = " + serdeInfo);
-        // SkewedInfo skewedInfo = storageDesc.getSkewedInfo();
-
-        String serdeInfoName = HiveDataTypes.HIVE_SERDE.getName();
-        Struct serdeInfoStruct = new Struct(serdeInfoName);
-
-        serdeInfoStruct.set(HiveDataModelGenerator.NAME, serdeInfo.getName());
-        serdeInfoStruct.set("serializationLib", serdeInfo.getSerializationLib());
-        serdeInfoStruct.set("parameters", serdeInfo.getParameters());
-
-        sdReferenceable.set("serdeInfo", serdeInfoStruct);
-        sdReferenceable.set(HiveDataModelGenerator.STORAGE_NUM_BUCKETS, storageDesc.getNumBuckets());
-        sdReferenceable
-                .set(HiveDataModelGenerator.STORAGE_IS_STORED_AS_SUB_DIRS, storageDesc.isStoredAsSubDirectories());
-
-        //Use the passed column list if not null, ex: use same references for table and SD
-        List<FieldSchema> columns = storageDesc.getCols();
-        if (columns != null && !columns.isEmpty()) {
-            if (colList != null) {
-                sdReferenceable.set("cols", colList);
-            } else {
-                sdReferenceable.set("cols", getColumns(columns));
-            }
-        }
-
-        List<Struct> sortColsStruct = new ArrayList<Struct>();
-        for (Order sortcol : storageDesc.getSortCols()) {
-            String hiveOrderName = HiveDataTypes.HIVE_ORDER.getName();
-            Struct colStruct = new Struct(hiveOrderName);
-            colStruct.set("col", sortcol.getCol());
-            colStruct.set("order", sortcol.getOrder());
-
-            sortColsStruct.add(colStruct);
-        }
-        if (sortColsStruct.size() > 0) {
-            sdReferenceable.set("sortCols", sortColsStruct);
-        }
-
-        sdReferenceable.set("location", storageDesc.getLocation());
-        sdReferenceable.set("inputFormat", storageDesc.getInputFormat());
-        sdReferenceable.set("outputFormat", storageDesc.getOutputFormat());
-        sdReferenceable.set("compressed", storageDesc.isCompressed());
-
-        if (storageDesc.getBucketCols().size() > 0) {
-            sdReferenceable.set("bucketCols", storageDesc.getBucketCols());
-        }
-
-        sdReferenceable.set("parameters", storageDesc.getParameters());
-        sdReferenceable.set("storedAsSubDirectories", storageDesc.isStoredAsSubDirectories());
-
-        return createInstance(sdReferenceable);
-    }
-
-    private List<Referenceable> getColumns(List<FieldSchema> schemaList) throws Exception {
-        List<Referenceable> colList = new ArrayList<Referenceable>();
-        for (FieldSchema fs : schemaList) {
-            LOG.debug("Processing field " + fs);
-            Referenceable colReferenceable = new Referenceable(HiveDataTypes.HIVE_COLUMN.getName());
-            colReferenceable.set(HiveDataModelGenerator.NAME, fs.getName());
-            colReferenceable.set("type", fs.getType());
-            colReferenceable.set(HiveDataModelGenerator.COMMENT, fs.getComment());
-
-            colList.add(createInstance(colReferenceable));
-        }
-        return colList;
-    }
-
-    public synchronized void registerHiveDataModel() throws Exception {
-        HiveDataModelGenerator dataModelGenerator = new HiveDataModelGenerator();
-        AtlasClient dgiClient = getAtlasClient();
-
-        //Register hive data model if its not already registered
-        if (dgiClient.getType(HiveDataTypes.HIVE_PROCESS.getName()) == null) {
-            LOG.info("Registering Hive data model");
-            dgiClient.createType(dataModelGenerator.getModelAsJson());
-        } else {
-            LOG.info("Hive data model is already registered!");
-        }
-    }
-
-    
-
-	Referenceable rawColumn(String name, String dataType, String comment, String... traitNames) throws Exception {
-	        Referenceable referenceable = new Referenceable(HiveDataTypes.HIVE_COLUMN.getName(), traitNames);
-	        referenceable.set("name", name);
-	        referenceable.set("type", dataType);
-	        referenceable.set("comment", comment);
-
-	        return referenceable;
-	    }
-	  
-	
-   
-    public void updateTable(Referenceable tableReferenceable, Table newTable) throws AtlasServiceException {
-        AtlasClient client = getAtlasClient();
-        client.updateEntity(tableReferenceable.getId()._getId(), HiveDataModelGenerator.TABLE_NAME,
-                newTable.getTableName().toLowerCase());
-        client.updateEntity(tableReferenceable.getId()._getId(), HiveDataModelGenerator.NAME,
-                getTableName(clusterName, newTable.getDbName(), newTable.getTableName()));
-    }
-}
\ No newline at end of file
diff --git a/codesamples/atlas/src/main/java/com/atlas/client/JsonHierarchy.java b/codesamples/atlas/src/main/java/com/atlas/client/JsonHierarchy.java
deleted file mode 100644
index a914ae8..0000000
--- a/codesamples/atlas/src/main/java/com/atlas/client/JsonHierarchy.java
+++ /dev/null
@@ -1,287 +0,0 @@
-/**
- *This is a generic JSON Parser
- * 
- */
-package com.atlas.client;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.ListIterator;
-import org.codehaus.jackson.JsonFactory;
-import org.codehaus.jackson.JsonParseException;
-import org.codehaus.jackson.JsonParser;
-import org.codehaus.jackson.JsonToken;
-import com.hortonworks.atlas.adapter.EntityModel;
-import com.hortonworks.atlas.adapter.TupleModel;
-
-/** 
- * @author sdutta
- *
- */
-public class JsonHierarchy {
-
-	public static String path = null;
-
-	JsonParser jParser;
-	private int i = 0;
-	JsonToken jtk = null;
-
-	ArrayList<TupleModel> tmapList = new ArrayList<TupleModel>();
-
-	ArrayList<TupleModel> rotatingList = new ArrayList<TupleModel>();
-
-	boolean hitarray = false;
-	int hitarray_index = 0;
-	private ArrayList<EntityModel> emList = new ArrayList<EntityModel>();
-	EntityModel EM = null;
-
-
-
-
-	/**
-	 * 
-	 * @param args
-	 * @throws JsonParseException
-	 * @throws IOException
-	 */
-	public static void main(String[] args) throws JsonParseException,
-			IOException {
-
-		path = "/Users/sdutta/DGI/codesamples/atlas/TestHierarchy.json";
-		JsonHierarchy jsn = new JsonHierarchy();
-		jsn.parseJSON(path);
-
-	}
-
-	
-	
-	/**
-	 * 
-	 * @param filepath
-	 * @throws JsonParseException
-	 * @throws IOException
-	 */
-	public void parseJSON(String filepath) throws JsonParseException,
-			IOException {
-
-		JsonFactory jfac = new JsonFactory();
-		jParser = jfac.createJsonParser(new File(filepath));
-
-		buildTrees(jParser, null, null);
-
-		ListIterator<TupleModel> tmI = tmapList.listIterator();
-		TupleModel tm = null;
-
-		while (tmI.hasNext()) {
-
-			tm = tmI.next();
-			System.out.println(tm.getCurrnode() + "--" + tm.getParentnode()
-					+ " -- " + tm.getLevel());
-		}
-
-		System.out.println("Printing entities");
-		ListIterator<EntityModel> emItr = this.emList.listIterator();
-		EntityModel em1 = null;
-
-		while (emItr.hasNext()) {
-
-			em1 = emItr.next();
-
-			System.out.println(em1.getType() + "--" + em1.getName() + " -- "
-					+ em1.getParent());
-		}
-
-	}
-
-	
-	public ArrayList<EntityModel> getEmList() {
-		return emList;
-	}
-
-	public ArrayList<TupleModel> getTmapList() {
-		return tmapList;
-	}
-
-	/**
-	 * 
-	 * use push and pop method to get to the parent
-	 * 
-	 * @param node
-	 * @param pnode
-	 * @throws IOException
-	 * @throws JsonParseException
-	 */
-	public void buildTrees(JsonParser jParser, String currnode,
-			String parentnode) throws JsonParseException, IOException {
-
-		jtk = jParser.nextToken();
-
-		if (jParser.hasCurrentToken()) {
-
-			if (jtk == JsonToken.FIELD_NAME && !hitarray) {
-
-				currnode = jParser.getCurrentName();
-
-				currnode = jParser.getCurrentName();
-
-				TupleModel tm = new TupleModel(currnode, parentnode);
-				
-				tm.setLevel(i);
-				tmapList.add(tm);
-				this.pushTuple(tm);
-				jtk = jParser.nextToken();
-
-			}
-
-			if (hitarray && jParser.getText() == "{") {
-				EM = new EntityModel();
-				EM.setParent(parentnode);
-			}
-
-			if (hitarray && jParser.getText() == "type") {
-				currnode = "type";
-				jtk = jParser.nextToken();
-				if (jtk.isScalarValue()) {
-					EM.setType(jParser.getText());
-				}
-
-			}
-
-			if (hitarray && jParser.getText() == "name") {
-				currnode = "name";
-				jtk = jParser.nextToken();
-				if (jtk.isScalarValue()) {
-					EM.setName(jParser.getText());
-				}
-
-			}
-
-			if (hitarray && jParser.getText() == "}") {
-				this.emList.add(EM);
-				EM = null;
-			}
-
-			if (jParser.getText() == "[") {
-
-				// parentnode = currnode;
-				hitarray = true;
-				hitarray_index++;
-
-			} else if (jParser.getText() == "]") {
-
-				hitarray_index--;
-				if (hitarray_index == 0)
-					hitarray = false;
-
-			}
-
-			if (jParser.getText() == "{" && !hitarray) {
-
-				i++;
-
-				parentnode = currnode;
-
-			} else if (jParser.getText() == "}" && !hitarray) {
-
-				this.popTupleModel();
-
-				TupleModel tm = this.rotatingList
-						.get(this.rotatingList.size() - 1);
-				parentnode = tm.getParentnode();
-
-			}
-
-			buildTrees(jParser, currnode, parentnode);
-
-		}
-
-	}
-
-	/**
-	 * This is to handle the Entities
-	 * 
-	 * @param jParser
-	 * @param parentnode
-	 * @throws JsonParseException
-	 * @throws IOException
-	 */
-	public void processEntities(JsonParser jParser, String parentnode,
-			EntityModel em) throws JsonParseException, IOException {
-
-		jtk = jParser.getCurrentToken();
-		if (jtk == JsonToken.FIELD_NAME) {
-
-			String currnode = jParser.getCurrentName();
-
-			em.setParent(parentnode);
-
-			jParser.nextToken();
-
-			if (jtk.isScalarValue()) {
-
-				if ("type".equalsIgnoreCase(currnode))
-					em.setType(currnode);
-				else if ("name".equalsIgnoreCase(currnode))
-					em.setName(currnode);
-
-			}
-		}
-
-		if (jParser.getText() == "[") {
-
-			jParser.nextToken();
-			processEntities(jParser, parentnode, em);
-
-		}
-
-		if (jParser.getText() == "{") {
-
-			jParser.nextToken();
-			EM = new EntityModel();
-			processEntities(jParser, parentnode, EM);
-
-		}
-
-		if (jParser.getText() == "}") {
-
-			jParser.nextToken();
-			emList.add(em);
-			processEntities(jParser, parentnode, EM);
-
-		}
-
-		if (jParser.getText() == "[") {
-
-			jParser.nextToken();
-			processEntities(jParser, parentnode, em);
-
-		}
-
-		if (jParser.getText() == "]") {
-
-		}
-
-	}
-
-	/**
-	 * 
-	 * push the last entry in
-	 * 
-	 * @param tupMd
-	 */
-	private void pushTuple(TupleModel tupMd) {
-
-		this.rotatingList.add(tupMd);
-	}
-
-	/**
-	 * pop the last entry out
-	 * 
-	 */
-	private void popTupleModel() {
-
-		this.rotatingList.remove(this.rotatingList.size() - 1);
-	}
-
-}
diff --git a/codesamples/atlas/src/main/java/com/atlas/client/MySqlIngester.java b/codesamples/atlas/src/main/java/com/atlas/client/MySqlIngester.java
deleted file mode 100644
index d3fd4cc..0000000
--- a/codesamples/atlas/src/main/java/com/atlas/client/MySqlIngester.java
+++ /dev/null
@@ -1,597 +0,0 @@
-package com.atlas.client;
-
-import com.google.common.base.Preconditions;
-import com.google.common.collect.ImmutableList;
-import com.sun.jersey.api.client.Client;
-import com.sun.jersey.api.client.ClientResponse;
-import com.sun.jersey.api.client.WebResource;
-import com.sun.jersey.api.client.config.DefaultClientConfig;
-import com.sun.jersey.client.urlconnection.URLConnectionClientHandler;
-import org.apache.atlas.AtlasClient;
-
-import org.apache.atlas.hive.model.HiveDataModelGenerator;
-import org.apache.atlas.hive.model.HiveDataTypes;
-import org.apache.atlas.security.SecureClientUtils;
-import org.apache.atlas.typesystem.Referenceable;
-import org.apache.atlas.typesystem.json.InstanceSerialization;
-import org.apache.atlas.typesystem.json.TypesSerialization;
-import org.apache.atlas.typesystem.persistence.Id;
-import org.apache.atlas.typesystem.types.*;
-import org.apache.atlas.typesystem.types.utils.TypesUtil;
-import org.apache.atlas.typesystem.TypesDef;
-import org.apache.commons.configuration.PropertiesConfiguration;
-import org.apache.hadoop.hive.metastore.api.Database;
-import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
-import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;
-import org.apache.hadoop.hive.ql.metadata.Table;
-import org.codehaus.jettison.json.JSONArray;
-import org.codehaus.jettison.json.JSONException;
-import org.codehaus.jettison.json.JSONObject;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import javax.ws.rs.HttpMethod;
-import javax.ws.rs.core.MediaType;
-import javax.ws.rs.core.Response;
-import javax.ws.rs.core.UriBuilder;
-
-import java.util.ArrayList;
-import java.util.Calendar;
-import java.util.List;
-
-/**
- * This is for loading data
- * 
- * @author sdutta
- *
- */
-public class MySqlIngester {
-
-	static Logger LOG = LoggerFactory.getLogger(MySqlIngester.class);
-
-	private static final String LOAD_PROCESS_TYPE = "LoadProcess";
-	private static final String STORAGE_DESC_TYPE = "StorageDesc";
-	private static final String MYSQL_TABLE_TYPE = "demotable_type10";
-	private static final String Sqoop_TYPE = "Sqoop_Process_Type2";
-	private static final String Falcon_Type = "Falcon_Type";
-	private static final String DATABASE_TYPE = "DB";
-	private static final String COLUMN_TYPE = "Column";
-	private static final String TABLE_TYPE = "Table";
-	private static final String VIEW_TYPE = "View";
-
-	private final AtlasClient metadataServiceClient;
-
-	public MySqlIngester(String baseurl) {
-
-		this.metadataServiceClient = new AtlasClient(baseurl);
-	}
-
-	private String clustername = null;
-
-	public static void main(String[] args) throws Exception {
-		// TODO Auto-generated method stub
-
-		if (args.length < 1) {
-			throw new Exception("Please provide the DGI host url");
-		}
-
-		System.setProperty("atlas.conf", "/Users/sdutta/Applications/conf");
-
-		String baseUrl = getServerUrl(args);
-
-		MySqlIngester sqlIngester = new MySqlIngester(baseUrl);
-
-		sqlIngester.createTypes();
-		System.out.println("Creating Entitites");
-		sqlIngester.createEntities("testers", "this is data being laoded",
-				"TestDB");
-
-	}
-
-	/*
-	 * This method creates a Type
-	 */
-
-	void createTypes() throws Exception {
-
-		TypesDef typesDef = this.createMysqlTypes();
-
-		String typesAsJSON = TypesSerialization.toJson(typesDef);
-
-		System.out.println("typesAsJSON = " + typesAsJSON);
-
-		
-	  this.metadataServiceClient.createType(typesAsJSON);
-
-		System.out.println("MySQL Type System Created");
-
-	}
-
-	/**
-	 * This creates of MysqlType
-	 * 
-	 * @return
-	 * @throws Exception
-	 */
-	public TypesDef createMysqlTypes() throws Exception {
-
-		HierarchicalTypeDefinition<ClassType> mysqlTable = TypesUtil
-				.createClassTypeDef(this.MYSQL_TABLE_TYPE, null, this.attrDef("name",
-						DataTypes.STRING_TYPE), this.attrDef("description",
-						DataTypes.STRING_TYPE), this.attrDef("sourceDB",
-						DataTypes.STRING_TYPE),  this.attrDef("destinationDB",
-								DataTypes.STRING_TYPE));
-
-		HierarchicalTypeDefinition<ClassType> sqoopProcess = TypesUtil
-				.createClassTypeDef(
-						this.Sqoop_TYPE,
-						ImmutableList.of("Process"),
-						attrDef("command", DataTypes.STRING_TYPE));
-/*
-		HierarchicalTypeDefinition<ClassType> falconProcess = TypesUtil
-				.createClassTypeDef(
-						this.Falcon_Type,
-						ImmutableList.of("Process"),
-						attrDef("entityName", DataTypes.STRING_TYPE,
-								Multiplicity.REQUIRED));
-
-		HierarchicalTypeDefinition<TraitType> dimTraitDef = TypesUtil
-				.createTraitTypeDef("Dimension", null);
-
-		HierarchicalTypeDefinition<TraitType> factTraitDef = TypesUtil
-				.createTraitTypeDef("Fact", null);
-
-		HierarchicalTypeDefinition<TraitType> piiTraitDef = TypesUtil
-				.createTraitTypeDef("PII", null);
-
-		HierarchicalTypeDefinition<TraitType> metricTraitDef = TypesUtil
-				.createTraitTypeDef("Metric", null);
-
-		HierarchicalTypeDefinition<TraitType> etlTraitDef = TypesUtil
-				.createTraitTypeDef("ETL", null);
-
-		HierarchicalTypeDefinition<TraitType> valueTraitDef = TypesUtil
-				.createTraitTypeDef("Value", null);
-*/
-		return TypeUtils.getTypesDef(ImmutableList.<EnumTypeDefinition> of(),
-				ImmutableList.<StructTypeDefinition> of(),
-				ImmutableList.<HierarchicalTypeDefinition<TraitType>>of(),
-				//ImmutableList.of(dimTraitDef, factTraitDef, piiTraitDef,
-					//	metricTraitDef, etlTraitDef, valueTraitDef),
-
-				ImmutableList.of(mysqlTable,sqoopProcess));
-
-	}
-
-	/**
-	 * 
-	 * @param name
-	 * @param dataType
-	 * @param comment
-	 * @param traitNames
-	 * @return
-	 * @throws Exception
-	 */
-	Referenceable rawColumn(String name, String dataType, String comment,
-			String... traitNames) throws Exception {
-		Referenceable referenceable = new Referenceable(COLUMN_TYPE, traitNames);
-		referenceable.set("name", name);
-		referenceable.set("dataType", dataType);
-		referenceable.set("comment", comment);
-
-		return referenceable;
-	}
-
-	/**
-	 * 
-	 * @param tablename
-	 * @param tabledescription
-	 * @param sourceDB
-	 * @throws Exception
-	 */
-	public void createEntities(String tablename, String tabledescription,
-			String sourceDB) throws Exception {
-
-		// Id salesDB = database("Sales", "Sales Database", "John ETL",
-		// "hdfs://host:8000/apps/warehouse/sales");
-
-		// Referenceable sd =
-		// rawStorageDescriptor("hdfs://host:8000/apps/warehouse/sales",
-		// "TextInputFormat", "TextOutputFormat",
-		// true);
-
-		List<Referenceable> salesFactColumns = ImmutableList.of(
-				rawColumn("time_id", "int", "time id"),
-				rawColumn("product_id", "int", "product id"),
-				rawColumn("customer_id", "int", "customer id", "PII"),
-				rawColumn("sales", "double", "product id", "Metric"));
-
-		Id mysqlFact = mysqltable(tablename + "source", sourceDB, tabledescription, "Hive");
-
-		//Id hivetable = registerTable("default", tablename + "_hive");
-		
-		Id hivetable = mysqltable(tablename + "destination", sourceDB, tabledescription, "Hive");
-
-		loadProcess("sqlingestion", "mysql ingestion of data - Sqoop Process",
-				ImmutableList.of(mysqlFact), ImmutableList.of(hivetable),
-				"PII", "ETL");
-
-	}
-
-	/**
-	 *
-	 */
-	Id mysqltable(String name, String sourcedb, String description,
-			String destdb, String... traitNames) throws Exception {
-		Referenceable referenceable = new Referenceable(this.MYSQL_TABLE_TYPE,
-				traitNames);
-		referenceable.set("name", name);
-		referenceable.set("description", description);
-		referenceable.set("sourcedb", sourcedb);
-		referenceable.set("destinationdb", destdb);
-
-		return createInstance(referenceable);
-	}
-
-	/*
-	 * Id table(String name, String description, Id dbId, Referenceable sd,
-	 * String owner, String tableType, List<Referenceable> columns, String...
-	 * traitNames) throws Exception {
-	 * 
-	 * Referenceable referenceable = new Referenceable(TABLE_TYPE, traitNames);
-	 * referenceable.set("name", name); referenceable.set("description",
-	 * description); referenceable.set("owner", owner);
-	 * referenceable.set("tableType", tableType);
-	 * referenceable.set("createTime", System.currentTimeMillis());
-	 * referenceable.set("lastAccessTime", System.currentTimeMillis());
-	 * referenceable.set("retention", System.currentTimeMillis());
-	 * referenceable.set("db", dbId); referenceable.set("sd", sd);
-	 * referenceable.set("columns", columns);
-	 * 
-	 * return createInstance(referenceable); }
-	 */
-
-	Id loadProcess(String name, String description, List<Id> inputTables,
-			List<Id> outputTables, String... traitNames) throws Exception {
-		Referenceable referenceable = new Referenceable(this.Sqoop_TYPE);
-		// super type attributes
-	//	referenceable.set("entityName", name);
-		//referenceable.set("entityDescription", description);
-		referenceable.set("inputs", inputTables);
-		referenceable.set("outputs", outputTables);
-		referenceable.set("command", "sqoop -import ....");
-
-		return createInstance(referenceable);
-	}
-
-	/**
-	 * This will register the DB
-	 * 
-	 */
-	public Referenceable registerDatabase(String databaseName,
-			String clusterName, String hiveDBName, String HiveDBDescription,
-			String location, String parameter, String owner) throws Exception {
-
-		Referenceable dbRef = new Referenceable(HiveDataTypes.HIVE_DB.getName());
-		
-		dbRef.set(HiveDataModelGenerator.NAME, hiveDBName.toLowerCase());
-		dbRef.set(HiveDataModelGenerator.CLUSTER_NAME, clusterName);
-		dbRef.set("description", HiveDBDescription);
-		dbRef.set("locationUri", location);
-		dbRef.set("parameters", parameter);
-		dbRef.set("ownerName", owner);
-
-		dbRef = createInstance2(dbRef);
-
-		return dbRef;
-	}
-
-	
-	
-	/**
-	 * 
-	 * @param dbRef
-	 * @param dbName
-	 * @param tableName
-	 * @return
-	 * @throws Exception
-	 */
-	public Referenceable registerTable(String dbRef, String dbName,
-			String tableName) throws Exception {
-
-		return registerTable(dbRef, dbName, tableName);
-	}
-
-	
-	
-	/*
-	 * private Referenceable getTableReference(String dbName, String tableName)
-	 * throws Exception { LOG.debug("Getting reference for table {}.{}", dbName,
-	 * tableName);
-	 * 
-	 * String typeName = HiveDataTypes.HIVE_TABLE.getName(); String entityName =
-	 * getTableName(clusterName, dbName, tableName); String dslQuery =
-	 * String.format("%s as t where name = '%s'", typeName, entityName); return
-	 * getEntityReferenceFromDSL(typeName, dslQuery); }
-	 */
-
-	public Id registerTable(String dbName, String tableName) throws Exception {
-		LOG.info("Attempting to register table [" + tableName + "]");
-
-		Referenceable tableRef = null;
-
-		LOG.info("Importing objects from " + dbName + "." + tableName);
-
-		// Table hiveTable = tableName;
-
-		tableRef = new Referenceable(HiveDataTypes.HIVE_TABLE.getName());
-		tableRef.set(HiveDataModelGenerator.NAME, tableName);
-		tableRef.set(HiveDataModelGenerator.TABLE_NAME, tableName.toLowerCase());
-		tableRef.set("owner", "HWX");
-
-		tableRef.set("createTime", Calendar.getInstance().getTime()
-				.toLocaleString());
-		tableRef.set("lastAccessTime", "10");
-		tableRef.set("retention", "10");
-
-		tableRef.set(HiveDataModelGenerator.COMMENT,
-				"This Table is generated by SQL Ingenstion");
-
-		
-		 // add reference to the database
-		 tableRef.set(HiveDataModelGenerator.DB, "Default");
-		 
-		 //List<Referenceable> colList = getColumns(hiveTable.getCols());
-		  tableRef.set("columns", ImmutableList.of("driver_id", "driver_name"));
-		  
-		  // add reference to the StorageDescriptor StorageDescriptor
-		  //storageDesc = hiveTable.getSd(); 
-		  
-		  //Referenceable sdReferenceable =
-		  //fillStorageDescStruct(storageDesc, colList); 
-		  
-		  tableRef.set("sd",
-		  "table");
-		 
-		  // add reference to the Partition Keys List<Referenceable> partKeys =
-		 // getColumns(hiveTable.getPartitionKeys());
-		  tableRef.set("partitionKeys", "driverid");
-		 
-		  tableRef.set("parameters", "noparam");
-		  
-		 // if (hiveTable.getViewOriginalText() != null) {
-			  tableRef.set("viewOriginalText", "original text"); 
-		  
-		  //if (hiveTable.getViewExpandedText() != null) {
-			  tableRef.set("viewExpandedText", "expanded view"); 
-		 
-		tableRef.set("tableType", HiveDataTypes.HIVE_TABLE.getName());
-		tableRef.set("temporary", false);
-
-		Id id = createInstance(tableRef);
-
-		return id;
-	}
-
-	/*
-	 * Creates a Reference and returns an Instance
-	 */
-	public Referenceable createInstance2(Referenceable referenceable)
-			throws Exception {
-		String typeName = referenceable.getTypeName();
-		LOG.debug("creating instance of type " + typeName);
-
-		String entityJSON = InstanceSerialization.toJson(referenceable, true);
-		LOG.debug("Submitting new entity {} = {}", referenceable.getTypeName(),
-				entityJSON);
-		JSONObject jsonObject = this.metadataServiceClient
-				.createEntity(entityJSON);
-		String guid = jsonObject.getString(AtlasClient.GUID);
-		LOG.debug("created instance for type " + typeName + ", guid: " + guid);
-
-		return new Referenceable(guid, referenceable.getTypeName(), null);
-	}
-
-	/*
-	 * TypesDef createTypeDefinitions() throws Exception {
-	 * 
-	 * 
-	 * HierarchicalTypeDefinition<ClassType> transportClsDef = TypesUtil
-	 * .createClassTypeDef(this.TRANSPORT_TYPE, null, attrDef("name",
-	 * DataTypes.STRING_TYPE), attrDef("description", DataTypes.STRING_TYPE),
-	 * attrDef("locationUri", DataTypes.STRING_TYPE), attrDef("owner",
-	 * DataTypes.STRING_TYPE), attrDef("createTime", DataTypes.INT_TYPE));
-	 * 
-	 * HierarchicalTypeDefinition<ClassType> carrierClsDef = TypesUtil
-	 * .createClassTypeDef(this.CARRIER_TYPE, null, attrDef("name",
-	 * DataTypes.STRING_TYPE), attrDef("location", DataTypes.STRING_TYPE),
-	 * attrDef("country", DataTypes.STRING_TYPE), attrDef("CEO",
-	 * DataTypes.STRING_TYPE) );
-	 * 
-	 * HierarchicalTypeDefinition<ClassType> routeClsDef = TypesUtil
-	 * .createClassTypeDef(this.ROUTE_TYPE, null, attrDef("name",
-	 * DataTypes.STRING_TYPE), attrDef("route_id", DataTypes.STRING_TYPE),
-	 * attrDef("comment", DataTypes.STRING_TYPE));
-	 * 
-	 * HierarchicalTypeDefinition<ClassType> motionClsDef = TypesUtil
-	 * .createClassTypeDef(this.MOTION_TYPE, null, attrDef("rating",
-	 * DataTypes.STRING_TYPE), attrDef("metrics", DataTypes.STRING_TYPE),
-	 * attrDef("comment", DataTypes.STRING_TYPE));
-	 * 
-	 * 
-	 * HierarchicalTypeDefinition<TraitType> dimTraitDef =
-	 * TypesUtil.createTraitTypeDef("Dimension", null);
-	 * 
-	 * HierarchicalTypeDefinition<TraitType> factTraitDef =
-	 * TypesUtil.createTraitTypeDef("Fact", null);
-	 * 
-	 * HierarchicalTypeDefinition<TraitType> piiTraitDef =
-	 * TypesUtil.createTraitTypeDef("PII", null);
-	 * 
-	 * HierarchicalTypeDefinition<TraitType> metricTraitDef =
-	 * TypesUtil.createTraitTypeDef("Metric", null);
-	 * 
-	 * HierarchicalTypeDefinition<TraitType> etlTraitDef =
-	 * TypesUtil.createTraitTypeDef("ETL", null);
-	 * 
-	 * return TypeUtils.getTypesDef(ImmutableList.<EnumTypeDefinition>of(),
-	 * ImmutableList.<StructTypeDefinition>of(),
-	 * 
-	 * ImmutableList.<HierarchicalTypeDefinition<TraitType>>of(),
-	 * //ImmutableList.of(dimTraitDef, factTraitDef, piiTraitDef,
-	 * metricTraitDef, etlTraitDef),
-	 * 
-	 * ImmutableList.of(transportClsDef, carrierClsDef, routeClsDef,
-	 * motionClsDef)); }
-	 */
-
-	AttributeDefinition attrDef(String name, IDataType dT) {
-		return attrDef(name, dT, Multiplicity.OPTIONAL, false, null);
-	}
-
-	AttributeDefinition attrDef(String name, IDataType dT, Multiplicity m) {
-		return attrDef(name, dT, m, false, null);
-	}
-
-	AttributeDefinition attrDef(String name, IDataType dT, Multiplicity m,
-			boolean isComposite, String reverseAttributeName) {
-		Preconditions.checkNotNull(name);
-		Preconditions.checkNotNull(dT);
-		return new AttributeDefinition(name, dT.getName(), m, isComposite,
-				reverseAttributeName);
-	}
-
-	/**
-	 * This creates a new Client
-	 * 
-	 * @param referenceable
-	 * @return
-	 * @throws Exception
-	 */
-	private Id createInstance(Referenceable referenceable) throws Exception {
-
-		String typeName = referenceable.getTypeName();
-
-		String entityJSON = InstanceSerialization.toJson(referenceable, true);
-
-		System.out.println("Submitting new entity= " + entityJSON);
-
-		JSONObject jsonObject = metadataServiceClient.createEntity(entityJSON);
-		String guid = jsonObject.getString(AtlasClient.GUID);
-
-		System.out.println("created instance for type " + typeName + ", guid: "
-				+ guid);
-
-		// return the Id for created instance with guid
-		return new Id(guid, referenceable.getId().getVersion(),
-				referenceable.getTypeName());
-	}
-
-	/**
-	 * Create Entities for the type definitions. Types can be class, struct or a
-	 * Java cas
-	 * 
-	 * @throws Exception
-	 */
-	/*
-	 * void createEntities() throws Exception {
-	 * 
-	 * Id boeingDB = transport("Boeing 747", "Best Plane in the United States",
-	 * "James McNeary", "http://wwww.boeing.com");
-	 * 
-	 * Referenceable carrier = carrier("United Airlines", "San Francisco",
-	 * "USA", "James McNeary");
-	 * 
-	 * Id carrierId = this.createInstance(carrier); }
-	 */
-	/*
-	 * Id transport(String name, String description, String owner, String
-	 * locationUri, String... traitNames) throws Exception { Referenceable
-	 * referenceable = new Referenceable(this.TRANSPORT_TYPE, traitNames);
-	 * referenceable.set("name", name); referenceable.set("description",
-	 * description); referenceable.set("owner", owner);
-	 * referenceable.set("locationUri", locationUri);
-	 * referenceable.set("createTime", System.currentTimeMillis());
-	 * 
-	 * return createInstance(referenceable); }
-	 * 
-	 * 
-	 * 
-	 * 
-	 * Referenceable carrier(String name, String location, String country,
-	 * String CEO) throws Exception { Referenceable referenceable = new
-	 * Referenceable(this.CARRIER_TYPE); referenceable.set("name", name);
-	 * referenceable.set("location", location); referenceable.set("country",
-	 * country); referenceable.set("CEO", CEO);
-	 * 
-	 * return referenceable; }
-	 * 
-	 * Referenceable route(String name, String route_id, String comment ) throws
-	 * Exception { Referenceable referenceable = new
-	 * Referenceable(this.ROUTE_TYPE); referenceable.set("name", name);
-	 * referenceable.set("dataType", route_id); referenceable.set("comment",
-	 * comment);
-	 * 
-	 * return referenceable; }
-	 * 
-	 * 
-	 * Referenceable motion(String rating, String metrics, String comment )
-	 * throws Exception { Referenceable referenceable = new
-	 * Referenceable(this.MOTION_TYPE); referenceable.set("rating", rating);
-	 * referenceable.set("metrics", metrics); referenceable.set("comment",
-	 * comment);
-	 * 
-	 * return referenceable; }
-	 */
-
-	/*
-	 * Id table(String name, String description, Id dbId, Referenceable sd,
-	 * String owner, String tableType, List<Referenceable> columns, String...
-	 * traitNames) throws Exception {
-	 * 
-	 * Referenceable referenceable = new Referenceable(TABLE_TYPE, traitNames);
-	 * referenceable.set("name", name); referenceable.set("description",
-	 * description); referenceable.set("owner", owner);
-	 * referenceable.set("tableType", tableType);
-	 * referenceable.set("createTime", System.currentTimeMillis());
-	 * referenceable.set("lastAccessTime", System.currentTimeMillis());
-	 * referenceable.set("retention", System.currentTimeMillis());
-	 * referenceable.set("db", dbId); referenceable.set("sd", sd);
-	 * referenceable.set("columns", columns);
-	 * 
-	 * return createInstance(referenceable); }
-	 * 
-	 * Id loadProcess(String name, String description, String user, List<Id>
-	 * inputTables, List<Id> outputTables, String queryText, String queryPlan,
-	 * String queryId, String queryGraph, String... traitNames) throws Exception
-	 * { Referenceable referenceable = new Referenceable(LOAD_PROCESS_TYPE,
-	 * traitNames); // super type attributes referenceable.set("name", name);
-	 * referenceable.set("description", description);
-	 * referenceable.set("inputs", inputTables); referenceable.set("outputs",
-	 * outputTables);
-	 * 
-	 * referenceable.set("user", user); referenceable.set("startTime",
-	 * System.currentTimeMillis()); referenceable.set("endTime",
-	 * System.currentTimeMillis() + 10000);
-	 * 
-	 * referenceable.set("queryText", queryText); referenceable.set("queryPlan",
-	 * queryPlan); referenceable.set("queryId", queryId);
-	 * referenceable.set("queryGraph", queryGraph);
-	 * 
-	 * return createInstance(referenceable); }
-	 */
-
-	/*
-	 * This function gets the data
-	 */
-	static String getServerUrl(String[] args) {
-		String baseUrl = "http://atlasdemo.cloud.hortonworks.com:21000";
-		if (args.length > 0) {
-			baseUrl = args[0];
-		}
-
-		System.out.println(baseUrl);
-		return baseUrl;
-	}
-
-}
diff --git a/codesamples/atlas/src/main/java/com/atlas/client/NewAtlasClient.java b/codesamples/atlas/src/main/java/com/atlas/client/NewAtlasClient.java
deleted file mode 100644
index 5669f72..0000000
--- a/codesamples/atlas/src/main/java/com/atlas/client/NewAtlasClient.java
+++ /dev/null
@@ -1,153 +0,0 @@
-package com.atlas.client;
-
-import java.util.ArrayList;
-import java.util.List;
-
-import javax.ws.rs.HttpMethod;
-import javax.ws.rs.core.MediaType;
-import javax.ws.rs.core.Response;
-import javax.ws.rs.core.UriBuilder;
-
-import org.apache.atlas.AtlasClient;
-import org.apache.atlas.AtlasServiceException;
-import org.codehaus.jettison.json.JSONArray;
-import org.codehaus.jettison.json.JSONException;
-import org.codehaus.jettison.json.JSONObject;
-
-import com.sun.jersey.api.client.ClientResponse;
-import com.sun.jersey.api.client.WebResource;
-
-public class NewAtlasClient extends AtlasClient {
-
-	 private WebResource service;
-	
-	public NewAtlasClient(String baseurl) {
-		super(baseurl);
-
-	}
-	
-	/**
-     * Return all trait names for the given entity id
-     * @param guid
-     * @return
-	 * @throws Exception 
-     */
-    public List<String> getTraitNames(String guid) throws Exception {
-        WebResource resource = getResource(API.LIST_TRAITS, guid, URI_TRAITS);
-        JSONObject response = callAPIWithResource(API.LIST_TRAITS, resource);
-        return extractResults(response);
-    }
-
-    private WebResource getResource(API api, String... pathParams) {
-        WebResource resource = service.path(api.getPath());
-        if (pathParams != null) {
-            for (String pathParam : pathParams) {
-                resource = resource.path(pathParam);
-            }
-        }
-        return resource;
-    }
-    
-    private List<String> extractResults(JSONObject response) throws AtlasServiceException {
-        try {
-            JSONArray results = response.getJSONArray(AtlasClient.RESULTS);
-            List<String> list = new ArrayList<String>();
-            for (int index = 0; index < results.length(); index++) {
-                list.add(results.getString(index));
-            }
-            return list;
-        } catch (JSONException e) {
-          throw new AtlasServiceException(e);
-        }
-    }
-    
-    enum API {
-
-        //Type operations
-        CREATE_TYPE(BASE_URI + TYPES, HttpMethod.POST),
-        GET_TYPE(BASE_URI + TYPES, HttpMethod.GET),
-        LIST_TYPES(BASE_URI + TYPES, HttpMethod.GET),
-        LIST_TRAIT_TYPES(BASE_URI + TYPES + "?type=trait", HttpMethod.GET),
-
-        //Entity operations
-        CREATE_ENTITY(BASE_URI + URI_ENTITIES, HttpMethod.POST),
-        GET_ENTITY(BASE_URI + URI_ENTITIES, HttpMethod.GET),
-        UPDATE_ENTITY(BASE_URI + URI_ENTITIES, HttpMethod.PUT),
-        LIST_ENTITY(BASE_URI + URI_ENTITIES, HttpMethod.GET),
-
-        //Trait operations
-        ADD_TRAITS(BASE_URI + URI_ENTITIES, HttpMethod.POST),
-        DELETE_TRAITS(BASE_URI + URI_ENTITIES, HttpMethod.DELETE),
-        LIST_TRAITS(BASE_URI + URI_ENTITIES, HttpMethod.GET),
-
-        //Search operations
-        SEARCH(BASE_URI + URI_SEARCH, HttpMethod.GET),
-        SEARCH_DSL(BASE_URI + URI_SEARCH + "/dsl", HttpMethod.GET),
-        SEARCH_GREMLIN(BASE_URI + URI_SEARCH + "/gremlin", HttpMethod.GET),
-        SEARCH_FULL_TEXT(BASE_URI + URI_SEARCH + "/fulltext", HttpMethod.GET),
-
-        //Lineage operations
-        LINEAGE_INPUTS_GRAPH(BASE_URI + URI_LINEAGE, HttpMethod.GET),
-        LINEAGE_OUTPUTS_GRAPH(BASE_URI + URI_LINEAGE, HttpMethod.GET),
-        LINEAGE_SCHEMA(BASE_URI + URI_LINEAGE, HttpMethod.GET);
-
-        private final String method;
-        private final String path;
-
-        API(String path, String method) {
-            this.path = path;
-            this.method = method;
-        }
-
-        public String getMethod() {
-            return method;
-        }
-
-        public String getPath() {
-            return path;
-        }
-    }
-    
-    private JSONObject callAPIWithResource(API api, WebResource resource) throws Exception {
-        return callAPIWithResource(api, resource, null);
-    }
-
-    private JSONObject callAPIWithResource(API api, WebResource resource, Object requestObject)
-    throws Exception {
-        ClientResponse clientResponse = resource.accept(JSON_MEDIA_TYPE).type(JSON_MEDIA_TYPE)
-                .method(api.getMethod(), ClientResponse.class, requestObject);
-
-        Response.Status expectedStatus =
-                HttpMethod.POST.equals(api.getMethod()) ? Response.Status.CREATED : Response.Status.OK;
-        if (clientResponse.getStatus() == expectedStatus.getStatusCode()) {
-            String responseAsString = clientResponse.getEntity(String.class);
-            try {
-                return new JSONObject(responseAsString);
-            } catch (JSONException e) {
-                throw new AtlasServiceException(e);
-            }
-        }
-
-        throw new Exception("Metadata service API " + api + " failed");
-    }
-    
-    /**
-     * Adds trait to the give entity
-     * @param guid
-     * @param traitDefinition
-     * @throws Exception 
-     */
-    public void addTrait(String guid, String traitDefinition) throws Exception {
-        callAPI(API.ADD_TRAITS, traitDefinition, guid, URI_TRAITS);
-    }
-    
-    private JSONObject callAPI(API api, Object requestObject, String... pathParams) throws Exception {
-        WebResource resource = getResource(api, pathParams);
-        return callAPIWithResource(api, resource, requestObject);
-    }
-    
-   
-
- 
-    
-}
diff --git a/codesamples/atlas/src/main/java/com/atlas/client/Taxonomy.java b/codesamples/atlas/src/main/java/com/atlas/client/Taxonomy.java
deleted file mode 100644
index e436923..0000000
--- a/codesamples/atlas/src/main/java/com/atlas/client/Taxonomy.java
+++ /dev/null
@@ -1,192 +0,0 @@
-package com.atlas.client;
-
-import java.util.ArrayList;
-
-import org.apache.atlas.AtlasClient;
-import org.apache.atlas.AtlasServiceException;
-import org.apache.atlas.typesystem.Referenceable;
-import org.apache.atlas.typesystem.json.TypesSerialization;
-import org.apache.atlas.typesystem.types.AttributeDefinition;
-import org.apache.atlas.typesystem.types.ClassType;
-import org.apache.atlas.typesystem.types.DataTypes;
-import org.apache.atlas.typesystem.types.EnumTypeDefinition;
-import org.apache.atlas.typesystem.types.HierarchicalTypeDefinition;
-import org.apache.atlas.typesystem.types.IDataType;
-import org.apache.atlas.typesystem.types.Multiplicity;
-import org.apache.atlas.typesystem.types.StructTypeDefinition;
-import org.apache.atlas.typesystem.types.TypeUtils;
-import org.apache.atlas.typesystem.types.utils.TypesUtil;
-
-import com.google.common.base.Preconditions;
-import com.google.common.collect.ImmutableList;
-
-public class Taxonomy {
-	{
-		System.setProperty("atlas.conf", "conf");
-	}
-	
-	@SuppressWarnings("unused")
-	private  AtlasClient ac = null;
-	
-	
-	
-	/**
-	 * This creates an instance of the taxonomy class
-	 * It will create a taxonomy using the traits
-	 * Constructor
-	 * @param baseurl
-	 */
-	
-	public Taxonomy(String baseurl, String traitname, String Supertrait) {
-		
-		ac = new AtlasClient(baseurl);
-		try {
-			ac.createType(this.createTraitTypes(traitname, Supertrait));
-		
-		
-		} catch (AtlasServiceException e) {
-			
-			e.printStackTrace();
-		}
-	
-	
-	}
-	
-	public Taxonomy() {
-	
-	
-	}
-
-	
-	
-	
-	/**
-	 * This is a generic method to create types
-	 * 
-	 */
-	public String createTraitTypes(String traitname, String supertrait){
-		
-		
-		//System.out.println("Supertrait: " + supertrait);
-				
-				if (supertrait == null ){
-					return  TypesSerialization.toJson(TypeUtils.getTypesDef(
-				ImmutableList.<EnumTypeDefinition> of(),
-				ImmutableList.<StructTypeDefinition> of(),
-		ImmutableList.of(TypesUtil.createTraitTypeDef(traitname, null )),
-		ImmutableList.<HierarchicalTypeDefinition<ClassType>>of()));
-				}else
-				{
-					return  TypesSerialization.toJson(TypeUtils.getTypesDef(
-							ImmutableList.<EnumTypeDefinition> of(),
-							ImmutableList.<StructTypeDefinition> of(),
-					ImmutableList.of(TypesUtil.createTraitTypeDef(traitname, ImmutableList.of(supertrait) )),
-					ImmutableList.<HierarchicalTypeDefinition<ClassType>>of()));
-					
-				}
-		
-		
-	}
-	
-
-	
-	
-	
-	/**
-	 * 
-	 * @param name
-	 * @param dT
-	 * @param m
-	 * @param isComposite
-	 * @param reverseAttributeName
-	 * @return
-	 */
-	@SuppressWarnings("rawtypes")
-	AttributeDefinition attrDef(String name, IDataType dT, Multiplicity m,
-			boolean isComposite, String reverseAttributeName) {
-		Preconditions.checkNotNull(name);
-		Preconditions.checkNotNull(dT);
-		return new AttributeDefinition(name, dT.getName(), m, isComposite,
-				reverseAttributeName);
-	}
-
-	/**
-	 * 
-	 * @param name
-	 * @param dT
-	 * @return
-	 */
-	@SuppressWarnings("rawtypes")
-	AttributeDefinition attrDef(String name, IDataType dT) {
-		return attrDef(name, dT, Multiplicity.OPTIONAL, false, null);
-	}
-
-	@SuppressWarnings("rawtypes")
-	AttributeDefinition attrDef(String name, IDataType dT, Multiplicity m) {
-		return attrDef(name, dT, m, false, null);
-	}
-	
-
-	/**
-	 * 
-	 * @param args
-	 * @throws Exception
-	 */
-	public static void main(String[] args) throws Exception {
-		
-		if(args.length < 2)
-			throw new Exception("Please pass the atlas base url and the typename");
-		
-		String traitname  = null;
-		String baseurl = args[0];
-		
-		String[] arr = null;
-		
-		
-		if(args[1] != null)
-		 traitname = args[1];
-		else
-			throw new Exception("Please pass the traitname");
-		
-		//ArrayList alist = null;
-		
-		Taxonomy tx = null;
-		if(args.length > 2 )
-		{
-			
-			//alist = new ArrayList<String>();
-					
-			String supertrait = args[2];
-			
-			tx = new Taxonomy(baseurl, traitname, supertrait);
-			
-			
-		}else
-			 tx = new Taxonomy(baseurl, traitname, null);
-		
-		
-		
-		
-		
-		System.out.println("Done creating trait " + traitname);
-		
-		
-		
-		/*AtlasEntityCreator aec = new AtlasEntityCreator(baseurl);
-		
-		    Referenceable referenceable = new Referenceable("DB", "SuperGreen3");
-	        referenceable.set("name",  "SuperGreen3Entity");
-	        referenceable.set("description", "this is to test trait inheritence");
-	        referenceable.set("owner", "Andrew");
-	        referenceable.set("locationUri", "hdfs://localhost:8020");
-	        referenceable.set("createTime", System.currentTimeMillis());
-	        
-	        aec.createEntity(referenceable);*/
-		
-		//aec.createEntity(aec.createRefObject("GOD_Type",, ));
-		//aec.createEntity(aec.createRefObject("GOD_Type", "GreenEntity", "this is to test trait inheritence"));	
-	}
-	
-	
-
-}
diff --git a/codesamples/atlas/src/main/java/com/atlas/client/TruckHiveMetaDataGenerator.java b/codesamples/atlas/src/main/java/com/atlas/client/TruckHiveMetaDataGenerator.java
deleted file mode 100644
index 0c452f1..0000000
--- a/codesamples/atlas/src/main/java/com/atlas/client/TruckHiveMetaDataGenerator.java
+++ /dev/null
@@ -1,567 +0,0 @@
-package com.atlas.client;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-
-import org.apache.atlas.AtlasClient;
-import org.apache.atlas.AtlasServiceException;
-import org.apache.atlas.hive.model.HiveDataModelGenerator;
-import org.apache.atlas.hive.model.HiveDataTypes;
-import org.apache.atlas.typesystem.Referenceable;
-import org.apache.atlas.typesystem.Struct;
-import org.apache.atlas.typesystem.json.InstanceSerialization;
-import org.apache.atlas.typesystem.persistence.Id;
-import org.apache.commons.lang.StringEscapeUtils;
-import org.apache.commons.lang.StringUtils;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.metastore.api.Database;
-import org.apache.hadoop.hive.metastore.api.FieldSchema;
-import org.apache.hadoop.hive.metastore.api.Index;
-import org.apache.hadoop.hive.metastore.api.Order;
-import org.apache.hadoop.hive.metastore.api.SerDeInfo;
-import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
-import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;
-import org.apache.hadoop.hive.ql.metadata.Hive;
-import org.apache.hadoop.hive.ql.metadata.Partition;
-import org.apache.hadoop.hive.ql.metadata.Table;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.codehaus.jettison.json.JSONArray;
-import org.codehaus.jettison.json.JSONException;
-import org.codehaus.jettison.json.JSONObject;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import com.google.common.collect.ImmutableList;
-
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Set;
-
-/**
- * A Bridge Utility that imports metadata from the Hive Meta Store
- * and registers then in Atlas.
- */
-public class TruckHiveMetaDataGenerator {
-	
-	{
-		System.setProperty("atlas.conf", "/Users/sdutta/Applications/conf");
-	}
-	
-	
-    private static final String DEFAULT_DGI_URL = "http://localhost:21000/";
-    
-    public static final String DEFAULT_CLUSTER_NAME = "primary";
-    private static String clusterName = "atlasdemo";
-
-    public static final String DGI_URL_PROPERTY = "hive.hook.dgi.url";
-
-    private static final Logger LOG = LoggerFactory.getLogger(TruckHiveMetaDataGenerator.class);
-
-    private final Hive hiveClient = null;
-    private  AtlasClient atlasClient;
-    private static String databasename = null;
-    private static String tablename = null;
-    
-    public static void main(String[] args) throws Exception {
-    	
-    	clusterName = args[1];
-    	String baseurl = args[0];
-    	databasename = args[2];
-    	tablename = args[3];
-    	
-    	
-    	
-    	TruckHiveMetaDataGenerator hmg = new TruckHiveMetaDataGenerator(baseurl);
-    	
-    	Referenceable db = hmg.registerDatabase(databasename, clusterName);
-    	hmg.registerTable(db, databasename, tablename);
-    	
-    }
-
-    /**
-     * 
-     * @param baseurl
-     */
-    public TruckHiveMetaDataGenerator(String baseurl) {
-    	
-    	atlasClient = new AtlasClient(baseurl);
-    
-    }
-    
-    
-    public AtlasClient getAtlasClient() {
-        return atlasClient;
-    }
-
-
-      
-
-    public Referenceable registerDatabase(String databaseName, String clusterName) throws Exception {
-        Referenceable dbRef = getDatabaseReference(databaseName, clusterName);
-        
-        if (dbRef == null) {
-            LOG.info("Importing objects from databaseName : " + databaseName);
-            //Database hiveDB = hiveClient.getDatabase(databaseName);
-
-            dbRef = new Referenceable(HiveDataTypes.HIVE_DB.getName());
-            dbRef.set(HiveDataModelGenerator.NAME, databaseName);
-            dbRef.set(HiveDataModelGenerator.CLUSTER_NAME, clusterName);
-            dbRef.set("description", "this is a default database");
-            dbRef.set("locationUri", "/hive/default");
-            dbRef.set("parameters", "key1=name1,key2=name2");
-            dbRef.set("ownerName", "Hortonworks");
-            dbRef = createInstance(dbRef);
-            
-            
-        } else {
-            LOG.info("Database {} is already registered with id {}", databaseName, dbRef.getId().id);
-        }
-        
-        return dbRef;
-    }
-
-    public Referenceable createInstance(Referenceable referenceable) throws Exception {
-        String typeName = referenceable.getTypeName();
-        LOG.debug("creating instance of type " + typeName);
-
-        String entityJSON = InstanceSerialization.toJson(referenceable, true);
-        LOG.debug("Submitting new entity {} = {}", referenceable.getTypeName(), entityJSON);
-        JSONObject jsonObject = atlasClient.createEntity(entityJSON);
-        String guid = jsonObject.getString(AtlasClient.GUID);
-        LOG.debug("created instance for type " + typeName + ", guid: " + guid);
-
-        return new Referenceable(guid, referenceable.getTypeName(), null);
-    }
-
-   
-
-    /**
-     * Gets reference for the database
-     *
-     *
-     * @param databaseName  database Name
-     * @param clusterName    cluster name
-     * @return Reference for database if exists, else null
-     * @throws Exception
-     */
-    private Referenceable getDatabaseReference(String databaseName, String clusterName) throws Exception {
-        LOG.debug("Getting reference for database {}", databaseName);
-        String typeName = HiveDataTypes.HIVE_DB.getName();
-
-        String dslQuery = String.format("%s where %s = '%s' and %s = '%s'", typeName, HiveDataModelGenerator.NAME,
-                databaseName.toLowerCase(), HiveDataModelGenerator.CLUSTER_NAME, clusterName);
-        return getEntityReferenceFromDSL(typeName, dslQuery);
-    }
-
-    public Referenceable getProcessReference(String queryStr) throws Exception {
-        LOG.debug("Getting reference for process with query {}", queryStr);
-        String typeName = HiveDataTypes.HIVE_PROCESS.getName();
-
-        //todo enable DSL
-        //        String dslQuery = String.format("%s where queryText = \"%s\"", typeName, queryStr);
-        //        return getEntityReferenceFromDSL(typeName, dslQuery);
-
-        String gremlinQuery =
-                String.format("g.V.has('__typeName', '%s').has('%s.queryText', \"%s\").toList()", typeName, typeName,
-                        StringEscapeUtils.escapeJava(queryStr));
-        return getEntityReferenceFromGremlin(typeName, gremlinQuery);
-    }
-
-    private Referenceable getEntityReferenceFromDSL(String typeName, String dslQuery) throws Exception {
-        AtlasClient dgiClient = getAtlasClient();
-        JSONArray results = dgiClient.searchByDSL(dslQuery);
-        if (results.length() == 0) {
-            return null;
-        } else {
-            String guid;
-            JSONObject row = results.getJSONObject(0);
-            if (row.has("$id$")) {
-                guid = row.getJSONObject("$id$").getString("id");
-            } else {
-                guid = row.getJSONObject("_col_0").getString("id");
-            }
-            return new Referenceable(guid, typeName, null);
-        }
-    }
-
-    public static String getTableName(String clusterName, String dbName, String tableName) {
-        return String.format("%s.%s@%s", dbName.toLowerCase(), tableName.toLowerCase(), clusterName);
-    }
-
-    /**
-     * Gets reference for the table
-     *
-     * @param dbName database name
-     * @param tableName table name
-     * @return table reference if exists, else null
-     * @throws Exception
-     */
-    private Referenceable getTableReference(String dbName, String tableName) throws Exception {
-        LOG.debug("Getting reference for table {}.{}", dbName, tableName);
-
-        String typeName = HiveDataTypes.HIVE_TABLE.getName();
-        String entityName = getTableName(clusterName, dbName, tableName);
-        String dslQuery = String.format("%s as t where name = '%s'", typeName, entityName);
-        return getEntityReferenceFromDSL(typeName, dslQuery);
-    }
-
-    private Referenceable getEntityReferenceFromGremlin(String typeName, String gremlinQuery)
-    throws AtlasServiceException, JSONException {
-        AtlasClient client = getAtlasClient();
-        JSONObject response = client.searchByGremlin(gremlinQuery);
-        JSONArray results = response.getJSONArray(AtlasClient.RESULTS);
-        if (results.length() == 0) {
-            return null;
-        }
-        String guid = results.getJSONObject(0).getString("__guid");
-        return new Referenceable(guid, typeName, null);
-    }
-
-    private Referenceable getPartitionReference(String dbName, String tableName, List<String> values) throws Exception {
-        String valuesStr = "['" + StringUtils.join(values, "', '") + "']";
-        LOG.debug("Getting reference for partition for {}.{} with values {}", dbName, tableName, valuesStr);
-        String typeName = HiveDataTypes.HIVE_PARTITION.getName();
-
-        //todo replace gremlin with DSL
-        //        String dslQuery = String.format("%s as p where values = %s, tableName where name = '%s', "
-        //                        + "dbName where name = '%s' and clusterName = '%s' select p", typeName, valuesStr,
-        // tableName,
-        //                dbName, clusterName);
-
-        String datasetType = AtlasClient.DATA_SET_SUPER_TYPE;
-        String tableEntityName = getTableName(clusterName, dbName, tableName);
-
-        String gremlinQuery = String.format("g.V.has('__typeName', '%s').has('%s.values', %s).as('p')."
-                        + "out('__%s.table').has('%s.name', '%s').back('p').toList()", typeName, typeName, valuesStr,
-                typeName, datasetType, tableEntityName);
-
-        return getEntityReferenceFromGremlin(typeName, gremlinQuery);
-    }
-
-    private Referenceable getSDForTable(String dbName, String tableName) throws Exception {
-        Referenceable tableRef = getTableReference(dbName, tableName);
-        if (tableRef == null) {
-            throw new IllegalArgumentException("Table " + dbName + "." + tableName + " doesn't exist");
-        }
-
-        AtlasClient dgiClient = getAtlasClient();
-        Referenceable tableInstance = dgiClient.getEntity(tableRef.getId().id);
-        Id sdId = (Id) tableInstance.get("sd");
-        return new Referenceable(sdId.id, sdId.getTypeName(), null);
-    }
-
-    /**
-     * 
-     * @param dbName
-     * @param tableName
-     * @return
-     * @throws Exception
-     */
-    public Referenceable registerTable(String dbName, String tableName) throws Exception {
-        Referenceable dbReferenceable = registerDatabase(dbName, clusterName);
-        return registerTable(dbReferenceable, dbName, tableName);
-    }
-
-    
-    /**
-     * 
-     * @param dbReference
-     * @param dbName
-     * @param tableName
-     * @return
-     * @throws Exception
-     */
-    public Referenceable registerTable(Referenceable dbReference, String dbName, String tableName) throws Exception {
-        LOG.info("Attempting to register table [" + tableName + "]");
-        Referenceable tableRef = getTableReference(dbName, tableName);
-        
-        if (tableRef == null) {
-            LOG.info("Importing objects from " + dbName + "." + tableName);
-
-            //Table hiveTable = hiveClient.getTable(dbName, tableName);
-
-            tableRef = new Referenceable(HiveDataTypes.HIVE_TABLE.getName(),"Trucks");
-            tableRef.set(HiveDataModelGenerator.NAME,
-                    getTableName(clusterName, dbName, tableName));
-            
-            tableRef.set(HiveDataModelGenerator.TABLE_NAME,tableName.toLowerCase());
-            tableRef.set("owner", "Hortonworks");
-
-            tableRef.set("createTime", System.currentTimeMillis());
-            tableRef.set("lastAccessTime",System.currentTimeMillis());
-            tableRef.set("retention", System.currentTimeMillis());
-
-            tableRef.set(HiveDataModelGenerator.COMMENT, "This is loaded by Sqoop job");
-
-            // add reference to the database
-            tableRef.set(HiveDataModelGenerator.DB, dbReference);
-            
-            List<Referenceable> truckcols = ImmutableList
-                    .of(rawColumn("model_id", "String", "model_id"), rawColumn("model_name", "String", "model name"),
-                            rawColumn("max_speed", "String", "maximum speed", "Red"),
-                            rawColumn("torque", "String", "torque"),
-                            rawColumn("engine_type", "String", "engine diesel/gas"),
-                            rawColumn("tow_capacity", "String", "towing capacity"),
-                            rawColumn("model_year", "String", "model_year"));
-            
-            
-            tableRef.set("columns", truckcols);
-            
-            // add reference to the StorageDescriptor
-            //StorageDescriptor storageDesc = hiveTable.getSd();
-            //Referenceable sdReferenceable = fillStorageDescStruct(storageDesc, colList);
-            //tableRef.set("sd", sdReferenceable);
-
-            // add reference to the Partition Keys
-            //List<Referenceable> partKeys = getColumns(hiveTable.getPartitionKeys());
-            //tableRef.set("partitionKeys", partKeys);
-
-           // tableRef.set("parameters", "params");
-
-            
-            tableRef.set("viewOriginalText", "Original text");
-           
-
-            
-           tableRef.set("viewExpandedText", "Expanded Text");
-            
-
-            tableRef.set("tableType", "Sqoop generated table");
-            tableRef.set("temporary", "false");
-
-
-            tableRef = createInstance(tableRef);
-            
-        } else {
-            LOG.info("Table {}.{} is already registered with id {}", dbName, tableName, tableRef.getId().id);
-        }
-        return tableRef;
-    }
-
-    
-    /**
-     * 
-     * @param db
-     * @param tableName
-     * @param tableReferenceable
-     * @param sdReferenceable
-     * @throws Exception
-     */
-    private void registerPartitions(String db, String tableName, Referenceable tableReferenceable,
-            Referenceable sdReferenceable) throws Exception {
-        Set<Partition> tableParts = hiveClient.getAllPartitionsOf(new Table(Table.getEmptyTable(db, tableName)));
-
-        if (tableParts.size() > 0) {
-            for (Partition hivePart : tableParts) {
-                registerPartition(hivePart, tableReferenceable, sdReferenceable);
-            }
-        }
-    }
-
-    /**
-     * 
-     * @param partition
-     * @return
-     * @throws Exception
-     */
-    public Referenceable registerPartition(Partition partition) throws Exception {
-        String dbName = partition.getTable().getDbName();
-        String tableName = partition.getTable().getTableName();
-        Referenceable tableRef = registerTable(dbName, tableName);
-        Referenceable sdRef = getSDForTable(dbName, tableName);
-        return registerPartition(partition, tableRef, sdRef);
-    }
-
-    private Referenceable registerPartition(Partition hivePart, Referenceable tableReferenceable,
-            Referenceable sdReferenceable) throws Exception {
-        LOG.info("Registering partition for {} with values {}", tableReferenceable,
-                StringUtils.join(hivePart.getValues(), ","));
-        String dbName = hivePart.getTable().getDbName();
-        String tableName = hivePart.getTable().getTableName();
-
-        Referenceable partRef = getPartitionReference(dbName, tableName, hivePart.getValues());
-        if (partRef == null) {
-            partRef = new Referenceable(HiveDataTypes.HIVE_PARTITION.getName());
-            partRef.set("values", hivePart.getValues());
-
-            partRef.set(HiveDataModelGenerator.TABLE, tableReferenceable);
-
-            //todo fix
-            partRef.set("createTime", hivePart.getLastAccessTime());
-            partRef.set("lastAccessTime", hivePart.getLastAccessTime());
-
-            // sdStruct = fillStorageDescStruct(hivePart.getSd());
-            // Instead of creating copies of the sdstruct for partitions we are reusing existing
-            // ones will fix to identify partitions with differing schema.
-            partRef.set("sd", sdReferenceable);
-
-            partRef.set("parameters", hivePart.getParameters());
-            partRef = createInstance(partRef);
-        } else {
-            LOG.info("Partition {}.{} with values {} is already registered with id {}", dbName, tableName,
-                    StringUtils.join(hivePart.getValues(), ","), partRef.getId().id);
-        }
-        return partRef;
-    }
-
-    private void importIndexes(String db, String table, Referenceable dbReferenceable, Referenceable tableReferenceable)
-    throws Exception {
-        List<Index> indexes = hiveClient.getIndexes(db, table, Short.MAX_VALUE);
-        if (indexes.size() > 0) {
-            for (Index index : indexes) {
-                importIndex(index, dbReferenceable, tableReferenceable);
-            }
-        }
-    }
-
-    //todo should be idempotent
-    private void importIndex(Index index, Referenceable dbReferenceable, Referenceable tableReferenceable)
-            throws Exception {
-        LOG.info("Importing index {} for {}.{}", index.getIndexName(), dbReferenceable, tableReferenceable);
-        Referenceable indexRef = new Referenceable(HiveDataTypes.HIVE_INDEX.getName());
-
-        indexRef.set(HiveDataModelGenerator.NAME, index.getIndexName());
-        indexRef.set("indexHandlerClass", index.getIndexHandlerClass());
-
-        indexRef.set(HiveDataModelGenerator.DB, dbReferenceable);
-
-        indexRef.set("createTime", index.getCreateTime());
-        indexRef.set("lastAccessTime", index.getLastAccessTime());
-        indexRef.set("origTable", index.getOrigTableName());
-        indexRef.set("indexTable", index.getIndexTableName());
-
-        Referenceable sdReferenceable = fillStorageDescStruct(index.getSd(), null);
-        indexRef.set("sd", sdReferenceable);
-
-        indexRef.set("parameters", index.getParameters());
-
-        tableReferenceable.set("deferredRebuild", index.isDeferredRebuild());
-
-        createInstance(indexRef);
-    }
-
-    private Referenceable fillStorageDescStruct(StorageDescriptor storageDesc, List<Referenceable> colList)
-    throws Exception {
-        LOG.debug("Filling storage descriptor information for " + storageDesc);
-
-        Referenceable sdReferenceable = new Referenceable(HiveDataTypes.HIVE_STORAGEDESC.getName());
-
-        SerDeInfo serdeInfo = storageDesc.getSerdeInfo();
-        LOG.debug("serdeInfo = " + serdeInfo);
-        // SkewedInfo skewedInfo = storageDesc.getSkewedInfo();
-
-        String serdeInfoName = HiveDataTypes.HIVE_SERDE.getName();
-        Struct serdeInfoStruct = new Struct(serdeInfoName);
-
-        serdeInfoStruct.set(HiveDataModelGenerator.NAME, serdeInfo.getName());
-        serdeInfoStruct.set("serializationLib", serdeInfo.getSerializationLib());
-        serdeInfoStruct.set("parameters", serdeInfo.getParameters());
-
-        sdReferenceable.set("serdeInfo", serdeInfoStruct);
-        sdReferenceable.set(HiveDataModelGenerator.STORAGE_NUM_BUCKETS, storageDesc.getNumBuckets());
-        sdReferenceable
-                .set(HiveDataModelGenerator.STORAGE_IS_STORED_AS_SUB_DIRS, storageDesc.isStoredAsSubDirectories());
-
-        //Use the passed column list if not null, ex: use same references for table and SD
-        List<FieldSchema> columns = storageDesc.getCols();
-        if (columns != null && !columns.isEmpty()) {
-            if (colList != null) {
-                sdReferenceable.set("cols", colList);
-            } else {
-                sdReferenceable.set("cols", getColumns(columns));
-            }
-        }
-
-        List<Struct> sortColsStruct = new ArrayList<Struct>();
-        for (Order sortcol : storageDesc.getSortCols()) {
-            String hiveOrderName = HiveDataTypes.HIVE_ORDER.getName();
-            Struct colStruct = new Struct(hiveOrderName);
-            colStruct.set("col", sortcol.getCol());
-            colStruct.set("order", sortcol.getOrder());
-
-            sortColsStruct.add(colStruct);
-        }
-        if (sortColsStruct.size() > 0) {
-            sdReferenceable.set("sortCols", sortColsStruct);
-        }
-
-        sdReferenceable.set("location", storageDesc.getLocation());
-        sdReferenceable.set("inputFormat", storageDesc.getInputFormat());
-        sdReferenceable.set("outputFormat", storageDesc.getOutputFormat());
-        sdReferenceable.set("compressed", storageDesc.isCompressed());
-
-        if (storageDesc.getBucketCols().size() > 0) {
-            sdReferenceable.set("bucketCols", storageDesc.getBucketCols());
-        }
-
-        sdReferenceable.set("parameters", storageDesc.getParameters());
-        sdReferenceable.set("storedAsSubDirectories", storageDesc.isStoredAsSubDirectories());
-
-        return createInstance(sdReferenceable);
-    }
-
-    private List<Referenceable> getColumns(List<FieldSchema> schemaList) throws Exception {
-        List<Referenceable> colList = new ArrayList<Referenceable>();
-        for (FieldSchema fs : schemaList) {
-            LOG.debug("Processing field " + fs);
-            Referenceable colReferenceable = new Referenceable(HiveDataTypes.HIVE_COLUMN.getName());
-            colReferenceable.set(HiveDataModelGenerator.NAME, fs.getName());
-            colReferenceable.set("type", fs.getType());
-            colReferenceable.set(HiveDataModelGenerator.COMMENT, fs.getComment());
-
-            colList.add(createInstance(colReferenceable));
-        }
-        return colList;
-    }
-
-    public synchronized void registerHiveDataModel() throws Exception {
-        HiveDataModelGenerator dataModelGenerator = new HiveDataModelGenerator();
-        AtlasClient dgiClient = getAtlasClient();
-
-        //Register hive data model if its not already registered
-        if (dgiClient.getType(HiveDataTypes.HIVE_PROCESS.getName()) == null) {
-            LOG.info("Registering Hive data model");
-            dgiClient.createType(dataModelGenerator.getModelAsJson());
-        } else {
-            LOG.info("Hive data model is already registered!");
-        }
-    }
-
-    
-
-	Referenceable rawColumn(String name, String dataType, String comment, String... traitNames) throws Exception {
-	        Referenceable referenceable = new Referenceable(HiveDataTypes.HIVE_COLUMN.getName(), traitNames);
-	        referenceable.set("name", name);
-	        referenceable.set("type", dataType);
-	        referenceable.set("comment", comment);
-
-	        return referenceable;
-	    }
-	  
-	
-   
-    public void updateTable(Referenceable tableReferenceable, Table newTable) throws AtlasServiceException {
-        AtlasClient client = getAtlasClient();
-        client.updateEntity(tableReferenceable.getId()._getId(), HiveDataModelGenerator.TABLE_NAME,
-                newTable.getTableName().toLowerCase());
-        client.updateEntity(tableReferenceable.getId()._getId(), HiveDataModelGenerator.NAME,
-                getTableName(clusterName, newTable.getDbName(), newTable.getTableName()));
-    }
-}
\ No newline at end of file
diff --git a/codesamples/atlas/src/main/java/com/atlas/client/TrucksClass.java b/codesamples/atlas/src/main/java/com/atlas/client/TrucksClass.java
deleted file mode 100644
index 26b1f21..0000000
--- a/codesamples/atlas/src/main/java/com/atlas/client/TrucksClass.java
+++ /dev/null
@@ -1,386 +0,0 @@
-package com.atlas.client;
-
-
-
-import com.google.common.base.Preconditions;
-import com.google.common.collect.ImmutableList;
-
-import org.apache.atlas.AtlasClient;
-import org.apache.atlas.typesystem.Referenceable;
-import org.apache.atlas.typesystem.TypesDef;
-import org.apache.atlas.typesystem.json.InstanceSerialization;
-import org.apache.atlas.typesystem.json.TypesSerialization;
-import org.apache.atlas.typesystem.persistence.Id;
-import org.apache.atlas.typesystem.types.AttributeDefinition;
-import org.apache.atlas.typesystem.types.ClassType;
-import org.apache.atlas.typesystem.types.DataTypes;
-import org.apache.atlas.typesystem.types.EnumTypeDefinition;
-import org.apache.atlas.typesystem.types.HierarchicalTypeDefinition;
-import org.apache.atlas.typesystem.types.IDataType;
-import org.apache.atlas.typesystem.types.Multiplicity;
-import org.apache.atlas.typesystem.types.StructTypeDefinition;
-import org.apache.atlas.typesystem.types.TraitType;
-import org.apache.atlas.typesystem.types.TypeUtils;
-import org.apache.atlas.typesystem.types.utils.TypesUtil;
-import org.codehaus.jettison.json.JSONArray;
-import org.codehaus.jettison.json.JSONObject;
-
-import java.util.List;
-
-/**
- * A driver that sets up sample types and data for testing purposes.
- * Please take a look at QueryDSL in docs for the Meta Model.
- * todo - move this to examples module.
- */
-public class TrucksClass {
-
-	{
-		System.setProperty("atlas.conf", "/Users/sdutta/Applications/conf");
-	}
-	
-
-    private static final String DATABASE_TYPE = "DB";
-    private static final String COLUMN_TYPE = "Column";
-    private static final String TABLE_TYPE = "Table";
-    private static final String VIEW_TYPE = "View";
-    private static final String LOAD_PROCESS_TYPE = "LoadProcess";
-    private static final String STORAGE_DESC_TYPE = "StorageDesc";
-    
-
-    private static final String[] TYPES =
-            {DATABASE_TYPE, TABLE_TYPE, STORAGE_DESC_TYPE, COLUMN_TYPE, LOAD_PROCESS_TYPE, VIEW_TYPE, "JdbcAccess",
-                    "ETL", "Metric", "PII", "Fact", "Dimension"};
-
-    private final AtlasClient metadataServiceClient;
-
-
-	/**
-	 * 
-	 * @param args
-	 * @throws Exception
-	 */
-    public static void main(String[] args) throws Exception {
-        
-    	
-    	String baseUrl = args[0];
-    	String databasename = args[1];
-    	String tablename = args[2];
-    	String tablename2 = args[3];
-    	String flag = args[4];
-    	
-        TrucksClass quickStart = new TrucksClass(baseUrl);
-
-        // Shows how to create types in Atlas for your meta model
-        
-        if("createtype".equalsIgnoreCase(flag))
-        		quickStart.createTypes();
-
-        // Shows how to create entities (instances) for the added types in Atlas
-        quickStart.createMysqlEntities(databasename, tablename, tablename2);
-
-        // Shows some search queries using DSL based on types
-        if("search".equalsIgnoreCase(flag))
-        	quickStart.search();
-    }
-
-    
-    TrucksClass(String baseUrl) {
-    	
-    	
-        metadataServiceClient = new AtlasClient(baseUrl);
-    }
-
-
-    void createTypes() throws Exception {
-        TypesDef typesDef = createTypeDefinitions();
-
-        String typesAsJSON = TypesSerialization.toJson(typesDef);
-        System.out.println("typesAsJSON = " + typesAsJSON);
-        metadataServiceClient.createType(typesAsJSON);
-
-        // verify types created
-        verifyTypesCreated();
-    }
-
-    
-    
-    
-    TypesDef createTypeDefinitions() throws Exception {
-    
-    	HierarchicalTypeDefinition<ClassType> dbClsDef = TypesUtil
-                .createClassTypeDef(DATABASE_TYPE, null, attrDef("name", DataTypes.STRING_TYPE),
-                        attrDef("description", DataTypes.STRING_TYPE), attrDef("locationUri", DataTypes.STRING_TYPE),
-                        attrDef("owner", DataTypes.STRING_TYPE), attrDef("createTime", DataTypes.INT_TYPE));
-
-        HierarchicalTypeDefinition<ClassType> storageDescClsDef = TypesUtil
-                .createClassTypeDef(STORAGE_DESC_TYPE, null, attrDef("location", DataTypes.STRING_TYPE),
-                        attrDef("inputFormat", DataTypes.STRING_TYPE), attrDef("outputFormat", DataTypes.STRING_TYPE),
-                        attrDef("compressed", DataTypes.STRING_TYPE, Multiplicity.REQUIRED, false, null));
-
-        HierarchicalTypeDefinition<ClassType> columnClsDef = TypesUtil
-                .createClassTypeDef(COLUMN_TYPE, null, attrDef("name", DataTypes.STRING_TYPE),
-                        attrDef("dataType", DataTypes.STRING_TYPE), attrDef("comment", DataTypes.STRING_TYPE));
-
-        
-        HierarchicalTypeDefinition<ClassType> tblClsDef = TypesUtil
-                .createClassTypeDef(TABLE_TYPE, ImmutableList.of("DataSet"),
-                        new AttributeDefinition("db", DATABASE_TYPE, Multiplicity.REQUIRED, false, null),
-                        new AttributeDefinition("sd", STORAGE_DESC_TYPE, Multiplicity.REQUIRED, true, null),
-                        attrDef("owner", DataTypes.STRING_TYPE), attrDef("createTime", DataTypes.INT_TYPE),
-                        attrDef("lastAccessTime", DataTypes.INT_TYPE), attrDef("retention", DataTypes.INT_TYPE),
-                        attrDef("viewOriginalText", DataTypes.STRING_TYPE),
-                        attrDef("viewExpandedText", DataTypes.STRING_TYPE), attrDef("tableType", DataTypes.STRING_TYPE),
-                        attrDef("temporary", DataTypes.BOOLEAN_TYPE),
-                        new AttributeDefinition("columns", DataTypes.arrayTypeName(COLUMN_TYPE),
-                                Multiplicity.COLLECTION, true, null));
-
-        
-        HierarchicalTypeDefinition<ClassType> loadProcessClsDef = TypesUtil
-                .createClassTypeDef(LOAD_PROCESS_TYPE, ImmutableList.of("Process"),
-                        attrDef("userName", DataTypes.STRING_TYPE), attrDef("startTime", DataTypes.INT_TYPE),
-                        attrDef("endTime", DataTypes.INT_TYPE),
-                        attrDef("queryText", DataTypes.STRING_TYPE, Multiplicity.REQUIRED),
-                        attrDef("queryPlan", DataTypes.STRING_TYPE, Multiplicity.REQUIRED),
-                        attrDef("queryId", DataTypes.STRING_TYPE, Multiplicity.REQUIRED),
-                        attrDef("queryGraph", DataTypes.STRING_TYPE, Multiplicity.REQUIRED));
-
-        
-        HierarchicalTypeDefinition<ClassType> viewClsDef = TypesUtil
-                .createClassTypeDef(VIEW_TYPE, null, attrDef("name", DataTypes.STRING_TYPE),
-                        new AttributeDefinition("db", DATABASE_TYPE, Multiplicity.REQUIRED, false, null),
-                        new AttributeDefinition("inputTables", DataTypes.arrayTypeName(TABLE_TYPE),
-                                Multiplicity.COLLECTION, false, null));
-
-        
-        HierarchicalTypeDefinition<TraitType> dimTraitDef = TypesUtil.createTraitTypeDef("Dimension", null);
-
-        
-        HierarchicalTypeDefinition<TraitType> factTraitDef = TypesUtil.createTraitTypeDef("Fact", null);
-
-        
-        HierarchicalTypeDefinition<TraitType> piiTraitDef = TypesUtil.createTraitTypeDef("PII", null);
-
-        
-        HierarchicalTypeDefinition<TraitType> metricTraitDef = TypesUtil.createTraitTypeDef("Metric", null);
-
-        
-        HierarchicalTypeDefinition<TraitType> etlTraitDef = TypesUtil.createTraitTypeDef("ETL", null);
-
-        HierarchicalTypeDefinition<TraitType> jdbcTraitDef = TypesUtil.createTraitTypeDef("JdbcAccess", null);
-
-        return TypeUtils.getTypesDef(ImmutableList.<EnumTypeDefinition>of(), ImmutableList.<StructTypeDefinition>of(),
-                ImmutableList.of(dimTraitDef, factTraitDef, piiTraitDef, metricTraitDef, etlTraitDef, jdbcTraitDef),
-                ImmutableList.of(dbClsDef, storageDescClsDef, tblClsDef, loadProcessClsDef, viewClsDef));
-    }
-
-    AttributeDefinition attrDef(String name, IDataType dT) {
-        return attrDef(name, dT, Multiplicity.OPTIONAL, false, null);
-    }
-
-    AttributeDefinition attrDef(String name, IDataType dT, Multiplicity m) {
-        return attrDef(name, dT, m, false, null);
-    }
-
-    AttributeDefinition attrDef(String name, IDataType dT, Multiplicity m, boolean isComposite,
-            String reverseAttributeName) {
-        Preconditions.checkNotNull(name);
-        Preconditions.checkNotNull(dT);
-        return new AttributeDefinition(name, dT.getName(), m, isComposite, reverseAttributeName);
-    }
-
-    
-    
-    void createMysqlEntities(String databasename, String tablename, String tablename2) throws Exception {
-        
-    	Id sourceDB = database(databasename, "MySQL Database", "Oracle", "mysql -u root");
-
-
-        Referenceable sd =
-                rawStorageDescriptor("hdfs://host:8000/apps/warehouse/products", "TextInputFormat", "TextOutputFormat",
-                        true);
-
-        List<Referenceable> driversColumns = ImmutableList
-                .of(rawColumn("model_id", "String", "model_id"), rawColumn("model_name", "String", "model name"),
-                        rawColumn("max_speed", "String", "maximum speed", "Red"),
-                        rawColumn("torque", "String", "torque"),
-                        rawColumn("engine_type", "String", "engine diesel/gas"),
-                        rawColumn("tow_capacity", "String", "towing capacity"),
-                        rawColumn("model_year", "String", "model_year"));
-
-        
-        Id trucks = table(tablename, "mysql table", sourceDB, sd, "Ford Jr", "Managed", driversColumns, "Trucks");
-
-        Id Motorcycle = table(tablename2, "mysql table", sourceDB, sd, "Ford Jr", "Managed", driversColumns, "Trucks");
-       
-       ;
-
-    }
-
-    private Id createInstance(Referenceable referenceable) throws Exception {
-        String typeName = referenceable.getTypeName();
-
-        String entityJSON = InstanceSerialization.toJson(referenceable, true);
-        System.out.println("Submitting new entity= " + entityJSON);
-        JSONObject jsonObject = metadataServiceClient.createEntity(entityJSON);
-        String guid = jsonObject.getString(AtlasClient.GUID);
-        System.out.println("created instance for type " + typeName + ", guid: " + guid);
-
-        // return the Id for created instance with guid
-        return new Id(guid, referenceable.getId().getVersion(), referenceable.getTypeName());
-    }
-
-    
-    
-    Id database(String name, String description, String owner, String locationUri, String... traitNames)
-    throws Exception {
-        Referenceable referenceable = new Referenceable(DATABASE_TYPE, traitNames);
-        referenceable.set("name", name);
-        referenceable.set("description", description);
-        referenceable.set("owner", owner);
-        referenceable.set("locationUri", locationUri);
-        referenceable.set("createTime", System.currentTimeMillis());
-
-        return createInstance(referenceable);
-    }
-
-    
-    
-    Referenceable rawStorageDescriptor(String location, String inputFormat, String outputFormat, boolean compressed)
-    throws Exception {
-        Referenceable referenceable = new Referenceable(STORAGE_DESC_TYPE);
-        referenceable.set("location", location);
-        referenceable.set("inputFormat", inputFormat);
-        referenceable.set("outputFormat", outputFormat);
-        referenceable.set("compressed", compressed);
-
-        return referenceable;
-    }
-
-    
-    
-    Referenceable rawColumn(String name, String dataType, String comment, String... traitNames) throws Exception {
-        Referenceable referenceable = new Referenceable(COLUMN_TYPE, traitNames);
-        referenceable.set("name", name);
-        referenceable.set("dataType", dataType);
-        referenceable.set("comment", comment);
-
-        return referenceable;
-    }
-
-    Id table(String name, String description, Id dbId, Referenceable sd, String owner, String tableType,
-            List<Referenceable> columns, String... traitNames) throws Exception {
-        Referenceable referenceable = new Referenceable(TABLE_TYPE, traitNames);
-        referenceable.set("name", name);
-        referenceable.set("description", description);
-        referenceable.set("owner", owner);
-        referenceable.set("tableType", tableType);
-        referenceable.set("createTime", System.currentTimeMillis());
-        referenceable.set("lastAccessTime", System.currentTimeMillis());
-        referenceable.set("retention", System.currentTimeMillis());
-        referenceable.set("db", dbId);
-        referenceable.set("sd", sd);
-        referenceable.set("columns", columns);
-
-        return createInstance(referenceable);
-    }
-
-    Id loadProcess(String name, String description, String user, List<Id> inputTables, List<Id> outputTables,
-            String queryText, String queryPlan, String queryId, String queryGraph, String... traitNames)
-    throws Exception {
-        Referenceable referenceable = new Referenceable(LOAD_PROCESS_TYPE, traitNames);
-        // super type attributes
-        referenceable.set("name", name);
-        referenceable.set("description", description);
-        referenceable.set("inputs", inputTables);
-        referenceable.set("outputs", outputTables);
-
-        referenceable.set("user", user);
-        referenceable.set("startTime", System.currentTimeMillis());
-        referenceable.set("endTime", System.currentTimeMillis() + 10000);
-
-        referenceable.set("queryText", queryText);
-        referenceable.set("queryPlan", queryPlan);
-        referenceable.set("queryId", queryId);
-        referenceable.set("queryGraph", queryGraph);
-
-        return createInstance(referenceable);
-    }
-
-    Id view(String name, Id dbId, List<Id> inputTables, String... traitNames) throws Exception {
-        Referenceable referenceable = new Referenceable(VIEW_TYPE, traitNames);
-        referenceable.set("name", name);
-        referenceable.set("db", dbId);
-
-        referenceable.set("inputTables", inputTables);
-
-        return createInstance(referenceable);
-    }
-
-    private void verifyTypesCreated() throws Exception {
-        List<String> types = metadataServiceClient.listTypes();
-        for (String type : TYPES) {
-            assert types.contains(type);
-        }
-    }
-
-    private String[] getDSLQueries() {
-        return new String[]{"from DB", "DB", "DB where name=\"Reporting\"", "DB where DB.name=\"Reporting\"",
-                "DB name = \"Reporting\"", "DB DB.name = \"Reporting\"",
-                "DB where name=\"Reporting\" select name, owner", "DB where DB.name=\"Reporting\" select name, owner",
-                "DB has name", "DB where DB has name", "DB, Table", "DB is JdbcAccess",
-            /*
-            "DB, hive_process has name",
-            "DB as db1, Table where db1.name = \"Reporting\"",
-            "DB where DB.name=\"Reporting\" and DB.createTime < " + System.currentTimeMillis()},
-            */
-                "from Table", "Table", "Table is Dimension", "Column where Column isa PII", "View is Dimension",
-            /*"Column where Column isa PII select Column.name",*/
-                "Column select Column.name", "Column select name", "Column where Column.name=\"customer_id\"",
-                "from Table select Table.name", "DB where (name = \"Reporting\")",
-                "DB where (name = \"Reporting\") select name as _col_0, owner as _col_1", "DB where DB is JdbcAccess",
-                "DB where DB has name", "DB Table", "DB where DB has name",
-                "DB as db1 Table where (db1.name = \"Reporting\")",
-                "DB where (name = \"Reporting\") select name as _col_0, (createTime + 1) as _col_1 ",
-            /*
-            todo: does not work
-            "DB where (name = \"Reporting\") and ((createTime + 1) > 0)",
-            "DB as db1 Table as tab where ((db1.createTime + 1) > 0) and (db1.name = \"Reporting\") select db1.name
-            as dbName, tab.name as tabName",
-            "DB as db1 Table as tab where ((db1.createTime + 1) > 0) or (db1.name = \"Reporting\") select db1.name as
-             dbName, tab.name as tabName",
-            "DB as db1 Table as tab where ((db1.createTime + 1) > 0) and (db1.name = \"Reporting\") or db1 has owner
-            select db1.name as dbName, tab.name as tabName",
-            "DB as db1 Table as tab where ((db1.createTime + 1) > 0) and (db1.name = \"Reporting\") or db1 has owner
-            select db1.name as dbName, tab.name as tabName",
-            */
-                // trait searches
-                "Dimension",
-            /*"Fact", - todo: does not work*/
-                "JdbcAccess", "ETL", "Metric", "PII",
-            /*
-            // Lineage - todo - fix this, its not working
-            "Table hive_process outputTables",
-            "Table loop (hive_process outputTables)",
-            "Table as _loop0 loop (hive_process outputTables) withPath",
-            "Table as src loop (hive_process outputTables) as dest select src.name as srcTable, dest.name as
-            destTable withPath",
-            */
-                "Table where name=\"sales_fact\", columns",
-                "Table where name=\"sales_fact\", columns as column select column.name, column.dataType, column"
-                        + ".comment",
-                "from DataSet", "from Process",};
-    }
-
-    private void search() throws Exception {
-        for (String dslQuery : getDSLQueries()) {
-            JSONObject response = metadataServiceClient.searchEntity(dslQuery);
-            JSONObject results = response.getJSONObject(AtlasClient.RESULTS);
-            if (!results.isNull("rows")) {
-                JSONArray rows = results.getJSONArray("rows");
-                System.out.println("query [" + dslQuery + "] returned [" + rows.length() + "] rows");
-            } else {
-                System.out.println("query [" + dslQuery + "] failed, results:" + results.toString());
-            }
-        }
-    }
-}
diff --git a/codesamples/atlas/src/main/java/com/atlas/client/TypeInheritence.java b/codesamples/atlas/src/main/java/com/atlas/client/TypeInheritence.java
deleted file mode 100644
index 14f10cf..0000000
--- a/codesamples/atlas/src/main/java/com/atlas/client/TypeInheritence.java
+++ /dev/null
@@ -1,120 +0,0 @@
-package com.atlas.client;
-
-import org.apache.atlas.AtlasClient;
-import org.apache.atlas.AtlasServiceException;
-import org.apache.atlas.typesystem.Referenceable;
-import org.apache.atlas.typesystem.json.TypesSerialization;
-import org.apache.atlas.typesystem.types.AttributeDefinition;
-import org.apache.atlas.typesystem.types.ClassType;
-import org.apache.atlas.typesystem.types.DataTypes;
-import org.apache.atlas.typesystem.types.EnumTypeDefinition;
-import org.apache.atlas.typesystem.types.HierarchicalTypeDefinition;
-import org.apache.atlas.typesystem.types.IDataType;
-import org.apache.atlas.typesystem.types.Multiplicity;
-import org.apache.atlas.typesystem.types.StructTypeDefinition;
-import org.apache.atlas.typesystem.types.TypeUtils;
-import org.apache.atlas.typesystem.types.utils.TypesUtil;
-
-import com.google.common.base.Preconditions;
-import com.google.common.collect.ImmutableList;
-
-public class TypeInheritence {
-	{
-		System.setProperty("atlas.conf", "conf");
-	}
-	
-	@SuppressWarnings("unused")
-	private  AtlasClient ac = null;
-	
-	
-	public TypeInheritence(String baseurl) {
-		
-		ac = new AtlasClient(baseurl);
-		try {
-			ac.createType(this.createTraitTypes());
-		} catch (AtlasServiceException e) {
-			// TODO Auto-generated catch block
-			e.printStackTrace();
-		}
-	}
-
-
-	/**
-	 * 
-	 * @param args
-	 * @throws Exception
-	 */
-	public static void main(String[] args) throws Exception {
-		if(args.length < 0)
-			throw new Exception("Please pass the atlas base url");
-		String baseurl = args[0];
-		
-		System.out.println(" Baseurl" + baseurl);
-		//TypeInheritence tIh = new TypeInheritence(baseurl);
-		//tIh.createTraitTypes();
-		
-		/*AtlasEntityCreator aec = new AtlasEntityCreator(baseurl);
-		
-		    Referenceable referenceable = new Referenceable("DB", "SuperGreen3");
-	        referenceable.set("name",  "SuperGreen3Entity");
-	        referenceable.set("description", "this is to test trait inheritence");
-	        referenceable.set("owner", "Andrew");
-	        referenceable.set("locationUri", "hdfs://localhost:8020");
-	        referenceable.set("createTime", System.currentTimeMillis());
-	        
-	        aec.createEntity(referenceable);*/
-		
-		//aec.createEntity(aec.createRefObject("GOD_Type",, ));
-		//aec.createEntity(aec.createRefObject("GOD_Type", "GreenEntity", "this is to test trait inheritence"));	
-	}
-	
-	/*
-	 * 
-	 */
-	public String createTraitTypes(){
-		
-		return TypesSerialization.toJson(TypeUtils.getTypesDef(
-				ImmutableList.<EnumTypeDefinition> of(),
-				ImmutableList.<StructTypeDefinition> of(),
-		ImmutableList.of(TypesUtil.createTraitTypeDef("PII", ImmutableList.of("Blue","White") )),
-		ImmutableList.<HierarchicalTypeDefinition<ClassType>>of()));
-		
-	}
-	
-	/**
-	 * 
-	 * @param name
-	 * @param dT
-	 * @param m
-	 * @param isComposite
-	 * @param reverseAttributeName
-	 * @return
-	 */
-	@SuppressWarnings("rawtypes")
-	AttributeDefinition attrDef(String name, IDataType dT, Multiplicity m,
-			boolean isComposite, String reverseAttributeName) {
-		Preconditions.checkNotNull(name);
-		Preconditions.checkNotNull(dT);
-		return new AttributeDefinition(name, dT.getName(), m, isComposite,
-				reverseAttributeName);
-	}
-
-	/**
-	 * 
-	 * @param name
-	 * @param dT
-	 * @return
-	 */
-	@SuppressWarnings("rawtypes")
-	AttributeDefinition attrDef(String name, IDataType dT) {
-		return attrDef(name, dT, Multiplicity.OPTIONAL, false, null);
-	}
-
-	@SuppressWarnings("rawtypes")
-	AttributeDefinition attrDef(String name, IDataType dT, Multiplicity m) {
-		return attrDef(name, dT, m, false, null);
-	}
-	
-	
-
-}
diff --git a/codesamples/atlas/src/main/java/com/atlas/client/XMLHierarchy.java b/codesamples/atlas/src/main/java/com/atlas/client/XMLHierarchy.java
deleted file mode 100644
index 38687f7..0000000
--- a/codesamples/atlas/src/main/java/com/atlas/client/XMLHierarchy.java
+++ /dev/null
@@ -1,22 +0,0 @@
-/**
- * 
- */
-package com.atlas.client;
-
-/**
- * @author sdutta
- * @param <T>
- *
- */
-public class XMLHierarchy<T> implements Hierarchy<T> {
-
-	
-//	public void 
-	
-	
-	public void parse() throws Exception{
-		// TODO Auto-generated method stub
-		
-	}
-
-}
diff --git a/codesamples/atlas/src/main/java/com/atlas/client/mysqlTypeCreator.java b/codesamples/atlas/src/main/java/com/atlas/client/mysqlTypeCreator.java
deleted file mode 100644
index 9c6b7b6..0000000
--- a/codesamples/atlas/src/main/java/com/atlas/client/mysqlTypeCreator.java
+++ /dev/null
@@ -1,391 +0,0 @@
-package com.atlas.client;
-
-
-
-import com.google.common.base.Preconditions;
-import com.google.common.collect.ImmutableList;
-
-import org.apache.atlas.AtlasClient;
-import org.apache.atlas.typesystem.Referenceable;
-import org.apache.atlas.typesystem.TypesDef;
-import org.apache.atlas.typesystem.json.InstanceSerialization;
-import org.apache.atlas.typesystem.json.TypesSerialization;
-import org.apache.atlas.typesystem.persistence.Id;
-import org.apache.atlas.typesystem.types.AttributeDefinition;
-import org.apache.atlas.typesystem.types.ClassType;
-import org.apache.atlas.typesystem.types.DataTypes;
-import org.apache.atlas.typesystem.types.EnumTypeDefinition;
-import org.apache.atlas.typesystem.types.HierarchicalTypeDefinition;
-import org.apache.atlas.typesystem.types.IDataType;
-import org.apache.atlas.typesystem.types.Multiplicity;
-import org.apache.atlas.typesystem.types.StructTypeDefinition;
-import org.apache.atlas.typesystem.types.TraitType;
-import org.apache.atlas.typesystem.types.TypeUtils;
-import org.apache.atlas.typesystem.types.utils.TypesUtil;
-import org.codehaus.jettison.json.JSONArray;
-import org.codehaus.jettison.json.JSONObject;
-
-import java.util.List;
-
-/**
- * A driver that sets up sample types and data for testing purposes.
- * Please take a look at QueryDSL in docs for the Meta Model.
- * todo - move this to examples module.
- */
-public class mysqlTypeCreator {
-
-	{
-		System.setProperty("atlas.conf", "/Users/sdutta/Applications/conf");
-	}
-	
-
-    private static final String DATABASE_TYPE = "DB";
-    private static final String COLUMN_TYPE = "Column";
-    private static final String TABLE_TYPE = "Table";
-    private static final String VIEW_TYPE = "View";
-    private static final String LOAD_PROCESS_TYPE = "LoadProcess";
-    private static final String STORAGE_DESC_TYPE = "StorageDesc";
-    
-
-    private static final String[] TYPES =
-            {DATABASE_TYPE, TABLE_TYPE, STORAGE_DESC_TYPE, COLUMN_TYPE, LOAD_PROCESS_TYPE, VIEW_TYPE, "JdbcAccess",
-                    "ETL", "Metric", "PII", "Fact", "Dimension"};
-
-    private final AtlasClient metadataServiceClient;
-
-
-	/**
-	 * 
-	 * @param args
-	 * @throws Exception
-	 */
-    public static void main(String[] args) throws Exception {
-        
-    	
-    	String baseUrl = args[0];
-    	String databasename = args[1];
-    	String tablename = args[2];
-    	String tablename2 = args[3];
-    	String flag = args[4];
-    	
-        mysqlTypeCreator quickStart = new mysqlTypeCreator(baseUrl);
-
-        // Shows how to create types in Atlas for your meta model
-        
-        if("createtype".equalsIgnoreCase(flag))
-        		quickStart.createTypes();
-
-        // Shows how to create entities (instances) for the added types in Atlas
-        quickStart.createMysqlEntities(databasename, tablename, tablename2);
-
-        // Shows some search queries using DSL based on types
-        if("search".equalsIgnoreCase(flag))
-        	quickStart.search();
-    }
-
-    
-    mysqlTypeCreator(String baseUrl) {
-    	
-    	
-        metadataServiceClient = new AtlasClient(baseUrl);
-    }
-
-
-    void createTypes() throws Exception {
-        TypesDef typesDef = createTypeDefinitions();
-
-        String typesAsJSON = TypesSerialization.toJson(typesDef);
-        System.out.println("typesAsJSON = " + typesAsJSON);
-        metadataServiceClient.createType(typesAsJSON);
-
-        // verify types created
-        verifyTypesCreated();
-    }
-
-    
-    
-    
-    TypesDef createTypeDefinitions() throws Exception {
-    
-    	HierarchicalTypeDefinition<ClassType> dbClsDef = TypesUtil
-                .createClassTypeDef(DATABASE_TYPE, null, attrDef("name", DataTypes.STRING_TYPE),
-                        attrDef("description", DataTypes.STRING_TYPE), attrDef("locationUri", DataTypes.STRING_TYPE),
-                        attrDef("owner", DataTypes.STRING_TYPE), attrDef("createTime", DataTypes.INT_TYPE));
-
-        HierarchicalTypeDefinition<ClassType> storageDescClsDef = TypesUtil
-                .createClassTypeDef(STORAGE_DESC_TYPE, null, attrDef("location", DataTypes.STRING_TYPE),
-                        attrDef("inputFormat", DataTypes.STRING_TYPE), attrDef("outputFormat", DataTypes.STRING_TYPE),
-                        attrDef("compressed", DataTypes.STRING_TYPE, Multiplicity.REQUIRED, false, null));
-
-        HierarchicalTypeDefinition<ClassType> columnClsDef = TypesUtil
-                .createClassTypeDef(COLUMN_TYPE, null, attrDef("name", DataTypes.STRING_TYPE),
-                        attrDef("dataType", DataTypes.STRING_TYPE), attrDef("comment", DataTypes.STRING_TYPE));
-
-        
-        HierarchicalTypeDefinition<ClassType> tblClsDef = TypesUtil
-                .createClassTypeDef(TABLE_TYPE, ImmutableList.of("DataSet"),
-                        new AttributeDefinition("db", DATABASE_TYPE, Multiplicity.REQUIRED, false, null),
-                        new AttributeDefinition("sd", STORAGE_DESC_TYPE, Multiplicity.REQUIRED, true, null),
-                        attrDef("owner", DataTypes.STRING_TYPE), attrDef("createTime", DataTypes.INT_TYPE),
-                        attrDef("lastAccessTime", DataTypes.INT_TYPE), attrDef("retention", DataTypes.INT_TYPE),
-                        attrDef("viewOriginalText", DataTypes.STRING_TYPE),
-                        attrDef("viewExpandedText", DataTypes.STRING_TYPE), attrDef("tableType", DataTypes.STRING_TYPE),
-                        attrDef("temporary", DataTypes.BOOLEAN_TYPE),
-                        new AttributeDefinition("columns", DataTypes.arrayTypeName(COLUMN_TYPE),
-                                Multiplicity.COLLECTION, true, null));
-
-        
-        HierarchicalTypeDefinition<ClassType> loadProcessClsDef = TypesUtil
-                .createClassTypeDef(LOAD_PROCESS_TYPE, ImmutableList.of("Process"),
-                        attrDef("userName", DataTypes.STRING_TYPE), attrDef("startTime", DataTypes.INT_TYPE),
-                        attrDef("endTime", DataTypes.INT_TYPE),
-                        attrDef("queryText", DataTypes.STRING_TYPE, Multiplicity.REQUIRED),
-                        attrDef("queryPlan", DataTypes.STRING_TYPE, Multiplicity.REQUIRED),
-                        attrDef("queryId", DataTypes.STRING_TYPE, Multiplicity.REQUIRED),
-                        attrDef("queryGraph", DataTypes.STRING_TYPE, Multiplicity.REQUIRED));
-
-        
-        HierarchicalTypeDefinition<ClassType> viewClsDef = TypesUtil
-                .createClassTypeDef(VIEW_TYPE, null, attrDef("name", DataTypes.STRING_TYPE),
-                        new AttributeDefinition("db", DATABASE_TYPE, Multiplicity.REQUIRED, false, null),
-                        new AttributeDefinition("inputTables", DataTypes.arrayTypeName(TABLE_TYPE),
-                                Multiplicity.COLLECTION, false, null));
-
-        
-        HierarchicalTypeDefinition<TraitType> dimTraitDef = TypesUtil.createTraitTypeDef("Dimension", null);
-
-        
-        HierarchicalTypeDefinition<TraitType> factTraitDef = TypesUtil.createTraitTypeDef("Fact", null);
-
-        
-        HierarchicalTypeDefinition<TraitType> piiTraitDef = TypesUtil.createTraitTypeDef("PII", null);
-
-        
-        HierarchicalTypeDefinition<TraitType> metricTraitDef = TypesUtil.createTraitTypeDef("Metric", null);
-
-        
-        HierarchicalTypeDefinition<TraitType> etlTraitDef = TypesUtil.createTraitTypeDef("ETL", null);
-
-        HierarchicalTypeDefinition<TraitType> jdbcTraitDef = TypesUtil.createTraitTypeDef("JdbcAccess", null);
-
-        return TypeUtils.getTypesDef(ImmutableList.<EnumTypeDefinition>of(), ImmutableList.<StructTypeDefinition>of(),
-                ImmutableList.of(dimTraitDef, factTraitDef, piiTraitDef, metricTraitDef, etlTraitDef, jdbcTraitDef),
-                ImmutableList.of(dbClsDef, storageDescClsDef, tblClsDef, loadProcessClsDef, viewClsDef));
-    }
-
-    AttributeDefinition attrDef(String name, IDataType dT) {
-        return attrDef(name, dT, Multiplicity.OPTIONAL, false, null);
-    }
-
-    AttributeDefinition attrDef(String name, IDataType dT, Multiplicity m) {
-        return attrDef(name, dT, m, false, null);
-    }
-
-    AttributeDefinition attrDef(String name, IDataType dT, Multiplicity m, boolean isComposite,
-            String reverseAttributeName) {
-        Preconditions.checkNotNull(name);
-        Preconditions.checkNotNull(dT);
-        return new AttributeDefinition(name, dT.getName(), m, isComposite, reverseAttributeName);
-    }
-
-    
-    
-    void createMysqlEntities(String databasename, String tablename, String tablename2) throws Exception {
-        
-    	Id sourceDB = database(databasename, "MySQL Database", "Oracle", "mysql -u root");
-
-
-        Referenceable sd =
-                rawStorageDescriptor("hdfs://host:8000/apps/warehouse/sales", "TextInputFormat", "TextOutputFormat",
-                        true);
-
-        List<Referenceable> driversColumns = ImmutableList
-                .of(rawColumn("driver_id", "String", "driver id"), rawColumn("driver_name", "String", "driver name"),
-                        rawColumn("certified", "String", "Certified", "PII"),
-                        rawColumn("wageplan", "String", "product id", "PII"));
-
-        
-        Id salesFact = table(tablename, "mysql table", sourceDB, sd, "Andrew", "Managed", driversColumns, "Fact");
-
-       
-
-        List<Referenceable> timesheetColumns = ImmutableList.of(rawColumn("driver_id", "int", "driver id", "PII"),
-                rawColumn("driver_week", "int", "week of the year"),
-                rawColumn("hours_logged", "string", "hours logged", "PII"),
-                rawColumn("Miles_logged", "string", "miles logged", "PII"));
-
-       
-        Id timesheetDim =
-                table(tablename2, "demo table", sourceDB, sd, "fetl", "Oracle", timesheetColumns,
-                        "Dimension");
-
-    }
-
-    private Id createInstance(Referenceable referenceable) throws Exception {
-        String typeName = referenceable.getTypeName();
-
-        String entityJSON = InstanceSerialization.toJson(referenceable, true);
-        System.out.println("Submitting new entity= " + entityJSON);
-        JSONObject jsonObject = metadataServiceClient.createEntity(entityJSON);
-        String guid = jsonObject.getString(AtlasClient.GUID);
-        System.out.println("created instance for type " + typeName + ", guid: " + guid);
-
-        // return the Id for created instance with guid
-        return new Id(guid, referenceable.getId().getVersion(), referenceable.getTypeName());
-    }
-
-    
-    
-    Id database(String name, String description, String owner, String locationUri, String... traitNames)
-    throws Exception {
-        Referenceable referenceable = new Referenceable(DATABASE_TYPE, traitNames);
-        referenceable.set("name", name);
-        referenceable.set("description", description);
-        referenceable.set("owner", owner);
-        referenceable.set("locationUri", locationUri);
-        referenceable.set("createTime", System.currentTimeMillis());
-
-        return createInstance(referenceable);
-    }
-
-    
-    
-    Referenceable rawStorageDescriptor(String location, String inputFormat, String outputFormat, boolean compressed)
-    throws Exception {
-        Referenceable referenceable = new Referenceable(STORAGE_DESC_TYPE);
-        referenceable.set("location", location);
-        referenceable.set("inputFormat", inputFormat);
-        referenceable.set("outputFormat", outputFormat);
-        referenceable.set("compressed", compressed);
-
-        return referenceable;
-    }
-
-    
-    
-    Referenceable rawColumn(String name, String dataType, String comment, String... traitNames) throws Exception {
-        Referenceable referenceable = new Referenceable(COLUMN_TYPE, traitNames);
-        referenceable.set("name", name);
-        referenceable.set("dataType", dataType);
-        referenceable.set("comment", comment);
-
-        return referenceable;
-    }
-
-    Id table(String name, String description, Id dbId, Referenceable sd, String owner, String tableType,
-            List<Referenceable> columns, String... traitNames) throws Exception {
-        Referenceable referenceable = new Referenceable(TABLE_TYPE, traitNames);
-        referenceable.set("name", name);
-        referenceable.set("description", description);
-        referenceable.set("owner", owner);
-        referenceable.set("tableType", tableType);
-        referenceable.set("createTime", System.currentTimeMillis());
-        referenceable.set("lastAccessTime", System.currentTimeMillis());
-        referenceable.set("retention", System.currentTimeMillis());
-        referenceable.set("db", dbId);
-        referenceable.set("sd", sd);
-        referenceable.set("columns", columns);
-
-        return createInstance(referenceable);
-    }
-
-    Id loadProcess(String name, String description, String user, List<Id> inputTables, List<Id> outputTables,
-            String queryText, String queryPlan, String queryId, String queryGraph, String... traitNames)
-    throws Exception {
-        Referenceable referenceable = new Referenceable(LOAD_PROCESS_TYPE, traitNames);
-        // super type attributes
-        referenceable.set("name", name);
-        referenceable.set("description", description);
-        referenceable.set("inputs", inputTables);
-        referenceable.set("outputs", outputTables);
-
-        referenceable.set("user", user);
-        referenceable.set("startTime", System.currentTimeMillis());
-        referenceable.set("endTime", System.currentTimeMillis() + 10000);
-
-        referenceable.set("queryText", queryText);
-        referenceable.set("queryPlan", queryPlan);
-        referenceable.set("queryId", queryId);
-        referenceable.set("queryGraph", queryGraph);
-
-        return createInstance(referenceable);
-    }
-
-    Id view(String name, Id dbId, List<Id> inputTables, String... traitNames) throws Exception {
-        Referenceable referenceable = new Referenceable(VIEW_TYPE, traitNames);
-        referenceable.set("name", name);
-        referenceable.set("db", dbId);
-
-        referenceable.set("inputTables", inputTables);
-
-        return createInstance(referenceable);
-    }
-
-    private void verifyTypesCreated() throws Exception {
-        List<String> types = metadataServiceClient.listTypes();
-        for (String type : TYPES) {
-            assert types.contains(type);
-        }
-    }
-
-    private String[] getDSLQueries() {
-        return new String[]{"from DB", "DB", "DB where name=\"Reporting\"", "DB where DB.name=\"Reporting\"",
-                "DB name = \"Reporting\"", "DB DB.name = \"Reporting\"",
-                "DB where name=\"Reporting\" select name, owner", "DB where DB.name=\"Reporting\" select name, owner",
-                "DB has name", "DB where DB has name", "DB, Table", "DB is JdbcAccess",
-            /*
-            "DB, hive_process has name",
-            "DB as db1, Table where db1.name = \"Reporting\"",
-            "DB where DB.name=\"Reporting\" and DB.createTime < " + System.currentTimeMillis()},
-            */
-                "from Table", "Table", "Table is Dimension", "Column where Column isa PII", "View is Dimension",
-            /*"Column where Column isa PII select Column.name",*/
-                "Column select Column.name", "Column select name", "Column where Column.name=\"customer_id\"",
-                "from Table select Table.name", "DB where (name = \"Reporting\")",
-                "DB where (name = \"Reporting\") select name as _col_0, owner as _col_1", "DB where DB is JdbcAccess",
-                "DB where DB has name", "DB Table", "DB where DB has name",
-                "DB as db1 Table where (db1.name = \"Reporting\")",
-                "DB where (name = \"Reporting\") select name as _col_0, (createTime + 1) as _col_1 ",
-            /*
-            todo: does not work
-            "DB where (name = \"Reporting\") and ((createTime + 1) > 0)",
-            "DB as db1 Table as tab where ((db1.createTime + 1) > 0) and (db1.name = \"Reporting\") select db1.name
-            as dbName, tab.name as tabName",
-            "DB as db1 Table as tab where ((db1.createTime + 1) > 0) or (db1.name = \"Reporting\") select db1.name as
-             dbName, tab.name as tabName",
-            "DB as db1 Table as tab where ((db1.createTime + 1) > 0) and (db1.name = \"Reporting\") or db1 has owner
-            select db1.name as dbName, tab.name as tabName",
-            "DB as db1 Table as tab where ((db1.createTime + 1) > 0) and (db1.name = \"Reporting\") or db1 has owner
-            select db1.name as dbName, tab.name as tabName",
-            */
-                // trait searches
-                "Dimension",
-            /*"Fact", - todo: does not work*/
-                "JdbcAccess", "ETL", "Metric", "PII",
-            /*
-            // Lineage - todo - fix this, its not working
-            "Table hive_process outputTables",
-            "Table loop (hive_process outputTables)",
-            "Table as _loop0 loop (hive_process outputTables) withPath",
-            "Table as src loop (hive_process outputTables) as dest select src.name as srcTable, dest.name as
-            destTable withPath",
-            */
-                "Table where name=\"sales_fact\", columns",
-                "Table where name=\"sales_fact\", columns as column select column.name, column.dataType, column"
-                        + ".comment",
-                "from DataSet", "from Process",};
-    }
-
-    private void search() throws Exception {
-        for (String dslQuery : getDSLQueries()) {
-            JSONObject response = metadataServiceClient.searchEntity(dslQuery);
-            JSONObject results = response.getJSONObject(AtlasClient.RESULTS);
-            if (!results.isNull("rows")) {
-                JSONArray rows = results.getJSONArray("rows");
-                System.out.println("query [" + dslQuery + "] returned [" + rows.length() + "] rows");
-            } else {
-                System.out.println("query [" + dslQuery + "] failed, results:" + results.toString());
-            }
-        }
-    }
-}
diff --git a/codesamples/atlas/src/main/java/com/hortonworks/atlas/App.java b/codesamples/atlas/src/main/java/com/hortonworks/atlas/App.java
deleted file mode 100644
index 001829f..0000000
--- a/codesamples/atlas/src/main/java/com/hortonworks/atlas/App.java
+++ /dev/null
@@ -1,13 +0,0 @@
-package com.hortonworks.atlas;
-
-/**
- * Hello world!
- *
- */
-public class App 
-{
-    public static void main( String[] args )
-    {
-        System.out.println( "Hello World!" );
-    }
-}
diff --git a/codesamples/atlas/src/main/java/com/hortonworks/atlas/adapter/AtlasTableInterface.java b/codesamples/atlas/src/main/java/com/hortonworks/atlas/adapter/AtlasTableInterface.java
index c54d7d1..bfa30db 100644
--- a/codesamples/atlas/src/main/java/com/hortonworks/atlas/adapter/AtlasTableInterface.java
+++ b/codesamples/atlas/src/main/java/com/hortonworks/atlas/adapter/AtlasTableInterface.java
@@ -18,6 +18,10 @@ import org.apache.atlas.AtlasClient;
 import org.apache.atlas.AtlasServiceException;
 
 import com.google.common.collect.ImmutableList;
+import com.google.common.collect.ImmutableList.Builder;
+import com.hortonworks.atlas.cli.AtlasCLIOptions;
+import com.hortonworks.atlas.client.AtlasEntityConnector;
+import com.hortonworks.atlas.client.HiveMetaDataGenerator;
 
 
 /**
@@ -34,6 +38,9 @@ public class AtlasTableInterface {
 	private static final String VIEW_TYPE = "View";
 	private static final String LOAD_PROCESS_TYPE = "LoadProcess";
 	private static final String STORAGE_DESC_TYPE = "StorageDesc";
+	private String clustername = null;
+	private boolean hiveExecflag = false;
+	private boolean genlineage =  false;
 	
 	 public static final String[] TYPES =
          {DATABASE_TYPE, TABLE_TYPE, STORAGE_DESC_TYPE, COLUMN_TYPE, LOAD_PROCESS_TYPE, VIEW_TYPE, "JdbcAccess",
@@ -52,12 +59,14 @@ public class AtlasTableInterface {
 	 * @param password
 	 * @throws Exception 
 	 */
-	public AtlasTableInterface(String baseUrl, String mysqlhost, String db, String username, String password) throws Exception {
+	public AtlasTableInterface(String baseUrl, String mysqlhost, String db, String username, String password, 
+			boolean hiveflag, String ambariclustername, boolean generateLineage) throws Exception {
 
 		metadataServiceClient = new AtlasClient(baseUrl);
 		
 		ArrayList<String> types = (ArrayList<String>) metadataServiceClient.listTypes();
 		ListIterator<String> typeList = types.listIterator();
+		this.genlineage = generateLineage;
 		
 		for(String type : TYPES){
 			if(!types.contains(type)){
@@ -65,7 +74,9 @@ public class AtlasTableInterface {
 			}
 		
 		}
-		
+		this.clustername = ambariclustername;
+		this.hiveExecflag = hiveflag;
+		this.genlineage = generateLineage;
 		
 		mysqa = new MySqlAdapter(mysqlhost, db, username, password);
 		createMysqlEntities();
@@ -88,7 +99,8 @@ public class AtlasTableInterface {
 		String table_name = null;
 		java.util.ArrayList<Column> clist  = null;
 		Iterator<Column> itrc = null;
-		Referenceable colref, tabref;
+		Referenceable colref, hivetabref = null;
+		AtlasEntityConnector aec = new AtlasEntityConnector();
 		
 		while(itr.hasNext()){
 			
@@ -103,22 +115,62 @@ public class AtlasTableInterface {
 			}
 			
 			Table t = hshmap.get(table_name);
+			
+
+			System.out.println(t.getTable_name());
+
+			System.out.println(t.getDb().getName());
+			
 			clist = t.getColumnArrayList();
 			itrc = clist.iterator();
 			ImmutableList<Referenceable> lst = ImmutableList.of();
 			
+			Builder bld = ImmutableList.<Referenceable>builder();
+			
 			
 			while (itrc.hasNext()){
 				
 				Column c = itrc.next();
 			    colref = this.rawColumn(c.getColumn_name(), c.getColumn_type(), c.getColumn_remarks());
+			    bld.add(colref);
+			   
 			    
-			    lst.add(colref);
 			    
 			    
 			}
 			
-			this.table(t.getTable_name(), t.getRemarks(), database(t.getDb().getName(), "MYSQL DB",  t.getRemarks(), "tbd"), rawStorageDescriptor(t.getDb().getName() + "." + t.getTable_name(),"Table","Binary", true ), t.getDb().getName(), t.getTable_type(),lst, t.getDb().getName());
+			lst = bld.build();
+			
+			Id tabid = this.table(t.getTable_name(), t.getRemarks(), database(t.getDb().getName(), "MYSQL DB",  t.getRemarks(), "tbd"), rawStorageDescriptor(t.getDb().getName() + "." + t.getTable_name(),"Table","Binary", true ), t.getDb().getName(), t.getTable_type(),lst);
+		
+			
+			if(this.hiveExecflag){
+				System.out.println("Create hive tables ..");
+				HiveMetaDataGenerator hmg = new HiveMetaDataGenerator(this.metadataServiceClient);
+		    	
+		    	Referenceable dbref = hmg.registerDatabase(t.getDb().getName(), this.clustername);
+		    	hivetabref = hmg.registerExtTable(dbref, t.getDb().getName(), table_name, clist);
+			}
+		
+			
+			if(this.genlineage && this.hiveExecflag){
+				
+				/*loadProcess(String name, String description, String user,
+						List<Id> inputTables, List<Id> outputTables, String queryText,
+						String queryPlan, String queryId, String queryGraph,
+						String... traitNames) */
+				
+				loadProcess("mysqlLoader", "Lineage creating during ingestion of mysqltables", "hive",
+						ImmutableList.of(tabid),
+						ImmutableList.of(hivetabref.getId()), "IMPORT",
+						"DATA", "DATA", "DATA"
+						);
+				
+				//Referenceable proc = aec.loadProcess(LOAD_PROCESS_TYPE, );
+
+				//createInstance(proc);
+				
+			}
 		
 		}
 		
@@ -233,6 +285,22 @@ public class AtlasTableInterface {
 		return createInstance(referenceable);
 	}
 
+	
+	/**
+	 * 
+	 * @param name
+	 * @param description
+	 * @param user
+	 * @param inputTables
+	 * @param outputTables
+	 * @param queryText
+	 * @param queryPlan
+	 * @param queryId
+	 * @param queryGraph
+	 * @param traitNames
+	 * @return
+	 * @throws Exception
+	 */
 	Id loadProcess(String name, String description, String user,
 			List<Id> inputTables, List<Id> outputTables, String queryText,
 			String queryPlan, String queryId, String queryGraph,
@@ -257,6 +325,15 @@ public class AtlasTableInterface {
 		return createInstance(referenceable);
 	}
 
+	/**
+	 * 
+	 * @param name
+	 * @param dbId
+	 * @param inputTables
+	 * @param traitNames
+	 * @return
+	 * @throws Exception
+	 */
 	Id view(String name, Id dbId, List<Id> inputTables, String... traitNames)
 			throws Exception {
 		Referenceable referenceable = new Referenceable(VIEW_TYPE, traitNames);
@@ -268,6 +345,10 @@ public class AtlasTableInterface {
 		return createInstance(referenceable);
 	}
 
+	/**
+	 * 
+	 * @throws Exception
+	 */
 	private void verifyTypesCreated() throws Exception {
 		List<String> types = metadataServiceClient.listTypes();
 		for (String type : TYPES) {
@@ -275,6 +356,11 @@ public class AtlasTableInterface {
 		}
 	}
 
+	
+	/**
+	 * 
+	 * @return
+	 */
 	private String[] getDSLQueries() {
 		return new String[] {
 				"from DB",
diff --git a/codesamples/atlas/src/main/java/com/hortonworks/atlas/adapter/MySqlAdapter.java b/codesamples/atlas/src/main/java/com/hortonworks/atlas/adapter/MySqlAdapter.java
index 2948d9b..8bcbc17 100644
--- a/codesamples/atlas/src/main/java/com/hortonworks/atlas/adapter/MySqlAdapter.java
+++ b/codesamples/atlas/src/main/java/com/hortonworks/atlas/adapter/MySqlAdapter.java
@@ -81,6 +81,7 @@ public class MySqlAdapter {
 					 t.setTable_name(res.getString("TABLE_NAME"));
 					 t.setTable_type(res.getString("TABLE_TYPE"));
 					 t.setRemarks(res.getString("REMARKS"));
+					 t.setDb(new DB(this.DBname));
 					 
 					 java.sql.ResultSet res1 = md.getColumns(null, null, res.getString("TABLE_NAME"), null);
 					 
diff --git a/codesamples/atlas/src/main/java/com/hortonworks/atlas/cli/AtlasCLI.java b/codesamples/atlas/src/main/java/com/hortonworks/atlas/cli/AtlasCLI.java
new file mode 100644
index 0000000..05d09aa
--- /dev/null
+++ b/codesamples/atlas/src/main/java/com/hortonworks/atlas/cli/AtlasCLI.java
@@ -0,0 +1,761 @@
+package com.hortonworks.atlas.cli;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.ListIterator;
+
+import org.apache.atlas.AtlasServiceException;
+import org.apache.atlas.typesystem.Referenceable;
+import org.apache.atlas.typesystem.Struct;
+import org.apache.atlas.typesystem.json.InstanceSerialization;
+import org.apache.atlas.typesystem.persistence.Id;
+import org.apache.atlas.typesystem.types.DataTypes;
+import org.apache.commons.cli.CommandLine;
+import org.apache.commons.cli.CommandLineParser;
+import org.apache.commons.cli.GnuParser;
+import org.apache.commons.cli.HelpFormatter;
+import org.apache.commons.cli.OptionBuilder;
+import org.apache.commons.cli.Options;
+import org.apache.commons.cli.ParseException;
+import org.apache.commons.cli.PosixParser;
+import org.codehaus.jackson.JsonParseException;
+import org.codehaus.jettison.json.JSONException;
+import org.codehaus.jettison.json.JSONObject;
+import org.apache.atlas.AtlasClient;
+
+import com.google.common.collect.ImmutableList;
+import com.hortonworks.atlas.adapter.AtlasTableInterface;
+import com.hortonworks.atlas.adapter.EntityModel;
+import com.hortonworks.atlas.adapter.TupleModel;
+import com.hortonworks.atlas.client.AtlasEntityConnector;
+import com.hortonworks.atlas.client.AtlasEntityCreator;
+import com.hortonworks.atlas.client.AtlasEntitySearch;
+import com.hortonworks.atlas.client.AtlasTypeDefCreator;
+import com.hortonworks.atlas.client.HiveMetaDataGenerator;
+import com.hortonworks.atlas.client.JsonHierarchy;
+import com.hortonworks.atlas.client.NewAtlasClient;
+import com.hortonworks.atlas.client.Taxonomy;
+
+/**
+ * 
+ * @author sdutta
+ *
+ */
+public class AtlasCLI {
+
+	String baseurl;
+	Options opt = null;
+	AtlasClient aClient = null;
+	String action = null;
+
+	{
+		System.setProperty("atlas.conf", "conf");
+	}
+
+	/**
+	 * 
+	 * @param args
+	 */
+	public static void main(String[] args) {
+
+		try {
+			@SuppressWarnings("unused")
+			AtlasCLI cli = new AtlasCLI(args);
+		} catch (ParseException e) {
+			e.printStackTrace();
+		}
+
+	}
+
+	/**
+	 * 
+	 * @param args
+	 * @throws ParseException
+	 * @throws Exception
+	 */
+
+	@SuppressWarnings("static-access")
+	AtlasCLI(String args[]) throws ParseException {
+
+		CommandLineParser parser = new GnuParser();
+
+		opt = new Options();
+
+		opt.addOption(OptionBuilder
+				.withLongOpt(AtlasCLIOptions.action)
+				.withDescription(
+						"action you want to perform [search|createSimpleType|createDataSetType|"
+								+ "createProcessType|createSimpleEntity|createDataSetEntity|createProcessEntity"
+								+ "|createtrait|loadtraithierarchy|importmysql]")
+				.hasArg().withArgName("action").create()
+
+		);
+
+		opt.addOption(OptionBuilder.withLongOpt(AtlasCLIOptions.url)
+				.withDescription("Url for the atlas host http://host:21000")
+				.hasArg().withArgName("URL").create()
+
+		);
+
+		opt.addOption(OptionBuilder
+				.withLongOpt(AtlasCLIOptions.type)
+				.withDescription(
+						"String describing the type of the object. You can find by querying the list - http://host:21000/api/atlas/types?type=CLASS")
+				.hasArg().withArgName("type").create());
+
+		opt.addOption(OptionBuilder.withLongOpt(AtlasCLIOptions.name)
+				.withDescription("name of type or entity").hasArg()
+				.withArgName("name").create());
+
+		opt.addOption(OptionBuilder.withLongOpt(AtlasCLIOptions.description)
+				.withDescription("description of type or entity").hasArg()
+				.withArgName("name").create());
+
+		opt.addOption(OptionBuilder.withLongOpt(AtlasCLIOptions.inp_type)
+				.withDescription("name of type for input to a lineage")
+				.hasArg().withArgName("inp_type").create()
+
+		);
+
+		opt.addOption(OptionBuilder.withLongOpt(AtlasCLIOptions.inp_value)
+				.withDescription("value for input to a lineage").hasArg()
+				.withArgName("inp_value").create());
+
+		opt.addOption(OptionBuilder.withLongOpt(AtlasCLIOptions.out_type)
+				.withDescription("name of output to a lineage").hasArg()
+				.withArgName("out_type").create());
+
+		opt.addOption(OptionBuilder.withLongOpt(AtlasCLIOptions.out_value)
+				.withDescription("value for output to a lineage").hasArg()
+				.withArgName("out_value").create());
+
+		opt.addOption(OptionBuilder.withLongOpt(AtlasCLIOptions.traitTypename)
+				.withDescription("value for trait type").hasArg()
+				.withArgName(AtlasCLIOptions.traitTypename).create());
+
+		opt.addOption(OptionBuilder.withLongOpt(AtlasCLIOptions.traitnames)
+				.withDescription("name of the trait").hasArg()
+				.withArgName(AtlasCLIOptions.traitnames).create());
+
+		opt.addOption(OptionBuilder
+				.withLongOpt(AtlasCLIOptions.parentTraitName)
+				.withDescription("value of parent trait ").hasArg()
+				.withArgName(AtlasCLIOptions.parentTraitName).create());
+
+		opt.addOption(OptionBuilder.withLongOpt(AtlasCLIOptions.supertype)
+				.withDescription("Super type").hasArg()
+				.withArgName(AtlasCLIOptions.supertype).create());
+
+		opt.addOption(OptionBuilder.withLongOpt(AtlasCLIOptions.username)
+				.withDescription("mysql username").hasArg()
+				.withArgName(AtlasCLIOptions.username).create());
+		opt.addOption(OptionBuilder.withLongOpt(AtlasCLIOptions.password)
+				.withDescription("mysql password").hasArg()
+				.withArgName(AtlasCLIOptions.password).create());
+
+		opt.addOption(OptionBuilder
+				.withLongOpt(AtlasCLIOptions.mysqlhost)
+				.withDescription(
+						"mysql host. It assumes mysql is running on port 3306")
+				.hasArg().withArgName(AtlasCLIOptions.mysqlhost).create());
+
+		opt.addOption(OptionBuilder.withLongOpt(AtlasCLIOptions.dbname)
+				.withDescription("mysql db").hasArg()
+				.withArgName(AtlasCLIOptions.dbname).create());
+
+		opt.addOption(OptionBuilder.withLongOpt(AtlasCLIOptions.filepath)
+				.withDescription("json filename. The complete filepath ")
+				.hasArg().withArgName(AtlasCLIOptions.filepath).create());
+		
+		opt.addOption(OptionBuilder.withLongOpt(AtlasCLIOptions.ambariCluster)
+				.withDescription("Ambari Cluster Name")
+				.hasArg().withArgName(AtlasCLIOptions.ambariCluster).create());
+
+		opt.addOption(AtlasCLIOptions.listtype, false, "display all types");
+		
+		opt.addOption(
+				AtlasCLIOptions.createHive,
+				false,
+				"used with importmysql option. Indicating if hive_table types should also be created");
+		
+
+		opt.addOption(
+				AtlasCLIOptions.genLineage,
+				false,
+				"used with importmysql option. Indicating if hive_table and the the mysql tables should show as lineage");
+
+
+		opt.addOption("help", false, "requesting help");
+
+		HelpFormatter formatter = new HelpFormatter();
+
+		CommandLine line = parser.parse(opt, args);
+
+		if (line.hasOption("help") || args.length < 1) {
+			formatter.printHelp("atlasclient", opt);
+		}
+
+		if (line.hasOption(AtlasCLIOptions.url)) {
+			baseurl = line.getOptionValue(AtlasCLIOptions.url);
+
+			this.aClient = new AtlasClient(baseurl);
+
+		} else {
+			System.err.println("url is a mandatory field");
+			formatter.printHelp("atlasclient", opt);
+			System.exit(1);
+		}
+
+		// This will list you list all types
+		if (line.hasOption(AtlasCLIOptions.listtype)) {
+
+			this.listTypes();
+
+		}
+
+		if (line.hasOption(AtlasCLIOptions.action)) {
+
+			this.action = line.getOptionValue(AtlasCLIOptions.action);
+
+			String name = line.getOptionValue(AtlasCLIOptions.name);
+
+			if (AtlasCLIOptions.search.equalsIgnoreCase(this.action)) {
+				searchEntities(line);
+
+			} else if (AtlasCLIOptions.createSimpleType
+					.equalsIgnoreCase(this.action)) {
+
+				this.createSimpleType(line);
+
+			} else if (AtlasCLIOptions.createDataSetType
+					.equalsIgnoreCase(this.action)) {
+
+				this.createDataSetType(line);
+
+			} else if (AtlasCLIOptions.createProcessType
+					.equalsIgnoreCase(this.action)) {
+
+				this.createProcessType(line);
+
+			} else if (AtlasCLIOptions.createSimpleEntity
+					.equalsIgnoreCase(this.action)) {
+
+				this.createSimpleEntity(line);
+
+			} else if (AtlasCLIOptions.createDataSetEntity
+					.equalsIgnoreCase(this.action)) {
+
+				this.createDataSetEntity(line);
+
+			} else if (AtlasCLIOptions.createProcessEntity
+					.equalsIgnoreCase(this.action)) {
+
+				this.createProcessEntity(line);
+
+			} else if (AtlasCLIOptions.createrait.equalsIgnoreCase(this.action)) {
+
+				this.createTraitType(line);
+
+			} else if (AtlasCLIOptions.importMySqlTables
+					.equalsIgnoreCase(this.action)) {
+
+				try {
+					this.importMysqlTables(line);
+				} catch (Exception e) {
+
+					e.printStackTrace();
+				}
+
+			} else if (AtlasCLIOptions.loadtraithierarchy
+					.equalsIgnoreCase(this.action)) {
+				this.loadHierarchy(line);
+
+			}
+
+			else {
+				formatter.printHelp("Usage:", opt);
+			}
+
+		}
+
+	}
+
+	/**
+	 * This method invoked the AtlasEntity Search
+	 * 
+	 * @param line
+	 */
+	private void searchEntities(CommandLine line) {
+
+		try {
+
+			AtlasEntitySearch aES = new AtlasEntitySearch(baseurl);
+			String type_name = line.getOptionValue(AtlasCLIOptions.type);
+			String value = line.getOptionValue(AtlasCLIOptions.name);
+			Referenceable ref = aES.getReferenceByName(type_name, value);
+
+			String entityJSON = InstanceSerialization.toJson(ref, true);
+
+			System.out.println("Search Result:");
+			System.out.println(entityJSON);
+
+		} catch (Exception e) {
+			e.printStackTrace();
+			System.exit(1);
+		}
+
+	}
+
+	/**
+	 * 
+	 * @param line
+	 */
+	private void createSimpleType(CommandLine line) {
+
+		try {
+
+			AtlasTypeDefCreator ad = new AtlasTypeDefCreator(baseurl);
+
+			String classtypename = line.getOptionValue(AtlasCLIOptions.type);
+			String traitname = line.getOptionValue(AtlasCLIOptions.traitnames);
+			String parentype = line.getOptionValue(AtlasCLIOptions.supertype);
+
+			String typeJson = ad.assembleSimpleType(traitname, classtypename,
+					parentype);
+
+			JSONObject createType = this.aClient.createType(typeJson);
+
+			System.out.println(createType.toString());
+
+		} catch (Exception e) {
+			e.printStackTrace();
+		}
+
+	}
+
+	/**
+	 * 
+	 * @param typename
+	 */
+	private void createTraitType(CommandLine line) {
+
+		try {
+
+			String traittype = line
+					.getOptionValue(AtlasCLIOptions.traitTypename);
+			String parenttrait = line
+					.getOptionValue(AtlasCLIOptions.parentTraitName);
+
+			Taxonomy tx = new Taxonomy();
+
+			String traitJson = tx.createTraitTypes(traittype, parenttrait);
+
+			this.aClient.createType(traitJson);
+
+			System.out.println("Trait Created with name :" + traittype);
+
+		} catch (Exception e) {
+			e.printStackTrace();
+		}
+
+	}
+
+	/**
+	 * This create a Process Type
+	 * 
+	 * @param processName
+	 */
+	private void createProcessType(CommandLine line) {
+		try {
+			AtlasTypeDefCreator ad = new AtlasTypeDefCreator(baseurl);
+			String typename = line.getOptionValue(AtlasCLIOptions.type);
+			String typeJson = ad.assembleProcessType(null, typename);
+			aClient.createType(typeJson);
+
+		} catch (Exception e) {
+			e.printStackTrace();
+		}
+
+	}
+
+	/**
+	 * 
+	 * @param dataSetTypeName
+	 */
+	private void createDataSetType(CommandLine line) {
+		try {
+			AtlasTypeDefCreator ad = new AtlasTypeDefCreator(baseurl);
+			String typename = line.getOptionValue(AtlasCLIOptions.type);
+
+			String typeJson = ad.assembleDataSetType(null, typename);
+
+			System.out.println(typeJson);
+			aClient.createType(typeJson);
+
+		} catch (Exception e) {
+			e.printStackTrace();
+		}
+
+	}
+
+	/**
+	 * 
+	 * @param type
+	 * @param name
+	 * @param description
+	 */
+	private void createSimpleEntity(CommandLine line) {
+
+		AtlasEntityCreator aec = new AtlasEntityCreator(baseurl);
+		String typename = line.getOptionValue(AtlasCLIOptions.type);
+		String name = line.getOptionValue(AtlasCLIOptions.name);
+		String description = line.getOptionValue(AtlasCLIOptions.description);
+
+		if (description == null || "".equals(description)) {
+			description = "This is is entity of type :" + typename
+					+ " with name:" + name;
+		}
+
+		try {
+			Referenceable createuniveralEntity = aec.createRefObject(typename,
+					name, description);
+			aec.createEntity(createuniveralEntity);
+
+		} catch (Exception e) {
+
+			e.printStackTrace();
+		}
+
+	}
+
+	/**
+	 * 
+	 * 
+	 */
+	private void createDataSetEntity(CommandLine line) {
+		AtlasEntityCreator aec = new AtlasEntityCreator(baseurl);
+		try {
+
+			String type = line.getOptionValue(AtlasCLIOptions.type);
+			String name = line.getOptionValue(AtlasCLIOptions.name);
+			String description = line
+					.getOptionValue(AtlasCLIOptions.description);
+
+			if (description == null || "".equals(description)) {
+				description = "This is is entity of type :" + type
+						+ " with name:" + name;
+			}
+
+			String traitnames = line.getOptionValue(AtlasCLIOptions.traitnames);
+
+			// TODO
+			// This needs to be replaced by columns
+
+			List<Referenceable> timeDimColumns = ImmutableList.of(
+					aec.rawColumn("time_id", "int", "time id"),
+					aec.rawColumn("dayOfYear", "int", "day Of Year"),
+					aec.rawColumn("weekDay", "int", "week Day"));
+
+			Referenceable referenceable;
+			if (traitnames != null)
+				referenceable = new Referenceable(type, traitnames);
+			else
+				referenceable = new Referenceable(type);
+			referenceable.set("name", name);
+			referenceable.set("description", description);
+			referenceable.set("createTime", System.currentTimeMillis());
+			referenceable.set("lastAccessTime", System.currentTimeMillis());
+
+			referenceable.set("columns", timeDimColumns);
+
+			aec.createEntity(referenceable);
+
+		} catch (Exception e) {
+			// TODO Auto-generated catch block
+			e.printStackTrace();
+		}
+	}
+
+	/**
+	 * This creates a process Entity
+	 * 
+	 */
+	private void createProcessEntity(CommandLine line) {
+
+		String type = line.getOptionValue(AtlasCLIOptions.type);
+		String description = line.getOptionValue(AtlasCLIOptions.description);
+		String name = line.getOptionValue(AtlasCLIOptions.name);
+
+		if (description == null || "".equals(description)) {
+			description = "This is is entity of type :" + type;
+		}
+
+		String inp_type_name = line.getOptionValue(AtlasCLIOptions.inp_type);
+		String inp_value = line.getOptionValue(AtlasCLIOptions.inp_value);
+		String out_type_name = line.getOptionValue(AtlasCLIOptions.out_type);
+		String out_value = line.getOptionValue(AtlasCLIOptions.out_value);
+		String traitname = line.getOptionValue(AtlasCLIOptions.traitnames);
+
+		AtlasEntitySearch aES = new AtlasEntitySearch(baseurl);
+		AtlasEntityConnector aec = new AtlasEntityConnector();
+		Referenceable inpref;
+		try {
+
+			inpref = aES.getReferenceByName(inp_type_name, inp_value);
+
+			Referenceable outref = aES.getReferenceByName(out_type_name,
+					out_value);
+
+			Referenceable proc = aec.loadProcess(type, name, description,
+					ImmutableList.of(inpref.getId()),
+					ImmutableList.of(outref.getId()), traitname);
+
+			createEntity(proc);
+			System.out.println(" Lineage formed in Atlas with name " + name
+					+ " of type " + type);
+		} catch (Exception e) {
+			// TODO Auto-generated catch block
+			e.printStackTrace();
+		}
+
+	}
+
+	/**
+	 * This method lists all types in the Atlas
+	 */
+	public void listTypes() {
+
+		List<String> lt;
+		try {
+			lt = aClient.listTypes();
+
+			ListIterator<String> lstItr = lt.listIterator();
+
+			System.out.println("Listing all Types in atlas: ");
+
+			while (lstItr.hasNext()) {
+
+				System.out.println(lstItr.next());
+
+			}
+		} catch (AtlasServiceException e) {
+			// TODO Auto-generated catch block
+			e.printStackTrace();
+		}
+
+	}
+	
+	
+	
+
+	/**
+	 * This method imports all the table metadata from mysqltable and loads it
+	 * to Atlas
+	 * 
+	 * @param line
+	 * @throws Exception
+	 */
+	public void importMysqlTables(CommandLine line) throws Exception {
+
+		String host = line.getOptionValue(AtlasCLIOptions.mysqlhost);
+		String db = line.getOptionValue(AtlasCLIOptions.dbname);
+		String user = line.getOptionValue(AtlasCLIOptions.username);
+		String password = line.getOptionValue(AtlasCLIOptions.password);
+		String url = line.getOptionValue(AtlasCLIOptions.url);
+
+		boolean hiveexecflag = line.hasOption(AtlasCLIOptions.createHive) ? true
+				: false;
+		
+		//System.out.println(hiveexecflag);
+				
+		boolean genlineage = line.hasOption(AtlasCLIOptions.genLineage) ? true
+				: false;
+		String cluster = line.hasOption(AtlasCLIOptions.ambariCluster) ? line
+				.getOptionValue(AtlasCLIOptions.ambariCluster) : null;
+
+		System.out.println(cluster);
+						
+		AtlasTableInterface atlasTabIn = new AtlasTableInterface(url, host, db,
+				user, password, hiveexecflag, cluster, genlineage);
+
+	}
+
+	
+	
+	
+	
+	/**
+	 * 
+	 * @param line
+	 */
+	public void loadHierarchy(CommandLine line) {
+
+		String path = line.getOptionValue(AtlasCLIOptions.filepath);
+		JsonHierarchy jsn;
+
+		try {
+
+			jsn = new JsonHierarchy();
+			jsn.parseJSON(path);
+
+			ArrayList<EntityModel> emList = jsn.getEmList();
+			ArrayList<TupleModel> tmModel = jsn.getTmapList();
+
+			/**
+			 * Create the Traits
+			 */
+
+			ListIterator<TupleModel> arMdl = tmModel.listIterator();
+			TupleModel tpM = null;
+			String trait = null;
+			String supertrait = null;
+			Taxonomy tx1 = new Taxonomy();
+
+			System.out.print("Getting new Client");
+			// NewAtlasClient nac = new NewAtlasClient(this.baseurl);
+
+			List<String> tl = null;
+			try {
+				tl = this.aClient.listTypes();
+			} catch (Exception e1) {
+				// TODO Auto-generated catch block
+				e1.printStackTrace();
+			}
+
+			while (arMdl.hasNext()) {
+				try {
+					tpM = arMdl.next();
+					trait = tpM.getCurrnode();
+					supertrait = tpM.getParentnode();
+
+					if (!tl.contains(trait)) {
+						String traitJson = tx1.createTraitTypes(trait,
+								supertrait);
+						System.out.println(trait + " created..");
+
+						this.aClient.createType(traitJson);
+
+					} else {
+						System.out.println(trait + " exists. Skipping");
+						continue;
+					}
+				} catch (AtlasServiceException e) {
+					// TODO Auto-generated catch block
+					e.printStackTrace();
+				}
+
+			}
+
+			/**
+			 * Create Entities
+			 */
+
+			ListIterator<EntityModel> lsiEM = emList.listIterator();
+			EntityModel em = null;
+			String type = null;
+			String name = null;
+			trait = null;
+			Referenceable ref;
+			String Id;
+			Struct stc;
+			AtlasEntitySearch aES;
+			String type_name;
+			String value;
+
+			while (lsiEM.hasNext()) {
+
+				em = lsiEM.next();
+				name = em.getName();
+				trait = em.getParent();
+
+				aES = new AtlasEntitySearch(baseurl);
+				type_name = em.getType();
+				value = em.getName();
+
+				try {
+					ref = aES.getReferenceByName(type_name, value);
+					if (ref != null) {
+						Id = ref.getId()._getId();
+						stc = new Struct(trait);
+						this.addTrait(Id, stc);
+
+						System.out.println(String.format(
+								"Trait %s update to entity %s", trait, value));
+					} else {
+
+						System.out
+								.println(String
+										.format("Entity %s, not found in Atlas. Trait not added",
+												trait, value));
+					}
+
+				} catch (Exception e) {
+					// TODO Auto-generated catch block
+					e.printStackTrace();
+				}
+
+			}
+
+		} catch (JsonParseException e) {
+			// TODO Auto-generated catch block
+			e.printStackTrace();
+		} catch (IOException e) {
+			// TODO Auto-generated catch block
+			e.printStackTrace();
+		}
+	}
+
+	
+	
+	/**
+	 * 
+	 * This is a generic method of creating entities of any class
+	 * 
+	 * @throws JSONException
+	 * @throws AtlasServiceException
+	 * @throws com.hortonworks.atlas.client.AtlasServiceException
+	 * 
+	 */
+	public Id createEntity(Referenceable ref) throws JSONException,
+			AtlasServiceException, AtlasServiceException {
+
+		String typename = ref.getTypeName();
+
+		String entityJSON = InstanceSerialization.toJson(ref, true);
+
+		System.out.println("Submitting new entity= " + entityJSON);
+
+		JSONObject jsonObject = aClient.createEntity(entityJSON);
+
+		String guid = jsonObject.getString(AtlasClient.GUID);
+
+		System.out.println("created instance for type " + typename + ", guid: "
+				+ guid);
+
+		// return the Id for created instance with guid
+
+		return new Id(guid, ref.getId().getVersion(), ref.getTypeName());
+
+	}
+	
+	
+
+	/**
+	 * 
+	 * @param guid
+	 * @param trait
+	 * @throws Exception
+	 */
+	private void addTrait(String guid, Struct trait) throws Exception {
+		NewAtlasClient newatlasClient = new NewAtlasClient(this.baseurl);
+		// String guid = referenceable.getId()._getId();
+		List<String> existTraits = newatlasClient.getTraitNames(guid);
+
+		if (existTraits == null || !existTraits.contains(trait.typeName)) {
+			String traitJson = InstanceSerialization.toJson(trait, true);
+			newatlasClient.addTrait(guid, traitJson);
+		}
+	}
+
+}
diff --git a/codesamples/atlas/src/main/java/com/hortonworks/atlas/cli/AtlasCLIOptions.java b/codesamples/atlas/src/main/java/com/hortonworks/atlas/cli/AtlasCLIOptions.java
new file mode 100644
index 0000000..294930e
--- /dev/null
+++ b/codesamples/atlas/src/main/java/com/hortonworks/atlas/cli/AtlasCLIOptions.java
@@ -0,0 +1,52 @@
+package com.hortonworks.atlas.cli;
+
+public class AtlasCLIOptions {
+
+	
+	public static String url = "url";
+	public static String action = "c";
+	
+	
+	public static String type = "type";
+	public static String name = "name";
+	public static String description = "description";
+	public static String supertype = "parenttype";
+	public static String listtype = "listtype";
+	
+	public static String inp_type = "inptype";
+
+	public static String inp_value = "inpvalue";
+
+	public static String out_type = "outtype";
+
+	public static String out_value = "outvalue";
+	
+	public static String traitnames = "traitnames";
+	public static String traitTypename = "traittype";
+	public static String parentTraitName = "parenttrait";
+	public static String filepath = "jsonfilepath";
+	
+	
+	
+	public static String search = "search";
+	public static String createSimpleType= "createSimpleType";
+	public static String createDataSetType= "createDatasetType";
+	public static String createProcessType= "createProcessType";
+	public static String createSimpleEntity = "createSimpleEntity";
+	public static String createDataSetEntity = "createDataSetEntity";
+	public static String createProcessEntity = "createProcessEntity";
+	public static String bindProcess = "bindprocess";
+	public static String createrait = "createtrait";
+	public static String loadtraithierarchy = "loadtraithierarchy";
+	
+	public static String importMySqlTables = "importmysql";
+	public static String dbname = "db";
+	public static String username = "username";
+	public static String password = "password";
+	public static String mysqlhost = "mysqlhost";
+	public static String createHive = "createHiveTables";
+	public static String ambariCluster = "ambariClusterName";
+	public static String genLineage = "genLineage";
+	
+	
+}
diff --git a/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/AtlasEntityConnector.java b/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/AtlasEntityConnector.java
new file mode 100644
index 0000000..ce52648
--- /dev/null
+++ b/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/AtlasEntityConnector.java
@@ -0,0 +1,118 @@
+package com.hortonworks.atlas.client;
+
+import java.util.List;
+
+import org.apache.atlas.AtlasClient;
+import org.apache.atlas.AtlasServiceException;
+import org.apache.atlas.typesystem.Referenceable;
+import org.apache.atlas.typesystem.json.InstanceSerialization;
+import org.apache.atlas.typesystem.persistence.Id;
+import org.apache.atlas.typesystem.types.DataTypes;
+import org.codehaus.jettison.json.JSONException;
+import org.codehaus.jettison.json.JSONObject;
+
+import com.google.common.collect.ImmutableList;
+
+public class AtlasEntityConnector {
+
+
+	{
+		System.setProperty("atlas.conf", "./conf");
+	}
+	
+	private  AtlasClient ac = null;
+	
+	
+	public static void main(String[] args) throws Exception {
+	
+		if(args.length < 0)
+			throw new Exception("Please pass the atlas base url");
+		
+		String baseurl = args[0];
+		String inp_type_name = args[1];
+		String inp_value = args[2];
+		
+		String out_type_name = args[3];
+		String out_value = args[4];
+		
+		System.out.println(baseurl);
+		System.out.println(inp_type_name);
+		System.out.println(inp_value);
+		
+		
+		System.out.println(" Baseurl" + baseurl);
+		AtlasEntitySearch aES = new AtlasEntitySearch(baseurl);
+		AtlasEntityConnector aec = new AtlasEntityConnector();
+		aec.ac = aES.getAtlasClient();
+		
+		try {
+			
+			Referenceable inpref = aES.getReferenceByName(inp_type_name, inp_value);
+			
+			Referenceable outref = aES.getReferenceByName(out_type_name, out_value);
+		
+			Referenceable proc = aec.loadProcess(AtlasTypeDefCreator.Type_New_Life, "AsteroidConnector", "This  connects 2 Asteroids", ImmutableList.of(inpref.getId()), ImmutableList.of(outref.getId()), "Red");
+			
+			aec.createEntity(proc);
+			
+			System.out.println(" The 2 objects are connected");
+			
+			
+		} catch (Exception e) {
+			// TODO Auto-generated catch block
+			e.printStackTrace();
+		}
+	}
+
+	
+	 /*
+	  * 
+	  * 
+	  */
+	public Referenceable loadProcess(String type, String name, String description, List<Id> inputTables, List<Id> outputTables,
+           String... traitNames)
+   throws Exception {
+       Referenceable referenceable = new Referenceable(type, traitNames);
+       // super type attributes
+       referenceable.set("name", name);
+       referenceable.set("description", description);
+       referenceable.set("inputs", inputTables);
+       referenceable.set("outputs", outputTables);
+       referenceable.set("userName", "sdutta");
+       referenceable.set("startTime", System.currentTimeMillis());
+       referenceable.set("endTime", System.currentTimeMillis());	
+       
+       return referenceable;
+
+	}
+	
+	
+	/**
+	 * 
+	 * This is a generic method of creating entities of any class
+	 * @throws JSONException 
+	 * @throws AtlasServiceException 
+	 * 
+	 */
+	public Id createEntity(Referenceable ref ) throws JSONException, AtlasServiceException{
+		
+		String typename = ref.getTypeName(); 
+		
+		String entityJSON = InstanceSerialization.toJson(ref, true);
+		
+		System.out.println("Submitting new entity= " + entityJSON);
+        
+        JSONObject jsonObject = ac.createEntity(entityJSON);
+       
+        String guid = jsonObject.getString(AtlasClient.GUID);
+        
+        System.out.println("created instance for type " + typename + ", guid: " + guid);
+
+        // return the Id for created instance with guid
+       
+        return new Id(guid, ref.getId().getVersion(), ref.getTypeName());
+		
+		
+	}
+
+}
diff --git a/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/AtlasEntityCreator.java b/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/AtlasEntityCreator.java
new file mode 100644
index 0000000..f43d71c
--- /dev/null
+++ b/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/AtlasEntityCreator.java
@@ -0,0 +1,253 @@
+package com.hortonworks.atlas.client;
+
+import java.util.List;
+
+import org.apache.atlas.AtlasClient;
+import org.apache.atlas.AtlasServiceException;
+import org.apache.atlas.typesystem.Referenceable;
+import org.apache.atlas.typesystem.json.InstanceSerialization;
+import org.apache.atlas.typesystem.persistence.Id;
+import org.apache.atlas.typesystem.types.AttributeDefinition;
+import org.apache.atlas.typesystem.types.ClassType;
+import org.apache.atlas.typesystem.types.DataTypes;
+import org.apache.atlas.typesystem.types.HierarchicalTypeDefinition;
+import org.apache.atlas.typesystem.types.IDataType;
+import org.apache.atlas.typesystem.types.Multiplicity;
+import org.apache.atlas.typesystem.types.utils.TypesUtil;
+import org.codehaus.jettison.json.JSONArray;
+import org.codehaus.jettison.json.JSONException;
+import org.codehaus.jettison.json.JSONObject;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.ImmutableList;
+
+
+/**
+ * This is a sample class to instantiate Entities for existing entities
+ * 
+ * @author sdutta
+ *
+ */
+public class AtlasEntityCreator {
+
+	{
+		System.setProperty("atlas.conf", "conf");
+	}
+	
+	private  AtlasClient ac = null;
+	
+	
+	/**
+	 * 
+	 * @param args
+	 * @throws Exception
+	 */
+	public static void main(String[] args) throws Exception {
+		// TODO Auto-generated method stub
+
+		if(args.length < 0)
+			throw new Exception("Please pass the atlas base url");
+		
+		String baseurl = args[0];
+		
+		System.out.println(" Baseurl" + baseurl);
+		AtlasEntityCreator aEC = new AtlasEntityCreator(baseurl);
+		//aEC.defineSimpleEntities();
+		
+		aEC.defineDataSetEntities();
+		
+		aEC.destroy();
+		
+	}
+	
+	
+	
+	/**
+	 * We created 3 types in the previous example
+	 * @throws Exception 
+	 * 
+	 * 
+	
+	private void defineSimpleEntities() throws Exception {
+		
+		Referenceable createuniveralEntity = this.createRefObjectWithTraits(AtlasTypeDefCreator.Type_GOD, "Level2_Prod", "Level 2 type company", "Product");
+		
+		//Referenceable creategeneralEntity = createRefObjectWithTraits(AtlasTypeDefCreator.Type_Planets, "Level2", "Level2 of type Prioduct","Product");
+         
+		Id universalObjId = this.createEntity(createuniveralEntity);
+		
+		//Id generalObjId = this.createEntity(creategeneralEntity);
+		
+		
+	} */
+	
+	 
+	
+	/**
+	 * 
+	 * @throws Exception
+	 */
+	private void defineDataSetEntities() throws Exception {
+
+		List<Referenceable> timeDimColumns = ImmutableList
+                .of(rawColumn("time_id", "int", "time id"), rawColumn("dayOfYear", "int", "day Of Year"),
+                        rawColumn("weekDay", "int", "week Day"));
+		
+		  Id Aster1 =
+	                createDataSetType("Aster1", "customer dimension table", timeDimColumns, 1000, "1 lightyear",  
+	                        "White");
+		
+		  List<Referenceable> spaceDimColumns = ImmutableList
+	                .of(rawColumn("space_id", "int", "space id"), rawColumn("timeOfYear", "int", "time Of Year"),
+	                        rawColumn("weekofDay", "int", "week Day"));
+			
+			  Id Aster2 =
+		                createDataSetType("Aster2", "customer dimension table", timeDimColumns, 1000, "1 lightyear",  
+		                        "White");
+			
+			 
+		
+		
+	}
+	 
+	
+	/**
+	 * 
+	 * @param name
+	 * @param description
+	 * @param columns
+	 * @param traitNames
+	 * @return
+	 * @throws Exception
+	 */
+	 
+	public Id createDataSetType(String name, String description, List<Referenceable> columns, int speed, String dist, String... traitNames) throws Exception {
+	        Referenceable referenceable = new Referenceable(AtlasTypeDefCreator.Type_Asteroids, traitNames);
+	        referenceable.set("name", name);
+	        referenceable.set("description", description);
+	        referenceable.set("createTime", System.currentTimeMillis());
+	        referenceable.set("lastAccessTime", System.currentTimeMillis());
+	        referenceable.set("speed", speed);
+	        referenceable.set("distance_frm_Earth", dist);
+	
+	        referenceable.set("columns", columns);
+
+	        return createEntity(referenceable);
+	    }
+	 
+	 
+	
+	/**
+	 * This method creates a simple entities
+	 * 
+	 * @param name
+	 * @param comment
+	 * @return
+	 * @throws Exception
+	 */
+	 public Referenceable createRefObject(String type, String name, String description, String... traits)
+	    	    throws Exception {
+	    	        Referenceable referenceable = new Referenceable(type, traits);
+	    	        referenceable.set("name", name);
+	    	        referenceable.set("description", description);
+	    	        referenceable.set("createTime", System.currentTimeMillis());
+	    	        referenceable.set("lastAccessTime", System.currentTimeMillis());
+	    	        	    	    
+	    	        return referenceable;
+	    	    }
+	 
+	 
+	 /**
+		 * This method creates a simple entities
+		 * 
+		 * @param name
+		 * @param comment
+		 * @return
+		 * @throws Exception
+		 */
+		 public Referenceable createRefObjectWithTraits(String type, String name, String description,String... traits)
+		    	    throws Exception {
+		    	        Referenceable referenceable = new Referenceable(type, traits);
+		    	        referenceable.set("name", name);
+		    	        referenceable.set("description", description);
+		    	        	    	    
+		    	        return referenceable;
+		    	    }
+	
+	  
+	/**
+	 * 
+	 * @param baseurl
+	 */
+	public  AtlasEntityCreator(String baseurl){
+		
+		System.out.println("Creating Client Connection" + baseurl);
+		ac = new AtlasClient(baseurl);
+		System.out.println("Client Object returned");
+	
+		
+	}
+	
+	
+
+	
+	/**
+	 * 
+	 * This is a generic method of creating entities of any class
+	 * @throws JSONException 
+	 * @throws AtlasServiceException 
+	 * 
+	 */
+	public Id createEntity(Referenceable ref ) throws JSONException, AtlasServiceException{
+		
+		String typename = ref.getTypeName(); 
+		
+		String entityJSON = InstanceSerialization.toJson(ref, true);
+		
+		System.out.println("Submitting new entity= " + entityJSON);
+        
+        JSONObject jsonObject = ac.createEntity(entityJSON);
+       
+        String guid = jsonObject.getString(AtlasClient.GUID);
+        
+        System.out.println("created instance for type " + typename + ", guid: " + guid);
+
+        // return the Id for created instance with guid
+       
+        return 
+        		new Id(guid, ref.getId().getVersion(), ref.getTypeName());
+		
+		
+	}
+	
+	
+	/**
+	 * Creates a rowCloumn
+	 * @param name
+	 * @param dataType
+	 * @param comment
+	 * @param traitNames
+	 * @return
+	 * @throws Exception
+	 */
+	public Referenceable rawColumn(String name, String dataType, String comment, String... traitNames) throws Exception {
+	        Referenceable referenceable = new Referenceable(AtlasTypeDefCreator.COLUMN_TYPE, traitNames);
+	        referenceable.set("name", name);
+	        referenceable.set("dataType", dataType);
+	        referenceable.set("comment", comment);
+
+	        return referenceable;
+	    }
+	  
+	
+	/**
+	 * 
+	 * 
+	 */
+	public void destroy() {
+		
+		ac = null;
+	}
+
+	
+}
diff --git a/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/AtlasEntitySearch.java b/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/AtlasEntitySearch.java
new file mode 100644
index 0000000..6d0fa39
--- /dev/null
+++ b/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/AtlasEntitySearch.java
@@ -0,0 +1,145 @@
+package com.hortonworks.atlas.client;
+
+import java.util.List;
+
+import org.apache.atlas.AtlasClient;
+import org.apache.atlas.typesystem.Referenceable;
+import org.apache.atlas.typesystem.json.InstanceSerialization;
+import org.apache.atlas.typesystem.persistence.Id;
+import org.codehaus.jettison.json.JSONArray;
+import org.codehaus.jettison.json.JSONObject;
+
+
+
+/**
+ * This code shows an example of searching for Entities by name
+ * @author sdutta
+ *
+ */
+public class AtlasEntitySearch {
+
+	{
+		System.setProperty("atlas.conf", "./conf");
+	}
+
+	private AtlasClient ac = null;
+
+ 
+	public void setAc(AtlasClient ac) {
+		this.ac = ac;
+	}
+
+	/**
+	 * 
+	 * @param args
+	 * @throws Exception
+	 */
+	public static void main(String[] args) throws Exception {
+
+		if (args.length < 0)
+			throw new Exception("Please pass the atlas base url");
+
+		String baseurl = args[0];
+		String type_name = args[1];
+		String value = args[2];
+
+		System.out.println(baseurl);
+		System.out.println(type_name);
+		System.out.println(value);
+
+		System.out.println(" Baseurl" + baseurl);
+		AtlasEntitySearch aES = new AtlasEntitySearch(baseurl);
+
+		try {
+
+			Referenceable ref = aES.getReferenceByName(type_name, value);
+
+			String entityJSON = InstanceSerialization.toJson(ref, true);
+
+			System.out.println(entityJSON);
+
+		} catch (Exception e) {
+			// TODO Auto-generated catch block
+			e.printStackTrace();
+		}
+
+		// aEC.destroy();
+
+	}
+
+	/**
+	 * 
+	 * @param baseurl
+	 */
+	public AtlasEntitySearch(String baseurl) {
+
+		System.out.println("Creating Client Connection" + baseurl);
+		ac = new AtlasClient(baseurl);
+		System.out.println("Client Object returned");
+
+	}
+	
+	/**
+	 * 
+	 * @param aclient
+	 */
+	public AtlasEntitySearch(AtlasClient aclient) {
+
+		
+		this.ac = aclient;
+		
+	}
+
+	/**
+	 * 
+	 * @param typeName
+	 * @param value
+	 * @return
+	 * @throws Exception
+	 */
+	public Referenceable getReferenceByName(String typeName, String value)
+			throws Exception {
+
+		System.out.println(String.format("Getting reference for Entity %s",
+				value));
+
+		String dslQuery = String.format("%s where %s = '%s'", typeName, "name",
+				value);
+
+		return getEntityReferenceFromDSL(typeName, dslQuery);
+	}
+
+	/*
+	 * This will get an entity Object
+	 */
+	private Referenceable getEntityReferenceFromDSL(String typeName,
+			String dslQuery) throws Exception {
+
+		AtlasClient dgiClient = ac;
+
+		JSONArray results = dgiClient.searchByDSL(dslQuery);
+		if (results.length() == 0) {
+			return null;
+		} else {
+			String guid;
+			JSONObject row = results.getJSONObject(0);
+
+			if (row.has("$id$")) {
+				guid = row.getJSONObject("$id$").getString("id");
+			} else {
+				guid = row.getJSONObject("_col_0").getString("id");
+			}
+
+			return new Referenceable(guid, typeName, null);
+		}
+	}
+
+	/*
+	 * 
+	 */
+	public AtlasClient getAtlasClient() {
+
+		return ac;
+	}
+
+}
diff --git a/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/AtlasServiceException.java b/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/AtlasServiceException.java
new file mode 100644
index 0000000..73e7542
--- /dev/null
+++ b/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/AtlasServiceException.java
@@ -0,0 +1,32 @@
+package com.hortonworks.atlas.client;
+
+import com.sun.jersey.api.client.ClientResponse;
+import org.codehaus.jettison.json.JSONException;
+
+public class AtlasServiceException extends Exception {
+    private ClientResponse.Status status;
+
+    public AtlasServiceException(NewAtlasClient.API api, Exception e) {
+        super("Metadata service API " + api + " failed", e);
+    }
+
+    public AtlasServiceException(NewAtlasClient.API api, ClientResponse response) {
+        super("Metadata service API " + api + " failed with status " +
+                response.getClientResponseStatus().getStatusCode() + "(" +
+                response.getClientResponseStatus().getReasonPhrase() + ") Response Body (" +
+                response.getEntity(String.class) + ")");
+        this.status = response.getClientResponseStatus();
+    }
+
+    public AtlasServiceException(Exception e) {
+        super(e);
+    }
+
+    public AtlasServiceException(String message, Exception e) {
+        super(message, e);
+    }
+
+    public ClientResponse.Status getStatus() {
+        return status;
+    }
+}
\ No newline at end of file
diff --git a/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/AtlasTypeDefCreator.java b/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/AtlasTypeDefCreator.java
new file mode 100644
index 0000000..5a233dc
--- /dev/null
+++ b/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/AtlasTypeDefCreator.java
@@ -0,0 +1,352 @@
+package com.hortonworks.atlas.client;
+
+import java.util.List;
+import java.util.ListIterator;
+
+import org.apache.atlas.AtlasClient;
+import org.apache.atlas.typesystem.TypesDef;
+import org.apache.atlas.typesystem.json.TypesSerialization;
+import org.apache.atlas.typesystem.types.AttributeDefinition;
+import org.apache.atlas.typesystem.types.ClassType;
+import org.apache.atlas.typesystem.types.DataTypes;
+import org.apache.atlas.typesystem.types.EnumTypeDefinition;
+import org.apache.atlas.typesystem.types.HierarchicalTypeDefinition;
+import org.apache.atlas.typesystem.types.IDataType;
+import org.apache.atlas.typesystem.types.Multiplicity;
+import org.apache.atlas.typesystem.types.StructTypeDefinition;
+import org.apache.atlas.typesystem.types.TraitType;
+import org.apache.atlas.typesystem.types.TypeUtils;
+import org.apache.atlas.typesystem.types.utils.TypesUtil;
+import org.apache.commons.math3.util.MultidimensionalCounter.Iterator;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.ImmutableList;
+
+import org.codehaus.jettison.json.JSONArray;
+import org.codehaus.jettison.json.JSONException;
+import org.codehaus.jettison.json.JSONObject;
+import org.apache.atlas.AtlasClient;
+import org.apache.atlas.AtlasServiceException;
+
+/*
+ * This is a class to create Type Definitions
+ * This is a simple class
+ * @author - Shivaji Dutta
+ */
+ 
+public class AtlasTypeDefCreator {
+
+	{
+		System.setProperty("atlas.conf", "conf");
+	}
+
+	private String traitName = "Green";
+
+	private AtlasClient ac = null;
+
+	private ImmutableList enumType = null;
+	private ImmutableList structType = null;
+	private ImmutableList classType = null;
+	private ImmutableList traitType = null;
+
+	/**
+	 * This assembles the type
+	 * If trait name is passed it creates a simple trait with the class name
+	 * if not, just the class type is created
+	 * 
+	 * @return
+	 * @throws Exception 
+	 */
+
+	public String assembleSimpleType(String traitName, String ClassTypeName, String parenttype) throws Exception {
+		
+
+		System.out.print(traitName+  ClassTypeName + parenttype);
+		
+		TypesDef tdef = TypeUtils.getTypesDef(ImmutableList.<EnumTypeDefinition> of(),
+				ImmutableList.<StructTypeDefinition> of(),
+				this.createTraitType(traitName),
+				this.createClassType(ClassTypeName, parenttype));
+		
+		
+
+		return TypesSerialization.toJson(tdef);
+	}
+
+	/**
+	 * This register the Process type
+	 * 
+	 * @return s
+	 */
+
+	public String assembleProcessType(String traitName, String ClassTypeName) {
+		TypesDef tdef = null;
+
+		tdef = TypeUtils.getTypesDef(ImmutableList.<EnumTypeDefinition> of(),
+				ImmutableList.<StructTypeDefinition> of(),
+				this.createTraitType(traitName),
+				ImmutableList.of(this.createProcessTypeByName(ClassTypeName)));
+
+		return TypesSerialization.toJson(tdef);
+	}
+
+	public String assembleDataSetType(String traitName, String ClassTypeName) {
+		TypesDef tdef = null;
+
+		tdef = TypeUtils.getTypesDef(ImmutableList.<EnumTypeDefinition> of(),
+				ImmutableList.<StructTypeDefinition> of(),
+				this.createTraitType(traitName),
+				ImmutableList.of(this.createDataSetTypeByName(ClassTypeName)));
+
+		return TypesSerialization.toJson(tdef);
+	}
+
+	/**
+	 * 
+	 * @param baseurl
+	 * @throws AtlasServiceException
+	 */
+	public AtlasTypeDefCreator(String baseurl) throws AtlasServiceException {
+
+		//System.out.println("Creating Client Connection" + baseurl);
+		ac = new AtlasClient(baseurl);
+		//System.out.println("Client Object returned");
+
+	}
+
+	AttributeDefinition attrDef(String name, IDataType dT, Multiplicity m,
+			boolean isComposite, String reverseAttributeName) {
+		Preconditions.checkNotNull(name);
+		Preconditions.checkNotNull(dT);
+		return new AttributeDefinition(name, dT.getName(), m, isComposite,
+				reverseAttributeName);
+	}
+
+	AttributeDefinition attrDef(String name, IDataType dT) {
+		return attrDef(name, dT, Multiplicity.OPTIONAL, false, null);
+	}
+
+	AttributeDefinition attrDef(String name, IDataType dT, Multiplicity m) {
+		return attrDef(name, dT, m, false, null);
+	}
+
+	//public static String Type_GOD = "GOD_Type";
+	//public static String Type_Planets = "Planet_Type";
+	//public static String Type_Forces = "Force_Type";
+	public static String Type_New_Life = "New_Life_Type";
+	public static String Type_Asteroids = "Asteroid_Type";
+	public static final String COLUMN_TYPE = "Column";
+
+	/**
+	 * This returns an ImmutableList of the type being created
+	 * 
+	 * @param typeName
+	 * @return
+	 * @throws Exception 
+	 */
+	@SuppressWarnings({ "unused", "unchecked" })
+	public ImmutableList<HierarchicalTypeDefinition<ClassType>> createClassType(
+			String typeName, String parenttype) throws Exception {
+
+		if(typeName == null ){
+			throw new Exception ("type or typename cannot be null");
+		}
+		
+		System.out.print("class type name" + typeName);
+		HierarchicalTypeDefinition<ClassType> genericType =  null;
+		if(parenttype != null)
+		   {
+			genericType = TypesUtil
+				.createClassTypeDef(typeName, ImmutableList.of(parenttype),
+						attrDef("name", DataTypes.STRING_TYPE),
+						attrDef("description", DataTypes.STRING_TYPE),
+						attrDef("createTime", DataTypes.INT_TYPE),
+						attrDef("lastAccessTime", DataTypes.INT_TYPE)
+						
+						);
+		
+		   } else{
+			   genericType = TypesUtil
+						.createClassTypeDef(typeName, null,
+								attrDef("name", DataTypes.STRING_TYPE),
+								attrDef("description", DataTypes.STRING_TYPE),
+								attrDef("createTime", DataTypes.INT_TYPE),
+								attrDef("lastAccessTime", DataTypes.INT_TYPE));
+				
+		   }
+		
+		this.classType = ImmutableList.of(genericType);
+		return this.classType;
+	}
+
+	/**
+	 * This lets you create Class Type
+	 * 
+	 * @return
+	 */
+	public HierarchicalTypeDefinition<ClassType> createProcessTypes() {
+
+		return TypesUtil.createClassTypeDef(this.Type_New_Life,
+				ImmutableList.of("Process"),
+				attrDef("userName", DataTypes.STRING_TYPE),
+				attrDef("startTime", DataTypes.INT_TYPE),
+				attrDef("endTime", DataTypes.INT_TYPE));
+
+	}
+
+	/**
+	 * This method helps you to create types
+	 * 
+	 * @param typeName
+	 * @return
+	 */
+	public HierarchicalTypeDefinition<ClassType> createProcessTypeByName(
+			String typeName) {
+
+		return TypesUtil.createClassTypeDef(typeName,
+				ImmutableList.of("Process"),
+				attrDef("userName", DataTypes.STRING_TYPE),
+				attrDef("startTime", DataTypes.INT_TYPE),
+				attrDef("endTime", DataTypes.INT_TYPE));
+
+	}
+
+	/**
+	 * 
+	 * @param typeName
+	 * @return
+	 */
+	public HierarchicalTypeDefinition<ClassType> createDataSetTypeByName(
+			String typeName) {
+
+		return TypesUtil.createClassTypeDef(
+				typeName,
+				ImmutableList.of("DataSet"),
+				attrDef("createTime", DataTypes.INT_TYPE),
+				attrDef("lastAccessTime", DataTypes.INT_TYPE),
+				new AttributeDefinition("columns", DataTypes
+						.arrayTypeName(COLUMN_TYPE), Multiplicity.COLLECTION,
+						true, null));
+
+	}
+
+	/**
+	 * This method helps for creating traits
+	 * 
+	 * @param trait
+	 * @return
+	 */
+	public ImmutableList<HierarchicalTypeDefinition<TraitType>> createTraitType(
+			String trait) {
+
+		if (trait != null)
+			return ImmutableList.of(TypesUtil.createTraitTypeDef(
+					trait, null));
+		else
+			return ImmutableList
+					.<HierarchicalTypeDefinition<TraitType>> of();
+
+		
+	}
+	
+	
+
+	/**
+	 * 
+	 * @param args
+	 * @throws Exception
+	 
+	public static void main(String[] args) throws Exception {
+
+		if (args.length < 0)
+			throw new Exception("Please pass the atlas base url");
+
+		String baseurl = args[0];
+
+		System.out.println(" Baseurl" + baseurl);
+		AtlasTypeDefCreator ad = new AtlasTypeDefCreator(baseurl);
+		ad.traitName = args[1];
+		// ad.registerTypes();
+
+	}
+	
+	*/
+
+	/**
+	 * This is for registering types
+	 * 
+	 * 
+	 * public void registerTypes() throws org.apache.atlas.AtlasServiceException
+	 * {
+	 * 
+	 * System.out.println("Registering Types"); ac.createType(assembleTypes());
+	 * System.out.println("Done Creating Types"); }
+	 */
+
+	/**
+	 * This is a sample code with hard coded values
+	 * 
+	 * @deprecated
+	 * @return
+	 * 
+	 *         ImmutableList<HierarchicalTypeDefinition<ClassType>>
+	 *         createClassTypes() {
+	 * 
+	 *         HierarchicalTypeDefinition<ClassType> UniversalTypes =
+	 * 
+	 *         TypesUtil.createClassTypeDef(this.Type_GOD, null, attrDef("name",
+	 *         DataTypes.STRING_TYPE), attrDef("description",
+	 *         DataTypes.STRING_TYPE));
+	 * 
+	 *         HierarchicalTypeDefinition<ClassType> GeneralTypes =
+	 * 
+	 *         TypesUtil.createClassTypeDef(this.Type_Planets, null,
+	 *         attrDef("name", DataTypes.STRING_TYPE), attrDef("description",
+	 *         DataTypes.STRING_TYPE));
+	 * 
+	 *         HierarchicalTypeDefinition<ClassType> ConnectorTypes =
+	 * 
+	 *         TypesUtil.createClassTypeDef(this.Type_Forces, null,
+	 *         attrDef("name", DataTypes.STRING_TYPE), attrDef("description",
+	 *         DataTypes.STRING_TYPE));
+	 * 
+	 *         HierarchicalTypeDefinition<ClassType> processtype = this
+	 *         .createProcessTypes();
+	 * 
+	 * 
+	 *         HierarchicalTypeDefinition<ClassType> columnClsDef = TypesUtil
+	 *         .createClassTypeDef(COLUMN_TYPE, null, attrDef("name",
+	 *         DataTypes.STRING_TYPE), attrDef("dataType",
+	 *         DataTypes.STRING_TYPE), attrDef("comment",
+	 *         DataTypes.STRING_TYPE));
+	 * 
+	 *         HierarchicalTypeDefinition<ClassType> asteroidDef = TypesUtil
+	 *         .createClassTypeDef( this.Type_Asteroids,
+	 *         ImmutableList.of("DataSet"), attrDef("createTime",
+	 *         DataTypes.INT_TYPE), attrDef("lastAccessTime",
+	 *         DataTypes.INT_TYPE), attrDef("speed", DataTypes.INT_TYPE),
+	 *         attrDef("distance_frm_Earth", DataTypes.STRING_TYPE), new
+	 *         AttributeDefinition("columns", DataTypes
+	 *         .arrayTypeName(COLUMN_TYPE), Multiplicity.COLLECTION, true,
+	 *         null));
+	 * 
+	 *         return // ImmutableList.of(UniversalTypes, GeneralTypes,
+	 *         ConnectorTypes); ImmutableList.of(columnClsDef, asteroidDef);
+	 * 
+	 *         }
+	 
+	 
+	 * The JSON Type String for the TypeDef
+	 * 
+	 * public String assembleTypes() {
+	 * 
+	 * TypesDef tdef = TypeUtils.getTypesDef( ImmutableList.<EnumTypeDefinition>
+	 * of(), ImmutableList.<StructTypeDefinition> of(),
+	 * this.createTraitType(traitName), this.createClassTypes());
+	 * 
+	 * return TypesSerialization.toJson(tdef);
+	 * 
+	 * }
+	 */
+
+}
diff --git a/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/Hierarchy.java b/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/Hierarchy.java
new file mode 100644
index 0000000..4650054
--- /dev/null
+++ b/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/Hierarchy.java
@@ -0,0 +1,8 @@
+package com.hortonworks.atlas.client;
+
+public interface Hierarchy<T> {
+	
+	public void parse() throws Exception;
+	
+
+}
diff --git a/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/HierarchyFactory.java b/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/HierarchyFactory.java
new file mode 100644
index 0000000..9928d8f
--- /dev/null
+++ b/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/HierarchyFactory.java
@@ -0,0 +1,10 @@
+package com.hortonworks.atlas.client;
+
+public class HierarchyFactory {
+
+	public static void main(String[] args) {
+		// TODO Auto-generated method stub
+
+	}
+
+}
diff --git a/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/HiveMetaDataGenerator.java b/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/HiveMetaDataGenerator.java
new file mode 100644
index 0000000..b16d572
--- /dev/null
+++ b/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/HiveMetaDataGenerator.java
@@ -0,0 +1,660 @@
+package com.hortonworks.atlas.client;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+
+import org.apache.atlas.AtlasClient;
+import org.apache.atlas.AtlasServiceException;
+import org.apache.atlas.hive.model.HiveDataModelGenerator;
+import org.apache.atlas.hive.model.HiveDataTypes;
+import org.apache.atlas.typesystem.Referenceable;
+import org.apache.atlas.typesystem.Struct;
+import org.apache.atlas.typesystem.json.InstanceSerialization;
+import org.apache.atlas.typesystem.persistence.Id;
+import org.apache.commons.lang.StringEscapeUtils;
+import org.apache.commons.lang.StringUtils;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.api.Database;
+import org.apache.hadoop.hive.metastore.api.FieldSchema;
+import org.apache.hadoop.hive.metastore.api.Index;
+import org.apache.hadoop.hive.metastore.api.Order;
+import org.apache.hadoop.hive.metastore.api.SerDeInfo;
+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
+import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;
+import org.apache.hadoop.hive.ql.metadata.Hive;
+import org.apache.hadoop.hive.ql.metadata.Partition;
+import org.apache.hadoop.hive.ql.metadata.Table;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.codehaus.jettison.json.JSONArray;
+import org.codehaus.jettison.json.JSONException;
+import org.codehaus.jettison.json.JSONObject;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.ImmutableList;
+import com.google.common.collect.ImmutableList.Builder;
+import com.hortonworks.atlas.adapter.Column;
+
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Set;
+
+/**
+ * A Bridge Utility that imports metadata from the Hive Meta Store
+ * and registers then in Atlas.
+ */
+public class HiveMetaDataGenerator {
+	
+	{
+		System.setProperty("atlas.conf", "/Users/sdutta/Applications/conf");
+	}
+	
+	
+    private static final String DEFAULT_DGI_URL = "http://localhost:21000/";
+    
+    public static final String DEFAULT_CLUSTER_NAME = "primary";
+    private static String clusterName = "atlasdemo";
+
+    public static final String DGI_URL_PROPERTY = "hive.hook.dgi.url";
+
+    private static final Logger LOG = LoggerFactory.getLogger(HiveMetaDataGenerator.class);
+
+    private final Hive hiveClient = null;
+    private  AtlasClient atlasClient;
+    private static String databasename = null;
+    private static String tablename = null;
+    
+    public static void main(String[] args) throws Exception {
+    	
+    	clusterName = args[1];
+    	String baseurl = args[0];
+    	databasename = args[2];
+    	tablename = args[3];
+    	
+    	
+    	
+    	HiveMetaDataGenerator hmg = new HiveMetaDataGenerator(baseurl);
+    	
+    	Referenceable db = hmg.registerDatabase(databasename, clusterName);
+    	hmg.registerTable(db, databasename, tablename);
+    	
+    }
+
+    /**
+     * 
+     * @param baseurl
+     */
+    public HiveMetaDataGenerator(String baseurl) {
+    	
+    	atlasClient = new AtlasClient(baseurl);
+    
+    }
+    
+    public HiveMetaDataGenerator(AtlasClient ac) {
+		// TODO Auto-generated constructor stub
+    	this.atlasClient = ac;
+	}
+
+	/**
+     * 
+     * @return
+     */
+    public AtlasClient getAtlasClient() {
+        return atlasClient;
+    }
+
+    /**
+     * 
+     * @param aC
+     */
+    public void setAtlasClient(AtlasClient aC) {
+        this.atlasClient =  aC;
+    }
+
+      
+
+    public Referenceable registerDatabase(String databaseName, String clusterName) throws Exception {
+        Referenceable dbRef = getDatabaseReference(databaseName, clusterName);
+        
+        if (dbRef == null) {
+            LOG.info("Importing objects from databaseName : " + databaseName);
+            //Database hiveDB = hiveClient.getDatabase(databaseName);
+
+            dbRef = new Referenceable(HiveDataTypes.HIVE_DB.getName());
+            dbRef.set(HiveDataModelGenerator.NAME, databaseName);
+            dbRef.set(HiveDataModelGenerator.CLUSTER_NAME, clusterName);
+            dbRef.set("description", "this is a default database");
+            dbRef.set("locationUri", "/hive/default");
+            //dbRef.set("parameters", "key1=name1,key2=name2");
+            dbRef.set("ownerName", "Hortonworks");
+            dbRef = createInstance(dbRef);
+            
+            
+        } else {
+            LOG.info("Database {} is already registered with id {}", databaseName, dbRef.getId().id);
+        }
+        
+        return dbRef;
+    }
+
+    /**
+     * 
+     * @param referenceable
+     * @return
+     * @throws Exception
+     */
+    public Referenceable createInstance(Referenceable referenceable) throws Exception {
+        String typeName = referenceable.getTypeName();
+        LOG.debug("creating instance of type " + typeName);
+
+        String entityJSON = InstanceSerialization.toJson(referenceable, true);
+        LOG.debug("Submitting new entity {} = {}", referenceable.getTypeName(), entityJSON);
+        JSONObject jsonObject = atlasClient.createEntity(entityJSON);
+        String guid = jsonObject.getString(AtlasClient.GUID);
+        LOG.debug("created instance for type " + typeName + ", guid: " + guid);
+
+        return new Referenceable(guid, referenceable.getTypeName(), null);
+    }
+
+   
+
+    /**
+     * Gets reference for the database
+     *
+     *
+     * @param databaseName  database Name
+     * @param clusterName    cluster name
+     * @return Reference for database if exists, else null
+     * @throws Exception
+     */
+    private Referenceable getDatabaseReference(String databaseName, String clusterName) throws Exception {
+        LOG.debug("Getting reference for database {}", databaseName);
+        String typeName = HiveDataTypes.HIVE_DB.getName();
+
+        String dslQuery = String.format("%s where %s = '%s' and %s = '%s'", typeName, HiveDataModelGenerator.NAME,
+                databaseName.toLowerCase(), HiveDataModelGenerator.CLUSTER_NAME, clusterName);
+        return getEntityReferenceFromDSL(typeName, dslQuery);
+    }
+
+    public Referenceable getProcessReference(String queryStr) throws Exception {
+        LOG.debug("Getting reference for process with query {}", queryStr);
+        String typeName = HiveDataTypes.HIVE_PROCESS.getName();
+
+        //todo enable DSL
+        //        String dslQuery = String.format("%s where queryText = \"%s\"", typeName, queryStr);
+        //        return getEntityReferenceFromDSL(typeName, dslQuery);
+
+        String gremlinQuery =
+                String.format("g.V.has('__typeName', '%s').has('%s.queryText', \"%s\").toList()", typeName, typeName,
+                        StringEscapeUtils.escapeJava(queryStr));
+        return getEntityReferenceFromGremlin(typeName, gremlinQuery);
+    }
+
+    private Referenceable getEntityReferenceFromDSL(String typeName, String dslQuery) throws Exception {
+        AtlasClient dgiClient = getAtlasClient();
+        JSONArray results = dgiClient.searchByDSL(dslQuery);
+        if (results.length() == 0) {
+            return null;
+        } else {
+            String guid;
+            JSONObject row = results.getJSONObject(0);
+            if (row.has("$id$")) {
+                guid = row.getJSONObject("$id$").getString("id");
+            } else {
+                guid = row.getJSONObject("_col_0").getString("id");
+            }
+            return new Referenceable(guid, typeName, null);
+        }
+    }
+
+    public static String getTableName(String clusterName, String dbName, String tableName) {
+        return String.format("%s.%s@%s", dbName.toLowerCase(), tableName.toLowerCase(), clusterName);
+    }
+
+    /**
+     * Gets reference for the table
+     *
+     * @param dbName database name
+     * @param tableName table name
+     * @return table reference if exists, else null
+     * @throws Exception
+     */
+    private Referenceable getTableReference(String dbName, String tableName) throws Exception {
+        LOG.debug("Getting reference for table {}.{}", dbName, tableName);
+
+        String typeName = HiveDataTypes.HIVE_TABLE.getName();
+        String entityName = getTableName(clusterName, dbName, tableName);
+        String dslQuery = String.format("%s as t where name = '%s'", typeName, entityName);
+        return getEntityReferenceFromDSL(typeName, dslQuery);
+    }
+
+    private Referenceable getEntityReferenceFromGremlin(String typeName, String gremlinQuery)
+    throws AtlasServiceException, JSONException {
+        AtlasClient client = getAtlasClient();
+        JSONObject response = client.searchByGremlin(gremlinQuery);
+        JSONArray results = response.getJSONArray(AtlasClient.RESULTS);
+        if (results.length() == 0) {
+            return null;
+        }
+        String guid = results.getJSONObject(0).getString("__guid");
+        return new Referenceable(guid, typeName, null);
+    }
+
+    private Referenceable getPartitionReference(String dbName, String tableName, List<String> values) throws Exception {
+        String valuesStr = "['" + StringUtils.join(values, "', '") + "']";
+        LOG.debug("Getting reference for partition for {}.{} with values {}", dbName, tableName, valuesStr);
+        String typeName = HiveDataTypes.HIVE_PARTITION.getName();
+
+        //todo replace gremlin with DSL
+        //        String dslQuery = String.format("%s as p where values = %s, tableName where name = '%s', "
+        //                        + "dbName where name = '%s' and clusterName = '%s' select p", typeName, valuesStr,
+        // tableName,
+        //                dbName, clusterName);
+
+        String datasetType = AtlasClient.DATA_SET_SUPER_TYPE;
+        String tableEntityName = getTableName(clusterName, dbName, tableName);
+
+        String gremlinQuery = String.format("g.V.has('__typeName', '%s').has('%s.values', %s).as('p')."
+                        + "out('__%s.table').has('%s.name', '%s').back('p').toList()", typeName, typeName, valuesStr,
+                typeName, datasetType, tableEntityName);
+
+        return getEntityReferenceFromGremlin(typeName, gremlinQuery);
+    }
+
+    private Referenceable getSDForTable(String dbName, String tableName) throws Exception {
+        Referenceable tableRef = getTableReference(dbName, tableName);
+        if (tableRef == null) {
+            throw new IllegalArgumentException("Table " + dbName + "." + tableName + " doesn't exist");
+        }
+
+        AtlasClient dgiClient = getAtlasClient();
+        Referenceable tableInstance = dgiClient.getEntity(tableRef.getId().id);
+        Id sdId = (Id) tableInstance.get("sd");
+        return new Referenceable(sdId.id, sdId.getTypeName(), null);
+    }
+
+    /**
+     * 
+     * @param dbName
+     * @param tableName
+     * @return
+     * @throws Exception
+     */
+    public Referenceable registerTable(String dbName, String tableName) throws Exception {
+        Referenceable dbReferenceable = registerDatabase(dbName, clusterName);
+        return registerTable(dbReferenceable, dbName, tableName);
+    }
+
+    
+    /**
+     * 
+     * @param dbReference
+     * @param dbName
+     * @param tableName
+     * @return
+     * @throws Exception
+     */
+    public Referenceable registerTable(Referenceable dbReference, String dbName, String tableName) throws Exception {
+        LOG.info("Attempting to register table [" + tableName + "]");
+        Referenceable tableRef = getTableReference(dbName, tableName);
+        
+        if (tableRef == null) {
+            LOG.info("Importing objects from " + dbName + "." + tableName);
+
+            //Table hiveTable = hiveClient.getTable(dbName, tableName);
+
+            tableRef = new Referenceable(HiveDataTypes.HIVE_TABLE.getName());
+            tableRef.set(HiveDataModelGenerator.NAME,
+                    getTableName(clusterName, dbName, tableName));
+            
+            tableRef.set(HiveDataModelGenerator.TABLE_NAME,tableName.toLowerCase());
+            tableRef.set("owner", "Hortonworks");
+
+            tableRef.set("createTime", System.currentTimeMillis());
+            tableRef.set("lastAccessTime",System.currentTimeMillis());
+            tableRef.set("retention", System.currentTimeMillis());
+
+            tableRef.set(HiveDataModelGenerator.COMMENT, "This is loaded by Sqoop job");
+
+            // add reference to the database
+            tableRef.set(HiveDataModelGenerator.DB, dbReference);
+            
+            List<Referenceable> timeDimColumns = ImmutableList
+                    .of(rawColumn("driver_id", "String", "Driver Id"), rawColumn("driver_name", "String", "Driver Name"),
+                            rawColumn("certified", "String", "certified_Y/N","PII"), rawColumn("wageplan", "String", "hours of weekly"));
+            
+            
+            tableRef.set("columns", timeDimColumns);
+            
+            // add reference to the StorageDescriptor
+            //StorageDescriptor storageDesc = hiveTable.getSd();
+            //Referenceable sdReferenceable = fillStorageDescStruct(storageDesc, colList);
+            //tableRef.set("sd", sdReferenceable);
+
+            // add reference to the Partition Keys
+            //List<Referenceable> partKeys = getColumns(hiveTable.getPartitionKeys());
+            //tableRef.set("partitionKeys", partKeys);
+
+           // tableRef.set("parameters", "params");
+
+            
+            tableRef.set("viewOriginalText", "Original text");
+           
+
+            
+           tableRef.set("viewExpandedText", "Expanded Text");
+            
+
+            tableRef.set("tableType", "Sqoop generated table");
+            tableRef.set("temporary", "false");
+
+
+            tableRef = createInstance(tableRef);
+            
+        } else {
+            LOG.info("Table {}.{} is already registered with id {}", dbName, tableName, tableRef.getId().id);
+        }
+        return tableRef;
+    }
+
+    
+    /**
+     * This is a generic method for mysql tables
+     * @param dbReference
+     * @param dbName
+     * @param tableName
+     * @return
+     * @throws Exception
+     */
+    public Referenceable registerExtTable(Referenceable dbReference, String dbName, String tableName, ArrayList<Column> clist) throws Exception {
+        LOG.info("Attempting to register table [" + tableName + "]");
+        Referenceable tableRef = getTableReference(dbName, tableName);
+        
+        if (tableRef == null) {
+        	
+            LOG.info("Importing objects from " + dbName + "." + tableName);
+
+            //Table hiveTable = hiveClient.getTable(dbName, tableName);
+
+            tableRef = new Referenceable(HiveDataTypes.HIVE_TABLE.getName());
+            tableRef.set(HiveDataModelGenerator.NAME,
+                    getTableName(clusterName, dbName, tableName));
+            
+            tableRef.set(HiveDataModelGenerator.TABLE_NAME,tableName.toLowerCase());
+            tableRef.set("owner", "Hortonworks");
+
+            tableRef.set("createTime", System.currentTimeMillis());
+            tableRef.set("lastAccessTime",System.currentTimeMillis());
+            tableRef.set("retention", System.currentTimeMillis());
+
+            tableRef.set(HiveDataModelGenerator.COMMENT, "This is loaded from mysql tables");
+
+            // add reference to the database
+            tableRef.set(HiveDataModelGenerator.DB, dbReference);
+            
+            Builder bld = ImmutableList.<Referenceable>builder();
+            Iterator<Column> itrc = clist.iterator();
+			ImmutableList<Referenceable> lst = ImmutableList.of();
+			Referenceable colref;
+			
+			
+			while (itrc.hasNext()){
+				
+				Column c = itrc.next();
+			    colref = this.rawColumn(c.getColumn_name(), c.getColumn_type(), c.getColumn_remarks());
+			    bld.add(colref);
+			    
+			}
+			
+			lst = bld.build();
+            
+            tableRef.set("columns", lst);
+            
+            
+            tableRef.set("viewOriginalText", "Original text");
+           
+
+            
+           tableRef.set("viewExpandedText", "Expanded Text");
+            
+
+            tableRef.set("tableType", "Mysql Imported table");
+            tableRef.set("temporary", "false");
+
+
+            tableRef = createInstance(tableRef);
+            
+        } else {
+            LOG.info("Table {}.{} is already registered with id {}", dbName, tableName, tableRef.getId().id);
+        }
+        return tableRef;
+    }
+    
+    /**
+     * 
+     * @param db
+     * @param tableName
+     * @param tableReferenceable
+     * @param sdReferenceable
+     * @throws Exception
+     */
+    private void registerPartitions(String db, String tableName, Referenceable tableReferenceable,
+            Referenceable sdReferenceable) throws Exception {
+        Set<Partition> tableParts = hiveClient.getAllPartitionsOf(new Table(Table.getEmptyTable(db, tableName)));
+
+        if (tableParts.size() > 0) {
+            for (Partition hivePart : tableParts) {
+                registerPartition(hivePart, tableReferenceable, sdReferenceable);
+            }
+        }
+    }
+
+    /**
+     * 
+     * @param partition
+     * @return
+     * @throws Exception
+     */
+    public Referenceable registerPartition(Partition partition) throws Exception {
+        String dbName = partition.getTable().getDbName();
+        String tableName = partition.getTable().getTableName();
+        Referenceable tableRef = registerTable(dbName, tableName);
+        Referenceable sdRef = getSDForTable(dbName, tableName);
+        return registerPartition(partition, tableRef, sdRef);
+    }
+
+    private Referenceable registerPartition(Partition hivePart, Referenceable tableReferenceable,
+            Referenceable sdReferenceable) throws Exception {
+        LOG.info("Registering partition for {} with values {}", tableReferenceable,
+                StringUtils.join(hivePart.getValues(), ","));
+        String dbName = hivePart.getTable().getDbName();
+        String tableName = hivePart.getTable().getTableName();
+
+        Referenceable partRef = getPartitionReference(dbName, tableName, hivePart.getValues());
+        if (partRef == null) {
+            partRef = new Referenceable(HiveDataTypes.HIVE_PARTITION.getName());
+            partRef.set("values", hivePart.getValues());
+
+            partRef.set(HiveDataModelGenerator.TABLE, tableReferenceable);
+
+            //todo fix
+            partRef.set("createTime", hivePart.getLastAccessTime());
+            partRef.set("lastAccessTime", hivePart.getLastAccessTime());
+
+            // sdStruct = fillStorageDescStruct(hivePart.getSd());
+            // Instead of creating copies of the sdstruct for partitions we are reusing existing
+            // ones will fix to identify partitions with differing schema.
+            partRef.set("sd", sdReferenceable);
+
+            partRef.set("parameters", hivePart.getParameters());
+            partRef = createInstance(partRef);
+        } else {
+            LOG.info("Partition {}.{} with values {} is already registered with id {}", dbName, tableName,
+                    StringUtils.join(hivePart.getValues(), ","), partRef.getId().id);
+        }
+        return partRef;
+    }
+
+    private void importIndexes(String db, String table, Referenceable dbReferenceable, Referenceable tableReferenceable)
+    throws Exception {
+        List<Index> indexes = hiveClient.getIndexes(db, table, Short.MAX_VALUE);
+        if (indexes.size() > 0) {
+            for (Index index : indexes) {
+                importIndex(index, dbReferenceable, tableReferenceable);
+            }
+        }
+    }
+
+    //todo should be idempotent
+    private void importIndex(Index index, Referenceable dbReferenceable, Referenceable tableReferenceable)
+            throws Exception {
+        LOG.info("Importing index {} for {}.{}", index.getIndexName(), dbReferenceable, tableReferenceable);
+        Referenceable indexRef = new Referenceable(HiveDataTypes.HIVE_INDEX.getName());
+
+        indexRef.set(HiveDataModelGenerator.NAME, index.getIndexName());
+        indexRef.set("indexHandlerClass", index.getIndexHandlerClass());
+
+        indexRef.set(HiveDataModelGenerator.DB, dbReferenceable);
+
+        indexRef.set("createTime", index.getCreateTime());
+        indexRef.set("lastAccessTime", index.getLastAccessTime());
+        indexRef.set("origTable", index.getOrigTableName());
+        indexRef.set("indexTable", index.getIndexTableName());
+
+        Referenceable sdReferenceable = fillStorageDescStruct(index.getSd(), null);
+        indexRef.set("sd", sdReferenceable);
+
+        indexRef.set("parameters", index.getParameters());
+
+        tableReferenceable.set("deferredRebuild", index.isDeferredRebuild());
+
+        createInstance(indexRef);
+    }
+
+    private Referenceable fillStorageDescStruct(StorageDescriptor storageDesc, List<Referenceable> colList)
+    throws Exception {
+        LOG.debug("Filling storage descriptor information for " + storageDesc);
+
+        Referenceable sdReferenceable = new Referenceable(HiveDataTypes.HIVE_STORAGEDESC.getName());
+
+        SerDeInfo serdeInfo = storageDesc.getSerdeInfo();
+        LOG.debug("serdeInfo = " + serdeInfo);
+        // SkewedInfo skewedInfo = storageDesc.getSkewedInfo();
+
+        String serdeInfoName = HiveDataTypes.HIVE_SERDE.getName();
+        Struct serdeInfoStruct = new Struct(serdeInfoName);
+
+        serdeInfoStruct.set(HiveDataModelGenerator.NAME, serdeInfo.getName());
+        serdeInfoStruct.set("serializationLib", serdeInfo.getSerializationLib());
+        serdeInfoStruct.set("parameters", serdeInfo.getParameters());
+
+        sdReferenceable.set("serdeInfo", serdeInfoStruct);
+        sdReferenceable.set(HiveDataModelGenerator.STORAGE_NUM_BUCKETS, storageDesc.getNumBuckets());
+        sdReferenceable
+                .set(HiveDataModelGenerator.STORAGE_IS_STORED_AS_SUB_DIRS, storageDesc.isStoredAsSubDirectories());
+
+        //Use the passed column list if not null, ex: use same references for table and SD
+        List<FieldSchema> columns = storageDesc.getCols();
+        if (columns != null && !columns.isEmpty()) {
+            if (colList != null) {
+                sdReferenceable.set("cols", colList);
+            } else {
+                sdReferenceable.set("cols", getColumns(columns));
+            }
+        }
+
+        List<Struct> sortColsStruct = new ArrayList<Struct>();
+        for (Order sortcol : storageDesc.getSortCols()) {
+            String hiveOrderName = HiveDataTypes.HIVE_ORDER.getName();
+            Struct colStruct = new Struct(hiveOrderName);
+            colStruct.set("col", sortcol.getCol());
+            colStruct.set("order", sortcol.getOrder());
+
+            sortColsStruct.add(colStruct);
+        }
+        if (sortColsStruct.size() > 0) {
+            sdReferenceable.set("sortCols", sortColsStruct);
+        }
+
+        sdReferenceable.set("location", storageDesc.getLocation());
+        sdReferenceable.set("inputFormat", storageDesc.getInputFormat());
+        sdReferenceable.set("outputFormat", storageDesc.getOutputFormat());
+        sdReferenceable.set("compressed", storageDesc.isCompressed());
+
+        if (storageDesc.getBucketCols().size() > 0) {
+            sdReferenceable.set("bucketCols", storageDesc.getBucketCols());
+        }
+
+        sdReferenceable.set("parameters", storageDesc.getParameters());
+        sdReferenceable.set("storedAsSubDirectories", storageDesc.isStoredAsSubDirectories());
+
+        return createInstance(sdReferenceable);
+    }
+
+    private List<Referenceable> getColumns(List<FieldSchema> schemaList) throws Exception {
+        List<Referenceable> colList = new ArrayList<Referenceable>();
+        for (FieldSchema fs : schemaList) {
+            LOG.debug("Processing field " + fs);
+            Referenceable colReferenceable = new Referenceable(HiveDataTypes.HIVE_COLUMN.getName());
+            colReferenceable.set(HiveDataModelGenerator.NAME, fs.getName());
+            colReferenceable.set("type", fs.getType());
+            colReferenceable.set(HiveDataModelGenerator.COMMENT, fs.getComment());
+
+            colList.add(createInstance(colReferenceable));
+        }
+        return colList;
+    }
+
+    public synchronized void registerHiveDataModel() throws Exception {
+        HiveDataModelGenerator dataModelGenerator = new HiveDataModelGenerator();
+        AtlasClient dgiClient = getAtlasClient();
+
+        //Register hive data model if its not already registered
+        if (dgiClient.getType(HiveDataTypes.HIVE_PROCESS.getName()) == null) {
+            LOG.info("Registering Hive data model");
+            dgiClient.createType(dataModelGenerator.getModelAsJson());
+        } else {
+            LOG.info("Hive data model is already registered!");
+        }
+    }
+
+    
+
+	Referenceable rawColumn(String name, String dataType, String comment, String... traitNames) throws Exception {
+	        Referenceable referenceable = new Referenceable(HiveDataTypes.HIVE_COLUMN.getName(), traitNames);
+	        System.out.println(name + " hive_column type " + dataType);
+	        referenceable.set("name", name);
+	        referenceable.set("type", dataType);
+	        referenceable.set("comment", comment);
+
+	        return referenceable;
+	    }
+	  
+	
+   
+    public void updateTable(Referenceable tableReferenceable, Table newTable) throws AtlasServiceException {
+        AtlasClient client = getAtlasClient();
+        client.updateEntity(tableReferenceable.getId()._getId(), HiveDataModelGenerator.TABLE_NAME,
+                newTable.getTableName().toLowerCase());
+        client.updateEntity(tableReferenceable.getId()._getId(), HiveDataModelGenerator.NAME,
+                getTableName(clusterName, newTable.getDbName(), newTable.getTableName()));
+    }
+}
\ No newline at end of file
diff --git a/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/JsonHierarchy.java b/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/JsonHierarchy.java
new file mode 100644
index 0000000..ab11c39
--- /dev/null
+++ b/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/JsonHierarchy.java
@@ -0,0 +1,326 @@
+/**
+ *This is a generic JSON Parser
+ * 
+ */
+package com.hortonworks.atlas.client;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.ListIterator;
+import org.codehaus.jackson.JsonFactory;
+import org.codehaus.jackson.JsonParseException;
+import org.codehaus.jackson.JsonParser;
+import org.codehaus.jackson.JsonToken;
+import com.hortonworks.atlas.adapter.EntityModel;
+import com.hortonworks.atlas.adapter.TupleModel;
+
+/** 
+ * @author sdutta
+ *
+ */
+public class JsonHierarchy {
+
+	public static String path = null;
+
+	JsonParser jParser;
+	private int i = 0;
+	JsonToken jtk = null;
+
+	ArrayList<TupleModel> tmapList = new ArrayList<TupleModel>();
+
+	ArrayList<TupleModel> rotatingList = new ArrayList<TupleModel>();
+
+	boolean hitarray = false;
+	int hitarray_index = 0;
+	private ArrayList<EntityModel> emList = new ArrayList<EntityModel>();
+	EntityModel EM = null;
+
+
+
+
+	/**
+	 * 
+	 * @param args
+	 * @throws JsonParseException
+	 * @throws IOException
+	 */
+	public static void main(String[] args) throws JsonParseException,
+			IOException {
+
+		path = "/Users/sdutta/DGI/codesamples/atlas/TestHierarchy.json";
+		JsonHierarchy jsn = new JsonHierarchy();
+		jsn.parseJSON(path);
+
+	}
+
+	
+	
+	/**
+	 * 
+	 * @param filepath
+	 * @throws JsonParseException
+	 * @throws IOException
+	 */
+	public void parseJSON(String filepath) throws JsonParseException,
+			IOException {
+
+		JsonFactory jfac = new JsonFactory();
+		jParser = jfac.createJsonParser(new File(filepath));
+
+		buildTrees(jParser, null, null);
+
+		ListIterator<TupleModel> tmI = tmapList.listIterator();
+		TupleModel tm = null;
+		String header = "Trait Name" + "   --   " + "Parent Trait"
+				+ "    --   " + "Level";
+		
+		for(int i = 0; i < header.length(); i++){
+			System.out.print("#");
+		}
+		System.out.println();
+		
+		System.out.println(header);
+
+		for(int i = 0; i < header.length(); i++){
+			System.out.print("#");
+		}
+		System.out.println();
+		
+		while (tmI.hasNext()) {
+
+			tm = tmI.next();
+			System.out.println(tm.getCurrnode() + "   --   " + tm.getParentnode()
+					+ "   --   " + tm.getLevel());
+		}
+
+		System.out.println("Printing entities");
+		ListIterator<EntityModel> emItr = this.emList.listIterator();
+		EntityModel em1 = null;
+
+		String header2 = "Entity Type" + "   --   " + "Entity Name"
+				+ "    --   " + " Trait Name";
+		
+		
+		for(int i = 0; i < header2.length(); i++){
+			System.out.print("#");
+		}
+		System.out.println();
+		
+		System.out.println(header2);
+
+		for(int i = 0; i < header2.length(); i++){
+			System.out.print("#");
+		}
+		System.out.println();
+		
+		
+		
+		while (emItr.hasNext()) {
+
+			em1 = emItr.next();
+
+			System.out.println(em1.getType() + "   --   " + em1.getName() + "   --   "
+					+ em1.getParent());
+		}
+
+	}
+
+	/**
+	 * 
+	 * @return
+	 */
+	public ArrayList<EntityModel> getEmList() {
+		return emList;
+	}
+
+	/**
+	 * 
+	 * @return
+	 */
+	public ArrayList<TupleModel> getTmapList() {
+		return tmapList;
+	}
+
+	/**
+	 * 
+	 * use push and pop method to get to the parent
+	 * 
+	 * @param node
+	 * @param pnode
+	 * @throws IOException
+	 * @throws JsonParseException
+	 */
+	public void buildTrees(JsonParser jParser, String currnode,
+			String parentnode) throws JsonParseException, IOException {
+
+		jtk = jParser.nextToken();
+
+		if (jParser.hasCurrentToken()) {
+
+			if (jtk == JsonToken.FIELD_NAME && !hitarray) {
+
+				currnode = jParser.getCurrentName();
+
+				currnode = jParser.getCurrentName();
+
+				TupleModel tm = new TupleModel(currnode, parentnode);
+				
+				tm.setLevel(i);
+				tmapList.add(tm);
+				this.pushTuple(tm);
+				jtk = jParser.nextToken();
+
+			}
+
+			if (hitarray && jParser.getText() == "{") {
+				EM = new EntityModel();
+				EM.setParent(parentnode);
+			}
+
+			if (hitarray && jParser.getText() == "type") {
+				currnode = "type";
+				jtk = jParser.nextToken();
+				if (jtk.isScalarValue()) {
+					EM.setType(jParser.getText());
+				}
+
+			}
+
+			if (hitarray && jParser.getText() == "name") {
+				currnode = "name";
+				jtk = jParser.nextToken();
+				if (jtk.isScalarValue()) {
+					EM.setName(jParser.getText());
+				}
+
+			}
+
+			if (hitarray && jParser.getText() == "}") {
+				this.emList.add(EM);
+				EM = null;
+			}
+
+			if (jParser.getText() == "[") {
+
+				// parentnode = currnode;
+				hitarray = true;
+				hitarray_index++;
+
+			} else if (jParser.getText() == "]") {
+
+				hitarray_index--;
+				if (hitarray_index == 0)
+					hitarray = false;
+
+			}
+
+			if (jParser.getText() == "{" && !hitarray) {
+
+				i++;
+
+				parentnode = currnode;
+
+			} else if (jParser.getText() == "}" && !hitarray) {
+
+				this.popTupleModel();
+
+				TupleModel tm = this.rotatingList
+						.get(this.rotatingList.size() - 1);
+				parentnode = tm.getParentnode();
+
+			}
+
+			buildTrees(jParser, currnode, parentnode);
+
+		}
+
+	}
+
+	/**
+	 * This is to handle the Entities
+	 * 
+	 * @param jParser
+	 * @param parentnode
+	 * @throws JsonParseException
+	 * @throws IOException
+	 */
+	public void processEntities(JsonParser jParser, String parentnode,
+			EntityModel em) throws JsonParseException, IOException {
+
+		jtk = jParser.getCurrentToken();
+		if (jtk == JsonToken.FIELD_NAME) {
+
+			String currnode = jParser.getCurrentName();
+
+			em.setParent(parentnode);
+
+			jParser.nextToken();
+
+			if (jtk.isScalarValue()) {
+
+				if ("type".equalsIgnoreCase(currnode))
+					em.setType(currnode);
+				else if ("name".equalsIgnoreCase(currnode))
+					em.setName(currnode);
+
+			}
+		}
+
+		if (jParser.getText() == "[") {
+
+			jParser.nextToken();
+			processEntities(jParser, parentnode, em);
+
+		}
+
+		if (jParser.getText() == "{") {
+
+			jParser.nextToken();
+			EM = new EntityModel();
+			processEntities(jParser, parentnode, EM);
+
+		}
+
+		if (jParser.getText() == "}") {
+
+			jParser.nextToken();
+			emList.add(em);
+			processEntities(jParser, parentnode, EM);
+
+		}
+
+		if (jParser.getText() == "[") {
+
+			jParser.nextToken();
+			processEntities(jParser, parentnode, em);
+
+		}
+
+		if (jParser.getText() == "]") {
+
+		}
+
+	}
+
+	/**
+	 * 
+	 * push the last entry in
+	 * 
+	 * @param tupMd
+	 */
+	private void pushTuple(TupleModel tupMd) {
+
+		this.rotatingList.add(tupMd);
+	}
+
+	/**
+	 * pop the last entry out
+	 * 
+	 */
+	private void popTupleModel() {
+
+		this.rotatingList.remove(this.rotatingList.size() - 1);
+	}
+
+}
diff --git a/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/NewAtlasClient.java b/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/NewAtlasClient.java
new file mode 100644
index 0000000..cb3c09f
--- /dev/null
+++ b/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/NewAtlasClient.java
@@ -0,0 +1,201 @@
+package com.hortonworks.atlas.client;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import javax.ws.rs.HttpMethod;
+import javax.ws.rs.core.MediaType;
+import javax.ws.rs.core.Response;
+import javax.ws.rs.core.UriBuilder;
+
+import org.apache.atlas.AtlasClient;
+import org.apache.atlas.AtlasException;
+import org.apache.atlas.AtlasServiceException;
+import org.apache.atlas.security.SecureClientUtils;
+import org.apache.atlas.typesystem.types.DataTypes;
+import org.apache.commons.configuration.PropertiesConfiguration;
+import org.apache.hadoop.conf.Configuration;
+import org.codehaus.jettison.json.JSONArray;
+import org.codehaus.jettison.json.JSONException;
+import org.codehaus.jettison.json.JSONObject;
+
+import com.sun.jersey.api.client.Client;
+import com.sun.jersey.api.client.ClientResponse;
+import com.sun.jersey.api.client.WebResource;
+import com.sun.jersey.api.client.config.DefaultClientConfig;
+import com.sun.jersey.client.urlconnection.URLConnectionClientHandler;
+
+import static org.apache.atlas.security.SecurityProperties.TLS_ENABLED;
+public class NewAtlasClient extends AtlasClient {
+	{
+		System.setProperty("atlas.conf", "conf");
+	}
+	 private WebResource service;
+	
+	public NewAtlasClient(String baseurl) {
+		super(baseurl);
+		DefaultClientConfig config = new DefaultClientConfig();
+        PropertiesConfiguration clientConfig = null;
+        try {
+            clientConfig = getClientProperties();
+            if (clientConfig.getBoolean(TLS_ENABLED, false)) {
+                // create an SSL properties configuration if one doesn't exist.  SSLFactory expects a file, so forced
+                // to create a
+                // configuration object, persist it, then subsequently pass in an empty configuration to SSLFactory
+                SecureClientUtils.persistSSLClientConfiguration(clientConfig);
+            }
+        } catch (Exception e) {
+           e.printStackTrace();
+        }
+
+        URLConnectionClientHandler handler = SecureClientUtils.getClientConnectionHandler(config, clientConfig);
+
+        Client client = new Client(handler, config);
+       // System.out.println(baseurl);
+        client.resource(UriBuilder.fromUri(baseurl).build());
+
+        service = client.resource(UriBuilder.fromUri(baseurl).build());
+
+	}
+	
+	
+
+	
+	/**
+     * Return all trait names for the given entity id
+     * @param guid
+     * @return
+	 * @throws Exception 
+     */
+    public List<String> getTraitNames(String guid) throws Exception {
+        WebResource resource = getResource(API.LIST_TRAITS, guid, URI_TRAITS);
+        JSONObject response = callAPIWithResource(API.LIST_TRAITS, resource);
+        return extractResults(response);
+    }
+
+    private WebResource getResource(API api, String... pathParams) {
+    	
+    	//System.out.println(api.getPath());
+        WebResource resource = service.path(api.getPath());
+        //System.out.println( resource.getURI().toString());
+       
+        if (pathParams != null) {
+            for (String pathParam : pathParams) {
+                resource = resource.path(pathParam);
+            }
+        }
+        return resource;
+    }
+    
+    private List<String> extractResults(JSONObject response) throws AtlasServiceException {
+        try {
+            JSONArray results = response.getJSONArray(AtlasClient.RESULTS);
+            List<String> list = new ArrayList<String>();
+            for (int index = 0; index < results.length(); index++) {
+                list.add(results.getString(index));
+            }
+            return list;
+        } catch (JSONException e) {
+          throw new AtlasServiceException(e);
+        }
+    }
+    
+    enum API {
+
+        //Type operations
+        CREATE_TYPE(BASE_URI + TYPES, HttpMethod.POST),
+        GET_TYPE(BASE_URI + TYPES, HttpMethod.GET),
+        LIST_TYPES(BASE_URI + TYPES, HttpMethod.GET),
+        LIST_TRAIT_TYPES(BASE_URI + TYPES + "?type=TRAIT", HttpMethod.GET),
+
+        //Entity operations
+        CREATE_ENTITY(BASE_URI + URI_ENTITIES, HttpMethod.POST),
+        GET_ENTITY(BASE_URI + URI_ENTITIES, HttpMethod.GET),
+        UPDATE_ENTITY(BASE_URI + URI_ENTITIES, HttpMethod.PUT),
+        LIST_ENTITY(BASE_URI + URI_ENTITIES, HttpMethod.GET),
+
+        //Trait operations
+        ADD_TRAITS(BASE_URI + URI_ENTITIES, HttpMethod.POST),
+        DELETE_TRAITS(BASE_URI + URI_ENTITIES, HttpMethod.DELETE),
+        LIST_TRAITS(BASE_URI + URI_ENTITIES, HttpMethod.GET),
+
+        //Search operations
+        SEARCH(BASE_URI + URI_SEARCH, HttpMethod.GET),
+        SEARCH_DSL(BASE_URI + URI_SEARCH + "/dsl", HttpMethod.GET),
+        SEARCH_GREMLIN(BASE_URI + URI_SEARCH + "/gremlin", HttpMethod.GET),
+        SEARCH_FULL_TEXT(BASE_URI + URI_SEARCH + "/fulltext", HttpMethod.GET),
+
+        //Lineage operations
+        LINEAGE_INPUTS_GRAPH(BASE_URI + URI_LINEAGE, HttpMethod.GET),
+        LINEAGE_OUTPUTS_GRAPH(BASE_URI + URI_LINEAGE, HttpMethod.GET),
+        LINEAGE_SCHEMA(BASE_URI + URI_LINEAGE, HttpMethod.GET);
+
+        private final String method;
+        private final String path;
+
+        API(String path, String method) {
+            this.path = path;
+            this.method = method;
+        }
+
+        public String getMethod() {
+            return method;
+        }
+
+        public String getPath() {
+            return path;
+        }
+    }
+    
+    private JSONObject callAPIWithResource(API api, WebResource resource) throws Exception {
+        return callAPIWithResource(api, resource, null);
+    }
+
+    private JSONObject callAPIWithResource(API api, WebResource resource, Object requestObject)
+    throws Exception {
+    	
+        ClientResponse clientResponse = resource.accept(JSON_MEDIA_TYPE).type(JSON_MEDIA_TYPE)
+                .method(api.getMethod(), ClientResponse.class, requestObject);
+
+        Response.Status expectedStatus =
+                HttpMethod.POST.equals(api.getMethod()) ? Response.Status.CREATED : Response.Status.OK;
+        
+        if (clientResponse.getStatus() == expectedStatus.getStatusCode()) {
+            String responseAsString = clientResponse.getEntity(String.class);
+            
+            try {
+                return new JSONObject(responseAsString);
+            } catch (JSONException e) {
+                throw new AtlasServiceException(e);
+            }
+        }
+
+        throw new Exception("Metadata service API " + api + " failed");
+    }
+    
+    /**
+     * Adds trait to the give entity
+     * @param guid
+     * @param traitDefinition
+     * @throws Exception 
+     */
+    public void addTrait(String guid, String traitDefinition) throws Exception {
+        callAPI(API.ADD_TRAITS, traitDefinition, guid, URI_TRAITS);
+    }
+    
+    private JSONObject callAPI(API api, Object requestObject, String... pathParams) throws Exception {
+        WebResource resource = getResource(api, pathParams);
+        return callAPIWithResource(api, resource, requestObject);
+    }
+    
+    
+
+    public List<String> listTypes(DataTypes.TypeCategory category) throws Exception {
+        final JSONObject jsonObject = callAPI(API.LIST_TRAIT_TYPES, null);
+        return extractResults(jsonObject);
+    }
+   
+
+ 
+    
+}
diff --git a/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/Taxonomy.java b/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/Taxonomy.java
new file mode 100644
index 0000000..b51ae3f
--- /dev/null
+++ b/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/Taxonomy.java
@@ -0,0 +1,192 @@
+package com.hortonworks.atlas.client;
+
+import java.util.ArrayList;
+
+import org.apache.atlas.AtlasClient;
+import org.apache.atlas.AtlasServiceException;
+import org.apache.atlas.typesystem.Referenceable;
+import org.apache.atlas.typesystem.json.TypesSerialization;
+import org.apache.atlas.typesystem.types.AttributeDefinition;
+import org.apache.atlas.typesystem.types.ClassType;
+import org.apache.atlas.typesystem.types.DataTypes;
+import org.apache.atlas.typesystem.types.EnumTypeDefinition;
+import org.apache.atlas.typesystem.types.HierarchicalTypeDefinition;
+import org.apache.atlas.typesystem.types.IDataType;
+import org.apache.atlas.typesystem.types.Multiplicity;
+import org.apache.atlas.typesystem.types.StructTypeDefinition;
+import org.apache.atlas.typesystem.types.TypeUtils;
+import org.apache.atlas.typesystem.types.utils.TypesUtil;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.ImmutableList;
+
+public class Taxonomy {
+	{
+		System.setProperty("atlas.conf", "conf");
+	}
+	
+	@SuppressWarnings("unused")
+	private  AtlasClient ac = null;
+	
+	
+	
+	/**
+	 * This creates an instance of the taxonomy class
+	 * It will create a taxonomy using the traits
+	 * Constructor
+	 * @param baseurl
+	 */
+	
+	public Taxonomy(String baseurl, String traitname, String Supertrait) {
+		
+		ac = new AtlasClient(baseurl);
+		try {
+			ac.createType(this.createTraitTypes(traitname, Supertrait));
+		
+		
+		} catch (AtlasServiceException e) {
+			
+			e.printStackTrace();
+		}
+	
+	
+	}
+	
+	public Taxonomy() {
+	
+	
+	}
+
+	
+	
+	
+	/**
+	 * This is a generic method to create types
+	 * 
+	 */
+	public String createTraitTypes(String traitname, String supertrait){
+		
+		
+		//System.out.println("Supertrait: " + supertrait);
+				
+				if (supertrait == null ){
+					return  TypesSerialization.toJson(TypeUtils.getTypesDef(
+				ImmutableList.<EnumTypeDefinition> of(),
+				ImmutableList.<StructTypeDefinition> of(),
+		ImmutableList.of(TypesUtil.createTraitTypeDef(traitname, null )),
+		ImmutableList.<HierarchicalTypeDefinition<ClassType>>of()));
+				}else
+				{
+					return  TypesSerialization.toJson(TypeUtils.getTypesDef(
+							ImmutableList.<EnumTypeDefinition> of(),
+							ImmutableList.<StructTypeDefinition> of(),
+					ImmutableList.of(TypesUtil.createTraitTypeDef(traitname, ImmutableList.of(supertrait) )),
+					ImmutableList.<HierarchicalTypeDefinition<ClassType>>of()));
+					
+				}
+		
+		
+	}
+	
+
+	
+	
+	
+	/**
+	 * 
+	 * @param name
+	 * @param dT
+	 * @param m
+	 * @param isComposite
+	 * @param reverseAttributeName
+	 * @return
+	 */
+	@SuppressWarnings("rawtypes")
+	AttributeDefinition attrDef(String name, IDataType dT, Multiplicity m,
+			boolean isComposite, String reverseAttributeName) {
+		Preconditions.checkNotNull(name);
+		Preconditions.checkNotNull(dT);
+		return new AttributeDefinition(name, dT.getName(), m, isComposite,
+				reverseAttributeName);
+	}
+
+	/**
+	 * 
+	 * @param name
+	 * @param dT
+	 * @return
+	 */
+	@SuppressWarnings("rawtypes")
+	AttributeDefinition attrDef(String name, IDataType dT) {
+		return attrDef(name, dT, Multiplicity.OPTIONAL, false, null);
+	}
+
+	@SuppressWarnings("rawtypes")
+	AttributeDefinition attrDef(String name, IDataType dT, Multiplicity m) {
+		return attrDef(name, dT, m, false, null);
+	}
+	
+
+	/**
+	 * 
+	 * @param args
+	 * @throws Exception
+	 */
+	public static void main(String[] args) throws Exception {
+		
+		if(args.length < 2)
+			throw new Exception("Please pass the atlas base url and the typename");
+		
+		String traitname  = null;
+		String baseurl = args[0];
+		
+		String[] arr = null;
+		
+		
+		if(args[1] != null)
+		 traitname = args[1];
+		else
+			throw new Exception("Please pass the traitname");
+		
+		//ArrayList alist = null;
+		
+		Taxonomy tx = null;
+		if(args.length > 2 )
+		{
+			
+			//alist = new ArrayList<String>();
+					
+			String supertrait = args[2];
+			
+			tx = new Taxonomy(baseurl, traitname, supertrait);
+			
+			
+		}else
+			 tx = new Taxonomy(baseurl, traitname, null);
+		
+		
+		
+		
+		
+		System.out.println("Done creating trait " + traitname);
+		
+		
+		
+		/*AtlasEntityCreator aec = new AtlasEntityCreator(baseurl);
+		
+		    Referenceable referenceable = new Referenceable("DB", "SuperGreen3");
+	        referenceable.set("name",  "SuperGreen3Entity");
+	        referenceable.set("description", "this is to test trait inheritence");
+	        referenceable.set("owner", "Andrew");
+	        referenceable.set("locationUri", "hdfs://localhost:8020");
+	        referenceable.set("createTime", System.currentTimeMillis());
+	        
+	        aec.createEntity(referenceable);*/
+		
+		//aec.createEntity(aec.createRefObject("GOD_Type",, ));
+		//aec.createEntity(aec.createRefObject("GOD_Type", "GreenEntity", "this is to test trait inheritence"));	
+	}
+	
+	
+
+}
diff --git a/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/TypeInheritence.java b/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/TypeInheritence.java
new file mode 100644
index 0000000..3ca3ad7
--- /dev/null
+++ b/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/TypeInheritence.java
@@ -0,0 +1,120 @@
+package com.hortonworks.atlas.client;
+
+import org.apache.atlas.AtlasClient;
+import org.apache.atlas.AtlasServiceException;
+import org.apache.atlas.typesystem.Referenceable;
+import org.apache.atlas.typesystem.json.TypesSerialization;
+import org.apache.atlas.typesystem.types.AttributeDefinition;
+import org.apache.atlas.typesystem.types.ClassType;
+import org.apache.atlas.typesystem.types.DataTypes;
+import org.apache.atlas.typesystem.types.EnumTypeDefinition;
+import org.apache.atlas.typesystem.types.HierarchicalTypeDefinition;
+import org.apache.atlas.typesystem.types.IDataType;
+import org.apache.atlas.typesystem.types.Multiplicity;
+import org.apache.atlas.typesystem.types.StructTypeDefinition;
+import org.apache.atlas.typesystem.types.TypeUtils;
+import org.apache.atlas.typesystem.types.utils.TypesUtil;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.ImmutableList;
+
+public class TypeInheritence {
+	{
+		System.setProperty("atlas.conf", "conf");
+	}
+	
+	@SuppressWarnings("unused")
+	private  AtlasClient ac = null;
+	
+	
+	public TypeInheritence(String baseurl) {
+		
+		ac = new AtlasClient(baseurl);
+		try {
+			ac.createType(this.createTraitTypes());
+		} catch (AtlasServiceException e) {
+			// TODO Auto-generated catch block
+			e.printStackTrace();
+		}
+	}
+
+
+	/**
+	 * 
+	 * @param args
+	 * @throws Exception
+	 */
+	public static void main(String[] args) throws Exception {
+		if(args.length < 0)
+			throw new Exception("Please pass the atlas base url");
+		String baseurl = args[0];
+		
+		System.out.println(" Baseurl" + baseurl);
+		//TypeInheritence tIh = new TypeInheritence(baseurl);
+		//tIh.createTraitTypes();
+		
+		/*AtlasEntityCreator aec = new AtlasEntityCreator(baseurl);
+		
+		    Referenceable referenceable = new Referenceable("DB", "SuperGreen3");
+	        referenceable.set("name",  "SuperGreen3Entity");
+	        referenceable.set("description", "this is to test trait inheritence");
+	        referenceable.set("owner", "Andrew");
+	        referenceable.set("locationUri", "hdfs://localhost:8020");
+	        referenceable.set("createTime", System.currentTimeMillis());
+	        
+	        aec.createEntity(referenceable);*/
+		
+		//aec.createEntity(aec.createRefObject("GOD_Type",, ));
+		//aec.createEntity(aec.createRefObject("GOD_Type", "GreenEntity", "this is to test trait inheritence"));	
+	}
+	
+	/*
+	 * 
+	 */
+	public String createTraitTypes(){
+		
+		return TypesSerialization.toJson(TypeUtils.getTypesDef(
+				ImmutableList.<EnumTypeDefinition> of(),
+				ImmutableList.<StructTypeDefinition> of(),
+		ImmutableList.of(TypesUtil.createTraitTypeDef("PII", ImmutableList.of("Blue","White") )),
+		ImmutableList.<HierarchicalTypeDefinition<ClassType>>of()));
+		
+	}
+	
+	/**
+	 * 
+	 * @param name
+	 * @param dT
+	 * @param m
+	 * @param isComposite
+	 * @param reverseAttributeName
+	 * @return
+	 */
+	@SuppressWarnings("rawtypes")
+	AttributeDefinition attrDef(String name, IDataType dT, Multiplicity m,
+			boolean isComposite, String reverseAttributeName) {
+		Preconditions.checkNotNull(name);
+		Preconditions.checkNotNull(dT);
+		return new AttributeDefinition(name, dT.getName(), m, isComposite,
+				reverseAttributeName);
+	}
+
+	/**
+	 * 
+	 * @param name
+	 * @param dT
+	 * @return
+	 */
+	@SuppressWarnings("rawtypes")
+	AttributeDefinition attrDef(String name, IDataType dT) {
+		return attrDef(name, dT, Multiplicity.OPTIONAL, false, null);
+	}
+
+	@SuppressWarnings("rawtypes")
+	AttributeDefinition attrDef(String name, IDataType dT, Multiplicity m) {
+		return attrDef(name, dT, m, false, null);
+	}
+	
+	
+
+}
diff --git a/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/XMLHierarchy.java b/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/XMLHierarchy.java
new file mode 100644
index 0000000..c7f7a30
--- /dev/null
+++ b/codesamples/atlas/src/main/java/com/hortonworks/atlas/client/XMLHierarchy.java
@@ -0,0 +1,22 @@
+/**
+ * 
+ */
+package com.hortonworks.atlas.client;
+
+/**
+ * @author sdutta
+ * @param <T>
+ *
+ */
+public class XMLHierarchy<T> implements Hierarchy<T> {
+
+	
+//	public void 
+	
+	
+	public void parse() throws Exception{
+		// TODO Auto-generated method stub
+		
+	}
+
+}
diff --git a/codesamples/atlas/src/main/java/com/hortonworks/atlas/trash/DemoClass.java b/codesamples/atlas/src/main/java/com/hortonworks/atlas/trash/DemoClass.java
new file mode 100644
index 0000000..b047564
--- /dev/null
+++ b/codesamples/atlas/src/main/java/com/hortonworks/atlas/trash/DemoClass.java
@@ -0,0 +1,333 @@
+package com.hortonworks.atlas.trash;
+import com.google.common.base.Preconditions;
+import com.google.common.collect.ImmutableList;
+import com.sun.jersey.api.client.Client;
+import com.sun.jersey.api.client.ClientResponse;
+import com.sun.jersey.api.client.WebResource;
+import com.sun.jersey.api.client.config.DefaultClientConfig;
+import com.sun.jersey.client.urlconnection.URLConnectionClientHandler;
+import org.apache.atlas.AtlasClient;
+
+import org.apache.atlas.security.SecureClientUtils;
+import org.apache.atlas.typesystem.Referenceable;
+import org.apache.atlas.typesystem.json.InstanceSerialization;
+import org.apache.atlas.typesystem.json.TypesSerialization;
+import org.apache.atlas.typesystem.persistence.Id;
+import org.apache.atlas.typesystem.types.*;
+import org.apache.atlas.typesystem.types.utils.TypesUtil;
+import org.apache.atlas.typesystem.TypesDef;
+import org.apache.commons.configuration.PropertiesConfiguration;
+import org.codehaus.jettison.json.JSONArray;
+import org.codehaus.jettison.json.JSONException;
+import org.codehaus.jettison.json.JSONObject;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import javax.ws.rs.HttpMethod;
+import javax.ws.rs.core.MediaType;
+import javax.ws.rs.core.Response;
+import javax.ws.rs.core.UriBuilder;
+
+import java.util.ArrayList;
+import java.util.List;
+
+
+
+/**
+ * This is for loading data
+ * @author sdutta
+ *
+ */
+public class DemoClass {
+
+	static Logger logger = LoggerFactory.getLogger(DemoClass.class);
+	
+	 	private static final String TRANSPORT_TYPE = "AeroPlane";
+	    private static final String CARRIER_TYPE = "Manufacturing";
+	    private static final String ROUTE_TYPE = "Air";
+	    private static final String MOTION_TYPE = "Speed";
+	    
+	//    private static final String LOAD_PROCESS_TYPE = "LoadProcess";
+	//    private static final String STORAGE_DESC_TYPE = "StorageDesc";
+	
+	public static void main(String[] args) throws Exception{
+		// TODO Auto-generated method stub
+		
+		if(args.length < 1)
+		{	
+			throw new Exception("Please provide the DGI host url");
+		}
+		
+		System.setProperty("atlas.conf", "/Users/sdutta/Applications/conf");
+		
+		String baseUrl = getServerUrl(args);
+		
+		DemoClass dc = new DemoClass(baseUrl);
+		 dc.createTypes();
+		 
+		 // Shows how to create types in Atlas for your meta model
+	        dc.createTypes();
+
+	        // Shows how to create entities (instances) for the added types in Atlas
+	        dc.createEntities();
+
+	        // Shows some search queries using DSL based on types
+	        //dc.search();
+		
+	}
+	
+	private static final String[] TYPES =
+        {TRANSPORT_TYPE, CARRIER_TYPE, ROUTE_TYPE, MOTION_TYPE,};
+	
+	private final AtlasClient metadataServiceClient;
+	
+	public DemoClass(String baseurl){
+		
+		this.metadataServiceClient = new AtlasClient(baseurl);
+	}
+	
+
+    void createTypes() throws Exception {
+        TypesDef typesDef = createTypeDefinitions();
+
+        String typesAsJSON = TypesSerialization.toJson(typesDef);
+        
+        System.out.println("typesAsJSON = " + typesAsJSON);
+        
+       metadataServiceClient.createType(typesAsJSON);
+
+        // verify types created
+        verifyTypesCreated();
+    }
+    
+    
+    /*
+     * This API will list the types on the system
+     */
+    private void verifyTypesCreated() throws Exception {
+        List<String> types = metadataServiceClient.listTypes();
+        for (String type : TYPES) {
+        	System.out.println(type);
+            assert types.contains(type);
+        }
+    }
+    
+    
+    
+    TypesDef createTypeDefinitions() throws Exception {
+    	
+    	
+        HierarchicalTypeDefinition<ClassType> transportClsDef = TypesUtil
+                .createClassTypeDef(this.TRANSPORT_TYPE, null, attrDef("name", DataTypes.STRING_TYPE),
+                        attrDef("description", DataTypes.STRING_TYPE), attrDef("locationUri", DataTypes.STRING_TYPE),
+                        attrDef("owner", DataTypes.STRING_TYPE), attrDef("createTime", DataTypes.INT_TYPE));
+
+        HierarchicalTypeDefinition<ClassType> carrierClsDef = TypesUtil
+                .createClassTypeDef(this.CARRIER_TYPE, null, 
+                		attrDef("name", DataTypes.STRING_TYPE),
+                        attrDef("location", DataTypes.STRING_TYPE), 
+                        attrDef("country", DataTypes.STRING_TYPE),
+                        attrDef("CEO", DataTypes.STRING_TYPE)
+                       );
+
+        HierarchicalTypeDefinition<ClassType> routeClsDef = TypesUtil
+                .createClassTypeDef(this.ROUTE_TYPE, null, 
+                		attrDef("name", DataTypes.STRING_TYPE),
+                        attrDef("route_id", DataTypes.STRING_TYPE), 
+                        attrDef("comment", DataTypes.STRING_TYPE));
+
+        HierarchicalTypeDefinition<ClassType> motionClsDef = TypesUtil
+                .createClassTypeDef(this.MOTION_TYPE, null, 
+                		attrDef("rating", DataTypes.STRING_TYPE),
+                        attrDef("metrics", DataTypes.STRING_TYPE), 
+                        attrDef("comment", DataTypes.STRING_TYPE));
+
+       
+        HierarchicalTypeDefinition<TraitType> dimTraitDef = TypesUtil.createTraitTypeDef("Dimension", null);
+
+        HierarchicalTypeDefinition<TraitType> factTraitDef = TypesUtil.createTraitTypeDef("Fact", null);
+
+        HierarchicalTypeDefinition<TraitType> piiTraitDef = TypesUtil.createTraitTypeDef("PII", null);
+
+        HierarchicalTypeDefinition<TraitType> metricTraitDef = TypesUtil.createTraitTypeDef("Metric", null);
+
+        HierarchicalTypeDefinition<TraitType> etlTraitDef = TypesUtil.createTraitTypeDef("ETL", null);
+
+        /**
+         * List of 
+         */
+        return TypeUtils.getTypesDef(ImmutableList.<EnumTypeDefinition>of(), 
+        		ImmutableList.<StructTypeDefinition>of(),
+        
+        		ImmutableList.<HierarchicalTypeDefinition<TraitType>>of(),
+        		//ImmutableList.of(dimTraitDef, factTraitDef, piiTraitDef, metricTraitDef, etlTraitDef),
+                
+                ImmutableList.of(transportClsDef, carrierClsDef, routeClsDef, motionClsDef));
+    }
+
+    AttributeDefinition attrDef(String name, IDataType dT) {
+        return attrDef(name, dT, Multiplicity.OPTIONAL, false, null);
+    }
+    
+    AttributeDefinition attrDef(String name, IDataType dT, Multiplicity m) {
+        return attrDef(name, dT, m, false, null);
+    }
+
+    AttributeDefinition attrDef(String name, IDataType dT, Multiplicity m, boolean isComposite,
+            String reverseAttributeName) {
+        Preconditions.checkNotNull(name);
+        Preconditions.checkNotNull(dT);
+        return new AttributeDefinition(name, dT.getName(), m, isComposite, reverseAttributeName);
+    }
+    
+    
+    /**
+     * This creates a new Client
+     * @param referenceable
+     * @return
+     * @throws Exception
+     */
+    private Id createInstance(Referenceable referenceable) throws Exception {
+     
+    	
+    	String typeName = referenceable.getTypeName();
+
+        String entityJSON = InstanceSerialization.toJson(referenceable, true);
+        
+        System.out.println("Submitting new entity= " + entityJSON);
+        
+        JSONObject jsonObject = metadataServiceClient.createEntity(entityJSON);
+        String guid = jsonObject.getString(AtlasClient.GUID);
+        
+        System.out.println("created instance for type " + typeName + ", guid: " + guid);
+
+        // return the Id for created instance with guid
+        return new Id(guid, referenceable.getId().getVersion(), referenceable.getTypeName());
+    }
+    
+    
+    /**
+     * Create Entities for the type definitions.
+     * Types can be class, struct or a Java cas
+     * @throws Exception
+     */
+    
+    void createEntities() throws Exception {
+
+    	Id boeingDB = transport("Boeing 747", "Best Plane in the United States", "James McNeary", "http://wwww.boeing.com");
+    	 
+        Referenceable carrier =
+                carrier("United Airlines", "San Francisco", "USA",
+                        "James McNeary");
+       
+
+       Id carrierId = this.createInstance(carrier);
+    }
+    
+    Id transport(String name, String description, String owner, String locationUri, String... traitNames)
+    	    throws Exception {
+    	        Referenceable referenceable = new Referenceable(this.TRANSPORT_TYPE, traitNames);
+    	        referenceable.set("name", name);
+    	        referenceable.set("description", description);
+    	        referenceable.set("owner", owner);
+    	        referenceable.set("locationUri", locationUri);
+    	        referenceable.set("createTime", System.currentTimeMillis());
+
+    	        return createInstance(referenceable);
+    	    }
+    
+    
+  
+
+    	    Referenceable carrier(String name, String location, String country, String CEO)
+    	    throws Exception {
+    	        Referenceable referenceable = new Referenceable(this.CARRIER_TYPE);
+    	        referenceable.set("name", name);
+    	        referenceable.set("location", location);
+    	        referenceable.set("country", country);
+    	        referenceable.set("CEO", CEO);
+
+    	        return referenceable;
+    	    }
+
+    	    Referenceable route(String name, String route_id, String comment ) throws Exception {
+    	        Referenceable referenceable = new Referenceable(this.ROUTE_TYPE);
+    	        referenceable.set("name", name);
+    	        referenceable.set("dataType", route_id);
+    	        referenceable.set("comment", comment);
+
+    	        return referenceable;
+    	    }
+    	    
+    	    
+    	    Referenceable motion(String rating, String metrics, String comment ) throws Exception {
+    	        Referenceable referenceable = new Referenceable(this.MOTION_TYPE);
+    	        referenceable.set("rating", rating);
+    	        referenceable.set("metrics", metrics);
+    	        referenceable.set("comment", comment);
+
+    	        return referenceable;
+    	    }
+    	    
+    	    
+
+    	  /*  Id table(String name, String description, Id dbId, Referenceable sd, String owner, String tableType,
+    	            List<Referenceable> columns, String... traitNames) throws Exception {
+    	    	
+    	        Referenceable referenceable = new Referenceable(TABLE_TYPE, traitNames);
+    	        referenceable.set("name", name);
+    	        referenceable.set("description", description);
+    	        referenceable.set("owner", owner);
+    	        referenceable.set("tableType", tableType);
+    	        referenceable.set("createTime", System.currentTimeMillis());
+    	        referenceable.set("lastAccessTime", System.currentTimeMillis());
+    	        referenceable.set("retention", System.currentTimeMillis());
+    	        referenceable.set("db", dbId);
+    	        referenceable.set("sd", sd);
+    	        referenceable.set("columns", columns);
+
+    	        return createInstance(referenceable);
+    	    }
+
+    	    Id loadProcess(String name, String description, String user, List<Id> inputTables, List<Id> outputTables,
+    	            String queryText, String queryPlan, String queryId, String queryGraph, String... traitNames)
+    	    throws Exception {
+    	        Referenceable referenceable = new Referenceable(LOAD_PROCESS_TYPE, traitNames);
+    	        // super type attributes
+    	        referenceable.set("name", name);
+    	        referenceable.set("description", description);
+    	        referenceable.set("inputs", inputTables);
+    	        referenceable.set("outputs", outputTables);
+
+    	        referenceable.set("user", user);
+    	        referenceable.set("startTime", System.currentTimeMillis());
+    	        referenceable.set("endTime", System.currentTimeMillis() + 10000);
+
+    	        referenceable.set("queryText", queryText);
+    	        referenceable.set("queryPlan", queryPlan);
+    	        referenceable.set("queryId", queryId);
+    	        referenceable.set("queryGraph", queryGraph);
+
+    	        return createInstance(referenceable);
+    	    }
+    */
+    
+	
+	/*
+	 * This function gets the data
+	 */
+	 static String getServerUrl(String[] args) {
+	        String baseUrl = "http://http://atlas-partner-demo01.cloud.hortonworks.com:21000";
+	        if (args.length > 0) {
+	            baseUrl = args[0];
+	        }
+
+	        System.out.println(baseUrl);
+	        return baseUrl;
+	    }
+	 
+	 
+	 
+	 
+
+}
diff --git a/codesamples/atlas/src/main/java/com/hortonworks/atlas/trash/MySqlIngester.java b/codesamples/atlas/src/main/java/com/hortonworks/atlas/trash/MySqlIngester.java
new file mode 100644
index 0000000..f91eee7
--- /dev/null
+++ b/codesamples/atlas/src/main/java/com/hortonworks/atlas/trash/MySqlIngester.java
@@ -0,0 +1,597 @@
+package com.hortonworks.atlas.trash;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.ImmutableList;
+import com.sun.jersey.api.client.Client;
+import com.sun.jersey.api.client.ClientResponse;
+import com.sun.jersey.api.client.WebResource;
+import com.sun.jersey.api.client.config.DefaultClientConfig;
+import com.sun.jersey.client.urlconnection.URLConnectionClientHandler;
+import org.apache.atlas.AtlasClient;
+
+import org.apache.atlas.hive.model.HiveDataModelGenerator;
+import org.apache.atlas.hive.model.HiveDataTypes;
+import org.apache.atlas.security.SecureClientUtils;
+import org.apache.atlas.typesystem.Referenceable;
+import org.apache.atlas.typesystem.json.InstanceSerialization;
+import org.apache.atlas.typesystem.json.TypesSerialization;
+import org.apache.atlas.typesystem.persistence.Id;
+import org.apache.atlas.typesystem.types.*;
+import org.apache.atlas.typesystem.types.utils.TypesUtil;
+import org.apache.atlas.typesystem.TypesDef;
+import org.apache.commons.configuration.PropertiesConfiguration;
+import org.apache.hadoop.hive.metastore.api.Database;
+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
+import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;
+import org.apache.hadoop.hive.ql.metadata.Table;
+import org.codehaus.jettison.json.JSONArray;
+import org.codehaus.jettison.json.JSONException;
+import org.codehaus.jettison.json.JSONObject;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import javax.ws.rs.HttpMethod;
+import javax.ws.rs.core.MediaType;
+import javax.ws.rs.core.Response;
+import javax.ws.rs.core.UriBuilder;
+
+import java.util.ArrayList;
+import java.util.Calendar;
+import java.util.List;
+
+/**
+ * This is for loading data
+ * 
+ * @author sdutta
+ *
+ */
+public class MySqlIngester {
+
+	static Logger LOG = LoggerFactory.getLogger(MySqlIngester.class);
+
+	private static final String LOAD_PROCESS_TYPE = "LoadProcess";
+	private static final String STORAGE_DESC_TYPE = "StorageDesc";
+	private static final String MYSQL_TABLE_TYPE = "demotable_type10";
+	private static final String Sqoop_TYPE = "Sqoop_Process_Type2";
+	private static final String Falcon_Type = "Falcon_Type";
+	private static final String DATABASE_TYPE = "DB";
+	private static final String COLUMN_TYPE = "Column";
+	private static final String TABLE_TYPE = "Table";
+	private static final String VIEW_TYPE = "View";
+
+	private final AtlasClient metadataServiceClient;
+
+	public MySqlIngester(String baseurl) {
+
+		this.metadataServiceClient = new AtlasClient(baseurl);
+	}
+
+	private String clustername = null;
+
+	public static void main(String[] args) throws Exception {
+		// TODO Auto-generated method stub
+
+		if (args.length < 1) {
+			throw new Exception("Please provide the DGI host url");
+		}
+
+		System.setProperty("atlas.conf", "/Users/sdutta/Applications/conf");
+
+		String baseUrl = getServerUrl(args);
+
+		MySqlIngester sqlIngester = new MySqlIngester(baseUrl);
+
+		sqlIngester.createTypes();
+		System.out.println("Creating Entitites");
+		sqlIngester.createEntities("testers", "this is data being laoded",
+				"TestDB");
+
+	}
+
+	/*
+	 * This method creates a Type
+	 */
+
+	void createTypes() throws Exception {
+
+		TypesDef typesDef = this.createMysqlTypes();
+
+		String typesAsJSON = TypesSerialization.toJson(typesDef);
+
+		System.out.println("typesAsJSON = " + typesAsJSON);
+
+		
+	  this.metadataServiceClient.createType(typesAsJSON);
+
+		System.out.println("MySQL Type System Created");
+
+	}
+
+	/**
+	 * This creates of MysqlType
+	 * 
+	 * @return
+	 * @throws Exception
+	 */
+	public TypesDef createMysqlTypes() throws Exception {
+
+		HierarchicalTypeDefinition<ClassType> mysqlTable = TypesUtil
+				.createClassTypeDef(this.MYSQL_TABLE_TYPE, null, this.attrDef("name",
+						DataTypes.STRING_TYPE), this.attrDef("description",
+						DataTypes.STRING_TYPE), this.attrDef("sourceDB",
+						DataTypes.STRING_TYPE),  this.attrDef("destinationDB",
+								DataTypes.STRING_TYPE));
+
+		HierarchicalTypeDefinition<ClassType> sqoopProcess = TypesUtil
+				.createClassTypeDef(
+						this.Sqoop_TYPE,
+						ImmutableList.of("Process"),
+						attrDef("command", DataTypes.STRING_TYPE));
+/*
+		HierarchicalTypeDefinition<ClassType> falconProcess = TypesUtil
+				.createClassTypeDef(
+						this.Falcon_Type,
+						ImmutableList.of("Process"),
+						attrDef("entityName", DataTypes.STRING_TYPE,
+								Multiplicity.REQUIRED));
+
+		HierarchicalTypeDefinition<TraitType> dimTraitDef = TypesUtil
+				.createTraitTypeDef("Dimension", null);
+
+		HierarchicalTypeDefinition<TraitType> factTraitDef = TypesUtil
+				.createTraitTypeDef("Fact", null);
+
+		HierarchicalTypeDefinition<TraitType> piiTraitDef = TypesUtil
+				.createTraitTypeDef("PII", null);
+
+		HierarchicalTypeDefinition<TraitType> metricTraitDef = TypesUtil
+				.createTraitTypeDef("Metric", null);
+
+		HierarchicalTypeDefinition<TraitType> etlTraitDef = TypesUtil
+				.createTraitTypeDef("ETL", null);
+
+		HierarchicalTypeDefinition<TraitType> valueTraitDef = TypesUtil
+				.createTraitTypeDef("Value", null);
+*/
+		return TypeUtils.getTypesDef(ImmutableList.<EnumTypeDefinition> of(),
+				ImmutableList.<StructTypeDefinition> of(),
+				ImmutableList.<HierarchicalTypeDefinition<TraitType>>of(),
+				//ImmutableList.of(dimTraitDef, factTraitDef, piiTraitDef,
+					//	metricTraitDef, etlTraitDef, valueTraitDef),
+
+				ImmutableList.of(mysqlTable,sqoopProcess));
+
+	}
+
+	/**
+	 * 
+	 * @param name
+	 * @param dataType
+	 * @param comment
+	 * @param traitNames
+	 * @return
+	 * @throws Exception
+	 */
+	Referenceable rawColumn(String name, String dataType, String comment,
+			String... traitNames) throws Exception {
+		Referenceable referenceable = new Referenceable(COLUMN_TYPE, traitNames);
+		referenceable.set("name", name);
+		referenceable.set("dataType", dataType);
+		referenceable.set("comment", comment);
+
+		return referenceable;
+	}
+
+	/**
+	 * 
+	 * @param tablename
+	 * @param tabledescription
+	 * @param sourceDB
+	 * @throws Exception
+	 */
+	public void createEntities(String tablename, String tabledescription,
+			String sourceDB) throws Exception {
+
+		// Id salesDB = database("Sales", "Sales Database", "John ETL",
+		// "hdfs://host:8000/apps/warehouse/sales");
+
+		// Referenceable sd =
+		// rawStorageDescriptor("hdfs://host:8000/apps/warehouse/sales",
+		// "TextInputFormat", "TextOutputFormat",
+		// true);
+
+		List<Referenceable> salesFactColumns = ImmutableList.of(
+				rawColumn("time_id", "int", "time id"),
+				rawColumn("product_id", "int", "product id"),
+				rawColumn("customer_id", "int", "customer id", "PII"),
+				rawColumn("sales", "double", "product id", "Metric"));
+
+		Id mysqlFact = mysqltable(tablename + "source", sourceDB, tabledescription, "Hive");
+
+		//Id hivetable = registerTable("default", tablename + "_hive");
+		
+		Id hivetable = mysqltable(tablename + "destination", sourceDB, tabledescription, "Hive");
+
+		loadProcess("sqlingestion", "mysql ingestion of data - Sqoop Process",
+				ImmutableList.of(mysqlFact), ImmutableList.of(hivetable),
+				"PII", "ETL");
+
+	}
+
+	/**
+	 *
+	 */
+	Id mysqltable(String name, String sourcedb, String description,
+			String destdb, String... traitNames) throws Exception {
+		Referenceable referenceable = new Referenceable(this.MYSQL_TABLE_TYPE,
+				traitNames);
+		referenceable.set("name", name);
+		referenceable.set("description", description);
+		referenceable.set("sourcedb", sourcedb);
+		referenceable.set("destinationdb", destdb);
+
+		return createInstance(referenceable);
+	}
+
+	/*
+	 * Id table(String name, String description, Id dbId, Referenceable sd,
+	 * String owner, String tableType, List<Referenceable> columns, String...
+	 * traitNames) throws Exception {
+	 * 
+	 * Referenceable referenceable = new Referenceable(TABLE_TYPE, traitNames);
+	 * referenceable.set("name", name); referenceable.set("description",
+	 * description); referenceable.set("owner", owner);
+	 * referenceable.set("tableType", tableType);
+	 * referenceable.set("createTime", System.currentTimeMillis());
+	 * referenceable.set("lastAccessTime", System.currentTimeMillis());
+	 * referenceable.set("retention", System.currentTimeMillis());
+	 * referenceable.set("db", dbId); referenceable.set("sd", sd);
+	 * referenceable.set("columns", columns);
+	 * 
+	 * return createInstance(referenceable); }
+	 */
+
+	Id loadProcess(String name, String description, List<Id> inputTables,
+			List<Id> outputTables, String... traitNames) throws Exception {
+		Referenceable referenceable = new Referenceable(this.Sqoop_TYPE);
+		// super type attributes
+	//	referenceable.set("entityName", name);
+		//referenceable.set("entityDescription", description);
+		referenceable.set("inputs", inputTables);
+		referenceable.set("outputs", outputTables);
+		referenceable.set("command", "sqoop -import ....");
+
+		return createInstance(referenceable);
+	}
+
+	/**
+	 * This will register the DB
+	 * 
+	 */
+	public Referenceable registerDatabase(String databaseName,
+			String clusterName, String hiveDBName, String HiveDBDescription,
+			String location, String parameter, String owner) throws Exception {
+
+		Referenceable dbRef = new Referenceable(HiveDataTypes.HIVE_DB.getName());
+		
+		dbRef.set(HiveDataModelGenerator.NAME, hiveDBName.toLowerCase());
+		dbRef.set(HiveDataModelGenerator.CLUSTER_NAME, clusterName);
+		dbRef.set("description", HiveDBDescription);
+		dbRef.set("locationUri", location);
+		dbRef.set("parameters", parameter);
+		dbRef.set("ownerName", owner);
+
+		dbRef = createInstance2(dbRef);
+
+		return dbRef;
+	}
+
+	
+	
+	/**
+	 * 
+	 * @param dbRef
+	 * @param dbName
+	 * @param tableName
+	 * @return
+	 * @throws Exception
+	 */
+	public Referenceable registerTable(String dbRef, String dbName,
+			String tableName) throws Exception {
+
+		return registerTable(dbRef, dbName, tableName);
+	}
+
+	
+	
+	/*
+	 * private Referenceable getTableReference(String dbName, String tableName)
+	 * throws Exception { LOG.debug("Getting reference for table {}.{}", dbName,
+	 * tableName);
+	 * 
+	 * String typeName = HiveDataTypes.HIVE_TABLE.getName(); String entityName =
+	 * getTableName(clusterName, dbName, tableName); String dslQuery =
+	 * String.format("%s as t where name = '%s'", typeName, entityName); return
+	 * getEntityReferenceFromDSL(typeName, dslQuery); }
+	 */
+
+	public Id registerTable(String dbName, String tableName) throws Exception {
+		LOG.info("Attempting to register table [" + tableName + "]");
+
+		Referenceable tableRef = null;
+
+		LOG.info("Importing objects from " + dbName + "." + tableName);
+
+		// Table hiveTable = tableName;
+
+		tableRef = new Referenceable(HiveDataTypes.HIVE_TABLE.getName());
+		tableRef.set(HiveDataModelGenerator.NAME, tableName);
+		tableRef.set(HiveDataModelGenerator.TABLE_NAME, tableName.toLowerCase());
+		tableRef.set("owner", "HWX");
+
+		tableRef.set("createTime", Calendar.getInstance().getTime()
+				.toLocaleString());
+		tableRef.set("lastAccessTime", "10");
+		tableRef.set("retention", "10");
+
+		tableRef.set(HiveDataModelGenerator.COMMENT,
+				"This Table is generated by SQL Ingenstion");
+
+		
+		 // add reference to the database
+		 tableRef.set(HiveDataModelGenerator.DB, "Default");
+		 
+		 //List<Referenceable> colList = getColumns(hiveTable.getCols());
+		  tableRef.set("columns", ImmutableList.of("driver_id", "driver_name"));
+		  
+		  // add reference to the StorageDescriptor StorageDescriptor
+		  //storageDesc = hiveTable.getSd(); 
+		  
+		  //Referenceable sdReferenceable =
+		  //fillStorageDescStruct(storageDesc, colList); 
+		  
+		  tableRef.set("sd",
+		  "table");
+		 
+		  // add reference to the Partition Keys List<Referenceable> partKeys =
+		 // getColumns(hiveTable.getPartitionKeys());
+		  tableRef.set("partitionKeys", "driverid");
+		 
+		  tableRef.set("parameters", "noparam");
+		  
+		 // if (hiveTable.getViewOriginalText() != null) {
+			  tableRef.set("viewOriginalText", "original text"); 
+		  
+		  //if (hiveTable.getViewExpandedText() != null) {
+			  tableRef.set("viewExpandedText", "expanded view"); 
+		 
+		tableRef.set("tableType", HiveDataTypes.HIVE_TABLE.getName());
+		tableRef.set("temporary", false);
+
+		Id id = createInstance(tableRef);
+
+		return id;
+	}
+
+	/*
+	 * Creates a Reference and returns an Instance
+	 */
+	public Referenceable createInstance2(Referenceable referenceable)
+			throws Exception {
+		String typeName = referenceable.getTypeName();
+		LOG.debug("creating instance of type " + typeName);
+
+		String entityJSON = InstanceSerialization.toJson(referenceable, true);
+		LOG.debug("Submitting new entity {} = {}", referenceable.getTypeName(),
+				entityJSON);
+		JSONObject jsonObject = this.metadataServiceClient
+				.createEntity(entityJSON);
+		String guid = jsonObject.getString(AtlasClient.GUID);
+		LOG.debug("created instance for type " + typeName + ", guid: " + guid);
+
+		return new Referenceable(guid, referenceable.getTypeName(), null);
+	}
+
+	/*
+	 * TypesDef createTypeDefinitions() throws Exception {
+	 * 
+	 * 
+	 * HierarchicalTypeDefinition<ClassType> transportClsDef = TypesUtil
+	 * .createClassTypeDef(this.TRANSPORT_TYPE, null, attrDef("name",
+	 * DataTypes.STRING_TYPE), attrDef("description", DataTypes.STRING_TYPE),
+	 * attrDef("locationUri", DataTypes.STRING_TYPE), attrDef("owner",
+	 * DataTypes.STRING_TYPE), attrDef("createTime", DataTypes.INT_TYPE));
+	 * 
+	 * HierarchicalTypeDefinition<ClassType> carrierClsDef = TypesUtil
+	 * .createClassTypeDef(this.CARRIER_TYPE, null, attrDef("name",
+	 * DataTypes.STRING_TYPE), attrDef("location", DataTypes.STRING_TYPE),
+	 * attrDef("country", DataTypes.STRING_TYPE), attrDef("CEO",
+	 * DataTypes.STRING_TYPE) );
+	 * 
+	 * HierarchicalTypeDefinition<ClassType> routeClsDef = TypesUtil
+	 * .createClassTypeDef(this.ROUTE_TYPE, null, attrDef("name",
+	 * DataTypes.STRING_TYPE), attrDef("route_id", DataTypes.STRING_TYPE),
+	 * attrDef("comment", DataTypes.STRING_TYPE));
+	 * 
+	 * HierarchicalTypeDefinition<ClassType> motionClsDef = TypesUtil
+	 * .createClassTypeDef(this.MOTION_TYPE, null, attrDef("rating",
+	 * DataTypes.STRING_TYPE), attrDef("metrics", DataTypes.STRING_TYPE),
+	 * attrDef("comment", DataTypes.STRING_TYPE));
+	 * 
+	 * 
+	 * HierarchicalTypeDefinition<TraitType> dimTraitDef =
+	 * TypesUtil.createTraitTypeDef("Dimension", null);
+	 * 
+	 * HierarchicalTypeDefinition<TraitType> factTraitDef =
+	 * TypesUtil.createTraitTypeDef("Fact", null);
+	 * 
+	 * HierarchicalTypeDefinition<TraitType> piiTraitDef =
+	 * TypesUtil.createTraitTypeDef("PII", null);
+	 * 
+	 * HierarchicalTypeDefinition<TraitType> metricTraitDef =
+	 * TypesUtil.createTraitTypeDef("Metric", null);
+	 * 
+	 * HierarchicalTypeDefinition<TraitType> etlTraitDef =
+	 * TypesUtil.createTraitTypeDef("ETL", null);
+	 * 
+	 * return TypeUtils.getTypesDef(ImmutableList.<EnumTypeDefinition>of(),
+	 * ImmutableList.<StructTypeDefinition>of(),
+	 * 
+	 * ImmutableList.<HierarchicalTypeDefinition<TraitType>>of(),
+	 * //ImmutableList.of(dimTraitDef, factTraitDef, piiTraitDef,
+	 * metricTraitDef, etlTraitDef),
+	 * 
+	 * ImmutableList.of(transportClsDef, carrierClsDef, routeClsDef,
+	 * motionClsDef)); }
+	 */
+
+	AttributeDefinition attrDef(String name, IDataType dT) {
+		return attrDef(name, dT, Multiplicity.OPTIONAL, false, null);
+	}
+
+	AttributeDefinition attrDef(String name, IDataType dT, Multiplicity m) {
+		return attrDef(name, dT, m, false, null);
+	}
+
+	AttributeDefinition attrDef(String name, IDataType dT, Multiplicity m,
+			boolean isComposite, String reverseAttributeName) {
+		Preconditions.checkNotNull(name);
+		Preconditions.checkNotNull(dT);
+		return new AttributeDefinition(name, dT.getName(), m, isComposite,
+				reverseAttributeName);
+	}
+
+	/**
+	 * This creates a new Client
+	 * 
+	 * @param referenceable
+	 * @return
+	 * @throws Exception
+	 */
+	private Id createInstance(Referenceable referenceable) throws Exception {
+
+		String typeName = referenceable.getTypeName();
+
+		String entityJSON = InstanceSerialization.toJson(referenceable, true);
+
+		System.out.println("Submitting new entity= " + entityJSON);
+
+		JSONObject jsonObject = metadataServiceClient.createEntity(entityJSON);
+		String guid = jsonObject.getString(AtlasClient.GUID);
+
+		System.out.println("created instance for type " + typeName + ", guid: "
+				+ guid);
+
+		// return the Id for created instance with guid
+		return new Id(guid, referenceable.getId().getVersion(),
+				referenceable.getTypeName());
+	}
+
+	/**
+	 * Create Entities for the type definitions. Types can be class, struct or a
+	 * Java cas
+	 * 
+	 * @throws Exception
+	 */
+	/*
+	 * void createEntities() throws Exception {
+	 * 
+	 * Id boeingDB = transport("Boeing 747", "Best Plane in the United States",
+	 * "James McNeary", "http://wwww.boeing.com");
+	 * 
+	 * Referenceable carrier = carrier("United Airlines", "San Francisco",
+	 * "USA", "James McNeary");
+	 * 
+	 * Id carrierId = this.createInstance(carrier); }
+	 */
+	/*
+	 * Id transport(String name, String description, String owner, String
+	 * locationUri, String... traitNames) throws Exception { Referenceable
+	 * referenceable = new Referenceable(this.TRANSPORT_TYPE, traitNames);
+	 * referenceable.set("name", name); referenceable.set("description",
+	 * description); referenceable.set("owner", owner);
+	 * referenceable.set("locationUri", locationUri);
+	 * referenceable.set("createTime", System.currentTimeMillis());
+	 * 
+	 * return createInstance(referenceable); }
+	 * 
+	 * 
+	 * 
+	 * 
+	 * Referenceable carrier(String name, String location, String country,
+	 * String CEO) throws Exception { Referenceable referenceable = new
+	 * Referenceable(this.CARRIER_TYPE); referenceable.set("name", name);
+	 * referenceable.set("location", location); referenceable.set("country",
+	 * country); referenceable.set("CEO", CEO);
+	 * 
+	 * return referenceable; }
+	 * 
+	 * Referenceable route(String name, String route_id, String comment ) throws
+	 * Exception { Referenceable referenceable = new
+	 * Referenceable(this.ROUTE_TYPE); referenceable.set("name", name);
+	 * referenceable.set("dataType", route_id); referenceable.set("comment",
+	 * comment);
+	 * 
+	 * return referenceable; }
+	 * 
+	 * 
+	 * Referenceable motion(String rating, String metrics, String comment )
+	 * throws Exception { Referenceable referenceable = new
+	 * Referenceable(this.MOTION_TYPE); referenceable.set("rating", rating);
+	 * referenceable.set("metrics", metrics); referenceable.set("comment",
+	 * comment);
+	 * 
+	 * return referenceable; }
+	 */
+
+	/*
+	 * Id table(String name, String description, Id dbId, Referenceable sd,
+	 * String owner, String tableType, List<Referenceable> columns, String...
+	 * traitNames) throws Exception {
+	 * 
+	 * Referenceable referenceable = new Referenceable(TABLE_TYPE, traitNames);
+	 * referenceable.set("name", name); referenceable.set("description",
+	 * description); referenceable.set("owner", owner);
+	 * referenceable.set("tableType", tableType);
+	 * referenceable.set("createTime", System.currentTimeMillis());
+	 * referenceable.set("lastAccessTime", System.currentTimeMillis());
+	 * referenceable.set("retention", System.currentTimeMillis());
+	 * referenceable.set("db", dbId); referenceable.set("sd", sd);
+	 * referenceable.set("columns", columns);
+	 * 
+	 * return createInstance(referenceable); }
+	 * 
+	 * Id loadProcess(String name, String description, String user, List<Id>
+	 * inputTables, List<Id> outputTables, String queryText, String queryPlan,
+	 * String queryId, String queryGraph, String... traitNames) throws Exception
+	 * { Referenceable referenceable = new Referenceable(LOAD_PROCESS_TYPE,
+	 * traitNames); // super type attributes referenceable.set("name", name);
+	 * referenceable.set("description", description);
+	 * referenceable.set("inputs", inputTables); referenceable.set("outputs",
+	 * outputTables);
+	 * 
+	 * referenceable.set("user", user); referenceable.set("startTime",
+	 * System.currentTimeMillis()); referenceable.set("endTime",
+	 * System.currentTimeMillis() + 10000);
+	 * 
+	 * referenceable.set("queryText", queryText); referenceable.set("queryPlan",
+	 * queryPlan); referenceable.set("queryId", queryId);
+	 * referenceable.set("queryGraph", queryGraph);
+	 * 
+	 * return createInstance(referenceable); }
+	 */
+
+	/*
+	 * This function gets the data
+	 */
+	static String getServerUrl(String[] args) {
+		String baseUrl = "http://atlasdemo.cloud.hortonworks.com:21000";
+		if (args.length > 0) {
+			baseUrl = args[0];
+		}
+
+		System.out.println(baseUrl);
+		return baseUrl;
+	}
+
+}
diff --git a/codesamples/atlas/src/main/java/com/hortonworks/atlas/trash/TruckHiveMetaDataGenerator.java b/codesamples/atlas/src/main/java/com/hortonworks/atlas/trash/TruckHiveMetaDataGenerator.java
new file mode 100644
index 0000000..b927470
--- /dev/null
+++ b/codesamples/atlas/src/main/java/com/hortonworks/atlas/trash/TruckHiveMetaDataGenerator.java
@@ -0,0 +1,567 @@
+package com.hortonworks.atlas.trash;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+
+import org.apache.atlas.AtlasClient;
+import org.apache.atlas.AtlasServiceException;
+import org.apache.atlas.hive.model.HiveDataModelGenerator;
+import org.apache.atlas.hive.model.HiveDataTypes;
+import org.apache.atlas.typesystem.Referenceable;
+import org.apache.atlas.typesystem.Struct;
+import org.apache.atlas.typesystem.json.InstanceSerialization;
+import org.apache.atlas.typesystem.persistence.Id;
+import org.apache.commons.lang.StringEscapeUtils;
+import org.apache.commons.lang.StringUtils;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.api.Database;
+import org.apache.hadoop.hive.metastore.api.FieldSchema;
+import org.apache.hadoop.hive.metastore.api.Index;
+import org.apache.hadoop.hive.metastore.api.Order;
+import org.apache.hadoop.hive.metastore.api.SerDeInfo;
+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
+import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;
+import org.apache.hadoop.hive.ql.metadata.Hive;
+import org.apache.hadoop.hive.ql.metadata.Partition;
+import org.apache.hadoop.hive.ql.metadata.Table;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.codehaus.jettison.json.JSONArray;
+import org.codehaus.jettison.json.JSONException;
+import org.codehaus.jettison.json.JSONObject;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.ImmutableList;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Set;
+
+/**
+ * A Bridge Utility that imports metadata from the Hive Meta Store
+ * and registers then in Atlas.
+ */
+public class TruckHiveMetaDataGenerator {
+	
+	{
+		System.setProperty("atlas.conf", "/Users/sdutta/Applications/conf");
+	}
+	
+	
+    private static final String DEFAULT_DGI_URL = "http://localhost:21000/";
+    
+    public static final String DEFAULT_CLUSTER_NAME = "primary";
+    private static String clusterName = "atlasdemo";
+
+    public static final String DGI_URL_PROPERTY = "hive.hook.dgi.url";
+
+    private static final Logger LOG = LoggerFactory.getLogger(TruckHiveMetaDataGenerator.class);
+
+    private final Hive hiveClient = null;
+    private  AtlasClient atlasClient;
+    private static String databasename = null;
+    private static String tablename = null;
+    
+    public static void main(String[] args) throws Exception {
+    	
+    	clusterName = args[1];
+    	String baseurl = args[0];
+    	databasename = args[2];
+    	tablename = args[3];
+    	
+    	
+    	
+    	TruckHiveMetaDataGenerator hmg = new TruckHiveMetaDataGenerator(baseurl);
+    	
+    	Referenceable db = hmg.registerDatabase(databasename, clusterName);
+    	hmg.registerTable(db, databasename, tablename);
+    	
+    }
+
+    /**
+     * 
+     * @param baseurl
+     */
+    public TruckHiveMetaDataGenerator(String baseurl) {
+    	
+    	atlasClient = new AtlasClient(baseurl);
+    
+    }
+    
+    
+    public AtlasClient getAtlasClient() {
+        return atlasClient;
+    }
+
+
+      
+
+    public Referenceable registerDatabase(String databaseName, String clusterName) throws Exception {
+        Referenceable dbRef = getDatabaseReference(databaseName, clusterName);
+        
+        if (dbRef == null) {
+            LOG.info("Importing objects from databaseName : " + databaseName);
+            //Database hiveDB = hiveClient.getDatabase(databaseName);
+
+            dbRef = new Referenceable(HiveDataTypes.HIVE_DB.getName());
+            dbRef.set(HiveDataModelGenerator.NAME, databaseName);
+            dbRef.set(HiveDataModelGenerator.CLUSTER_NAME, clusterName);
+            dbRef.set("description", "this is a default database");
+            dbRef.set("locationUri", "/hive/default");
+            dbRef.set("parameters", "key1=name1,key2=name2");
+            dbRef.set("ownerName", "Hortonworks");
+            dbRef = createInstance(dbRef);
+            
+            
+        } else {
+            LOG.info("Database {} is already registered with id {}", databaseName, dbRef.getId().id);
+        }
+        
+        return dbRef;
+    }
+
+    public Referenceable createInstance(Referenceable referenceable) throws Exception {
+        String typeName = referenceable.getTypeName();
+        LOG.debug("creating instance of type " + typeName);
+
+        String entityJSON = InstanceSerialization.toJson(referenceable, true);
+        LOG.debug("Submitting new entity {} = {}", referenceable.getTypeName(), entityJSON);
+        JSONObject jsonObject = atlasClient.createEntity(entityJSON);
+        String guid = jsonObject.getString(AtlasClient.GUID);
+        LOG.debug("created instance for type " + typeName + ", guid: " + guid);
+
+        return new Referenceable(guid, referenceable.getTypeName(), null);
+    }
+
+   
+
+    /**
+     * Gets reference for the database
+     *
+     *
+     * @param databaseName  database Name
+     * @param clusterName    cluster name
+     * @return Reference for database if exists, else null
+     * @throws Exception
+     */
+    private Referenceable getDatabaseReference(String databaseName, String clusterName) throws Exception {
+        LOG.debug("Getting reference for database {}", databaseName);
+        String typeName = HiveDataTypes.HIVE_DB.getName();
+
+        String dslQuery = String.format("%s where %s = '%s' and %s = '%s'", typeName, HiveDataModelGenerator.NAME,
+                databaseName.toLowerCase(), HiveDataModelGenerator.CLUSTER_NAME, clusterName);
+        return getEntityReferenceFromDSL(typeName, dslQuery);
+    }
+
+    public Referenceable getProcessReference(String queryStr) throws Exception {
+        LOG.debug("Getting reference for process with query {}", queryStr);
+        String typeName = HiveDataTypes.HIVE_PROCESS.getName();
+
+        //todo enable DSL
+        //        String dslQuery = String.format("%s where queryText = \"%s\"", typeName, queryStr);
+        //        return getEntityReferenceFromDSL(typeName, dslQuery);
+
+        String gremlinQuery =
+                String.format("g.V.has('__typeName', '%s').has('%s.queryText', \"%s\").toList()", typeName, typeName,
+                        StringEscapeUtils.escapeJava(queryStr));
+        return getEntityReferenceFromGremlin(typeName, gremlinQuery);
+    }
+
+    private Referenceable getEntityReferenceFromDSL(String typeName, String dslQuery) throws Exception {
+        AtlasClient dgiClient = getAtlasClient();
+        JSONArray results = dgiClient.searchByDSL(dslQuery);
+        if (results.length() == 0) {
+            return null;
+        } else {
+            String guid;
+            JSONObject row = results.getJSONObject(0);
+            if (row.has("$id$")) {
+                guid = row.getJSONObject("$id$").getString("id");
+            } else {
+                guid = row.getJSONObject("_col_0").getString("id");
+            }
+            return new Referenceable(guid, typeName, null);
+        }
+    }
+
+    public static String getTableName(String clusterName, String dbName, String tableName) {
+        return String.format("%s.%s@%s", dbName.toLowerCase(), tableName.toLowerCase(), clusterName);
+    }
+
+    /**
+     * Gets reference for the table
+     *
+     * @param dbName database name
+     * @param tableName table name
+     * @return table reference if exists, else null
+     * @throws Exception
+     */
+    private Referenceable getTableReference(String dbName, String tableName) throws Exception {
+        LOG.debug("Getting reference for table {}.{}", dbName, tableName);
+
+        String typeName = HiveDataTypes.HIVE_TABLE.getName();
+        String entityName = getTableName(clusterName, dbName, tableName);
+        String dslQuery = String.format("%s as t where name = '%s'", typeName, entityName);
+        return getEntityReferenceFromDSL(typeName, dslQuery);
+    }
+
+    private Referenceable getEntityReferenceFromGremlin(String typeName, String gremlinQuery)
+    throws AtlasServiceException, JSONException {
+        AtlasClient client = getAtlasClient();
+        JSONObject response = client.searchByGremlin(gremlinQuery);
+        JSONArray results = response.getJSONArray(AtlasClient.RESULTS);
+        if (results.length() == 0) {
+            return null;
+        }
+        String guid = results.getJSONObject(0).getString("__guid");
+        return new Referenceable(guid, typeName, null);
+    }
+
+    private Referenceable getPartitionReference(String dbName, String tableName, List<String> values) throws Exception {
+        String valuesStr = "['" + StringUtils.join(values, "', '") + "']";
+        LOG.debug("Getting reference for partition for {}.{} with values {}", dbName, tableName, valuesStr);
+        String typeName = HiveDataTypes.HIVE_PARTITION.getName();
+
+        //todo replace gremlin with DSL
+        //        String dslQuery = String.format("%s as p where values = %s, tableName where name = '%s', "
+        //                        + "dbName where name = '%s' and clusterName = '%s' select p", typeName, valuesStr,
+        // tableName,
+        //                dbName, clusterName);
+
+        String datasetType = AtlasClient.DATA_SET_SUPER_TYPE;
+        String tableEntityName = getTableName(clusterName, dbName, tableName);
+
+        String gremlinQuery = String.format("g.V.has('__typeName', '%s').has('%s.values', %s).as('p')."
+                        + "out('__%s.table').has('%s.name', '%s').back('p').toList()", typeName, typeName, valuesStr,
+                typeName, datasetType, tableEntityName);
+
+        return getEntityReferenceFromGremlin(typeName, gremlinQuery);
+    }
+
+    private Referenceable getSDForTable(String dbName, String tableName) throws Exception {
+        Referenceable tableRef = getTableReference(dbName, tableName);
+        if (tableRef == null) {
+            throw new IllegalArgumentException("Table " + dbName + "." + tableName + " doesn't exist");
+        }
+
+        AtlasClient dgiClient = getAtlasClient();
+        Referenceable tableInstance = dgiClient.getEntity(tableRef.getId().id);
+        Id sdId = (Id) tableInstance.get("sd");
+        return new Referenceable(sdId.id, sdId.getTypeName(), null);
+    }
+
+    /**
+     * 
+     * @param dbName
+     * @param tableName
+     * @return
+     * @throws Exception
+     */
+    public Referenceable registerTable(String dbName, String tableName) throws Exception {
+        Referenceable dbReferenceable = registerDatabase(dbName, clusterName);
+        return registerTable(dbReferenceable, dbName, tableName);
+    }
+
+    
+    /**
+     * 
+     * @param dbReference
+     * @param dbName
+     * @param tableName
+     * @return
+     * @throws Exception
+     */
+    public Referenceable registerTable(Referenceable dbReference, String dbName, String tableName) throws Exception {
+        LOG.info("Attempting to register table [" + tableName + "]");
+        Referenceable tableRef = getTableReference(dbName, tableName);
+        
+        if (tableRef == null) {
+            LOG.info("Importing objects from " + dbName + "." + tableName);
+
+            //Table hiveTable = hiveClient.getTable(dbName, tableName);
+
+            tableRef = new Referenceable(HiveDataTypes.HIVE_TABLE.getName(),"Trucks");
+            tableRef.set(HiveDataModelGenerator.NAME,
+                    getTableName(clusterName, dbName, tableName));
+            
+            tableRef.set(HiveDataModelGenerator.TABLE_NAME,tableName.toLowerCase());
+            tableRef.set("owner", "Hortonworks");
+
+            tableRef.set("createTime", System.currentTimeMillis());
+            tableRef.set("lastAccessTime",System.currentTimeMillis());
+            tableRef.set("retention", System.currentTimeMillis());
+
+            tableRef.set(HiveDataModelGenerator.COMMENT, "This is loaded by Sqoop job");
+
+            // add reference to the database
+            tableRef.set(HiveDataModelGenerator.DB, dbReference);
+            
+            List<Referenceable> truckcols = ImmutableList
+                    .of(rawColumn("model_id", "String", "model_id"), rawColumn("model_name", "String", "model name"),
+                            rawColumn("max_speed", "String", "maximum speed", "Red"),
+                            rawColumn("torque", "String", "torque"),
+                            rawColumn("engine_type", "String", "engine diesel/gas"),
+                            rawColumn("tow_capacity", "String", "towing capacity"),
+                            rawColumn("model_year", "String", "model_year"));
+            
+            
+            tableRef.set("columns", truckcols);
+            
+            // add reference to the StorageDescriptor
+            //StorageDescriptor storageDesc = hiveTable.getSd();
+            //Referenceable sdReferenceable = fillStorageDescStruct(storageDesc, colList);
+            //tableRef.set("sd", sdReferenceable);
+
+            // add reference to the Partition Keys
+            //List<Referenceable> partKeys = getColumns(hiveTable.getPartitionKeys());
+            //tableRef.set("partitionKeys", partKeys);
+
+           // tableRef.set("parameters", "params");
+
+            
+            tableRef.set("viewOriginalText", "Original text");
+           
+
+            
+           tableRef.set("viewExpandedText", "Expanded Text");
+            
+
+            tableRef.set("tableType", "Sqoop generated table");
+            tableRef.set("temporary", "false");
+
+
+            tableRef = createInstance(tableRef);
+            
+        } else {
+            LOG.info("Table {}.{} is already registered with id {}", dbName, tableName, tableRef.getId().id);
+        }
+        return tableRef;
+    }
+
+    
+    /**
+     * 
+     * @param db
+     * @param tableName
+     * @param tableReferenceable
+     * @param sdReferenceable
+     * @throws Exception
+     */
+    private void registerPartitions(String db, String tableName, Referenceable tableReferenceable,
+            Referenceable sdReferenceable) throws Exception {
+        Set<Partition> tableParts = hiveClient.getAllPartitionsOf(new Table(Table.getEmptyTable(db, tableName)));
+
+        if (tableParts.size() > 0) {
+            for (Partition hivePart : tableParts) {
+                registerPartition(hivePart, tableReferenceable, sdReferenceable);
+            }
+        }
+    }
+
+    /**
+     * 
+     * @param partition
+     * @return
+     * @throws Exception
+     */
+    public Referenceable registerPartition(Partition partition) throws Exception {
+        String dbName = partition.getTable().getDbName();
+        String tableName = partition.getTable().getTableName();
+        Referenceable tableRef = registerTable(dbName, tableName);
+        Referenceable sdRef = getSDForTable(dbName, tableName);
+        return registerPartition(partition, tableRef, sdRef);
+    }
+
+    private Referenceable registerPartition(Partition hivePart, Referenceable tableReferenceable,
+            Referenceable sdReferenceable) throws Exception {
+        LOG.info("Registering partition for {} with values {}", tableReferenceable,
+                StringUtils.join(hivePart.getValues(), ","));
+        String dbName = hivePart.getTable().getDbName();
+        String tableName = hivePart.getTable().getTableName();
+
+        Referenceable partRef = getPartitionReference(dbName, tableName, hivePart.getValues());
+        if (partRef == null) {
+            partRef = new Referenceable(HiveDataTypes.HIVE_PARTITION.getName());
+            partRef.set("values", hivePart.getValues());
+
+            partRef.set(HiveDataModelGenerator.TABLE, tableReferenceable);
+
+            //todo fix
+            partRef.set("createTime", hivePart.getLastAccessTime());
+            partRef.set("lastAccessTime", hivePart.getLastAccessTime());
+
+            // sdStruct = fillStorageDescStruct(hivePart.getSd());
+            // Instead of creating copies of the sdstruct for partitions we are reusing existing
+            // ones will fix to identify partitions with differing schema.
+            partRef.set("sd", sdReferenceable);
+
+            partRef.set("parameters", hivePart.getParameters());
+            partRef = createInstance(partRef);
+        } else {
+            LOG.info("Partition {}.{} with values {} is already registered with id {}", dbName, tableName,
+                    StringUtils.join(hivePart.getValues(), ","), partRef.getId().id);
+        }
+        return partRef;
+    }
+
+    private void importIndexes(String db, String table, Referenceable dbReferenceable, Referenceable tableReferenceable)
+    throws Exception {
+        List<Index> indexes = hiveClient.getIndexes(db, table, Short.MAX_VALUE);
+        if (indexes.size() > 0) {
+            for (Index index : indexes) {
+                importIndex(index, dbReferenceable, tableReferenceable);
+            }
+        }
+    }
+
+    //todo should be idempotent
+    private void importIndex(Index index, Referenceable dbReferenceable, Referenceable tableReferenceable)
+            throws Exception {
+        LOG.info("Importing index {} for {}.{}", index.getIndexName(), dbReferenceable, tableReferenceable);
+        Referenceable indexRef = new Referenceable(HiveDataTypes.HIVE_INDEX.getName());
+
+        indexRef.set(HiveDataModelGenerator.NAME, index.getIndexName());
+        indexRef.set("indexHandlerClass", index.getIndexHandlerClass());
+
+        indexRef.set(HiveDataModelGenerator.DB, dbReferenceable);
+
+        indexRef.set("createTime", index.getCreateTime());
+        indexRef.set("lastAccessTime", index.getLastAccessTime());
+        indexRef.set("origTable", index.getOrigTableName());
+        indexRef.set("indexTable", index.getIndexTableName());
+
+        Referenceable sdReferenceable = fillStorageDescStruct(index.getSd(), null);
+        indexRef.set("sd", sdReferenceable);
+
+        indexRef.set("parameters", index.getParameters());
+
+        tableReferenceable.set("deferredRebuild", index.isDeferredRebuild());
+
+        createInstance(indexRef);
+    }
+
+    private Referenceable fillStorageDescStruct(StorageDescriptor storageDesc, List<Referenceable> colList)
+    throws Exception {
+        LOG.debug("Filling storage descriptor information for " + storageDesc);
+
+        Referenceable sdReferenceable = new Referenceable(HiveDataTypes.HIVE_STORAGEDESC.getName());
+
+        SerDeInfo serdeInfo = storageDesc.getSerdeInfo();
+        LOG.debug("serdeInfo = " + serdeInfo);
+        // SkewedInfo skewedInfo = storageDesc.getSkewedInfo();
+
+        String serdeInfoName = HiveDataTypes.HIVE_SERDE.getName();
+        Struct serdeInfoStruct = new Struct(serdeInfoName);
+
+        serdeInfoStruct.set(HiveDataModelGenerator.NAME, serdeInfo.getName());
+        serdeInfoStruct.set("serializationLib", serdeInfo.getSerializationLib());
+        serdeInfoStruct.set("parameters", serdeInfo.getParameters());
+
+        sdReferenceable.set("serdeInfo", serdeInfoStruct);
+        sdReferenceable.set(HiveDataModelGenerator.STORAGE_NUM_BUCKETS, storageDesc.getNumBuckets());
+        sdReferenceable
+                .set(HiveDataModelGenerator.STORAGE_IS_STORED_AS_SUB_DIRS, storageDesc.isStoredAsSubDirectories());
+
+        //Use the passed column list if not null, ex: use same references for table and SD
+        List<FieldSchema> columns = storageDesc.getCols();
+        if (columns != null && !columns.isEmpty()) {
+            if (colList != null) {
+                sdReferenceable.set("cols", colList);
+            } else {
+                sdReferenceable.set("cols", getColumns(columns));
+            }
+        }
+
+        List<Struct> sortColsStruct = new ArrayList<Struct>();
+        for (Order sortcol : storageDesc.getSortCols()) {
+            String hiveOrderName = HiveDataTypes.HIVE_ORDER.getName();
+            Struct colStruct = new Struct(hiveOrderName);
+            colStruct.set("col", sortcol.getCol());
+            colStruct.set("order", sortcol.getOrder());
+
+            sortColsStruct.add(colStruct);
+        }
+        if (sortColsStruct.size() > 0) {
+            sdReferenceable.set("sortCols", sortColsStruct);
+        }
+
+        sdReferenceable.set("location", storageDesc.getLocation());
+        sdReferenceable.set("inputFormat", storageDesc.getInputFormat());
+        sdReferenceable.set("outputFormat", storageDesc.getOutputFormat());
+        sdReferenceable.set("compressed", storageDesc.isCompressed());
+
+        if (storageDesc.getBucketCols().size() > 0) {
+            sdReferenceable.set("bucketCols", storageDesc.getBucketCols());
+        }
+
+        sdReferenceable.set("parameters", storageDesc.getParameters());
+        sdReferenceable.set("storedAsSubDirectories", storageDesc.isStoredAsSubDirectories());
+
+        return createInstance(sdReferenceable);
+    }
+
+    private List<Referenceable> getColumns(List<FieldSchema> schemaList) throws Exception {
+        List<Referenceable> colList = new ArrayList<Referenceable>();
+        for (FieldSchema fs : schemaList) {
+            LOG.debug("Processing field " + fs);
+            Referenceable colReferenceable = new Referenceable(HiveDataTypes.HIVE_COLUMN.getName());
+            colReferenceable.set(HiveDataModelGenerator.NAME, fs.getName());
+            colReferenceable.set("type", fs.getType());
+            colReferenceable.set(HiveDataModelGenerator.COMMENT, fs.getComment());
+
+            colList.add(createInstance(colReferenceable));
+        }
+        return colList;
+    }
+
+    public synchronized void registerHiveDataModel() throws Exception {
+        HiveDataModelGenerator dataModelGenerator = new HiveDataModelGenerator();
+        AtlasClient dgiClient = getAtlasClient();
+
+        //Register hive data model if its not already registered
+        if (dgiClient.getType(HiveDataTypes.HIVE_PROCESS.getName()) == null) {
+            LOG.info("Registering Hive data model");
+            dgiClient.createType(dataModelGenerator.getModelAsJson());
+        } else {
+            LOG.info("Hive data model is already registered!");
+        }
+    }
+
+    
+
+	Referenceable rawColumn(String name, String dataType, String comment, String... traitNames) throws Exception {
+	        Referenceable referenceable = new Referenceable(HiveDataTypes.HIVE_COLUMN.getName(), traitNames);
+	        referenceable.set("name", name);
+	        referenceable.set("type", dataType);
+	        referenceable.set("comment", comment);
+
+	        return referenceable;
+	    }
+	  
+	
+   
+    public void updateTable(Referenceable tableReferenceable, Table newTable) throws AtlasServiceException {
+        AtlasClient client = getAtlasClient();
+        client.updateEntity(tableReferenceable.getId()._getId(), HiveDataModelGenerator.TABLE_NAME,
+                newTable.getTableName().toLowerCase());
+        client.updateEntity(tableReferenceable.getId()._getId(), HiveDataModelGenerator.NAME,
+                getTableName(clusterName, newTable.getDbName(), newTable.getTableName()));
+    }
+}
\ No newline at end of file
diff --git a/codesamples/atlas/src/main/java/com/hortonworks/atlas/trash/TrucksClass.java b/codesamples/atlas/src/main/java/com/hortonworks/atlas/trash/TrucksClass.java
new file mode 100644
index 0000000..0e6b3e4
--- /dev/null
+++ b/codesamples/atlas/src/main/java/com/hortonworks/atlas/trash/TrucksClass.java
@@ -0,0 +1,386 @@
+package com.hortonworks.atlas.trash;
+
+
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.ImmutableList;
+
+import org.apache.atlas.AtlasClient;
+import org.apache.atlas.typesystem.Referenceable;
+import org.apache.atlas.typesystem.TypesDef;
+import org.apache.atlas.typesystem.json.InstanceSerialization;
+import org.apache.atlas.typesystem.json.TypesSerialization;
+import org.apache.atlas.typesystem.persistence.Id;
+import org.apache.atlas.typesystem.types.AttributeDefinition;
+import org.apache.atlas.typesystem.types.ClassType;
+import org.apache.atlas.typesystem.types.DataTypes;
+import org.apache.atlas.typesystem.types.EnumTypeDefinition;
+import org.apache.atlas.typesystem.types.HierarchicalTypeDefinition;
+import org.apache.atlas.typesystem.types.IDataType;
+import org.apache.atlas.typesystem.types.Multiplicity;
+import org.apache.atlas.typesystem.types.StructTypeDefinition;
+import org.apache.atlas.typesystem.types.TraitType;
+import org.apache.atlas.typesystem.types.TypeUtils;
+import org.apache.atlas.typesystem.types.utils.TypesUtil;
+import org.codehaus.jettison.json.JSONArray;
+import org.codehaus.jettison.json.JSONObject;
+
+import java.util.List;
+
+/**
+ * A driver that sets up sample types and data for testing purposes.
+ * Please take a look at QueryDSL in docs for the Meta Model.
+ * todo - move this to examples module.
+ */
+public class TrucksClass {
+
+	{
+		System.setProperty("atlas.conf", "/Users/sdutta/Applications/conf");
+	}
+	
+
+    private static final String DATABASE_TYPE = "DB";
+    private static final String COLUMN_TYPE = "Column";
+    private static final String TABLE_TYPE = "Table";
+    private static final String VIEW_TYPE = "View";
+    private static final String LOAD_PROCESS_TYPE = "LoadProcess";
+    private static final String STORAGE_DESC_TYPE = "StorageDesc";
+    
+
+    private static final String[] TYPES =
+            {DATABASE_TYPE, TABLE_TYPE, STORAGE_DESC_TYPE, COLUMN_TYPE, LOAD_PROCESS_TYPE, VIEW_TYPE, "JdbcAccess",
+                    "ETL", "Metric", "PII", "Fact", "Dimension"};
+
+    private final AtlasClient metadataServiceClient;
+
+
+	/**
+	 * 
+	 * @param args
+	 * @throws Exception
+	 */
+    public static void main(String[] args) throws Exception {
+        
+    	
+    	String baseUrl = args[0];
+    	String databasename = args[1];
+    	String tablename = args[2];
+    	String tablename2 = args[3];
+    	String flag = args[4];
+    	
+        TrucksClass quickStart = new TrucksClass(baseUrl);
+
+        // Shows how to create types in Atlas for your meta model
+        
+        if("createtype".equalsIgnoreCase(flag))
+        		quickStart.createTypes();
+
+        // Shows how to create entities (instances) for the added types in Atlas
+        quickStart.createMysqlEntities(databasename, tablename, tablename2);
+
+        // Shows some search queries using DSL based on types
+        if("search".equalsIgnoreCase(flag))
+        	quickStart.search();
+    }
+
+    
+    TrucksClass(String baseUrl) {
+    	
+    	
+        metadataServiceClient = new AtlasClient(baseUrl);
+    }
+
+
+    void createTypes() throws Exception {
+        TypesDef typesDef = createTypeDefinitions();
+
+        String typesAsJSON = TypesSerialization.toJson(typesDef);
+        System.out.println("typesAsJSON = " + typesAsJSON);
+        metadataServiceClient.createType(typesAsJSON);
+
+        // verify types created
+        verifyTypesCreated();
+    }
+
+    
+    
+    
+    TypesDef createTypeDefinitions() throws Exception {
+    
+    	HierarchicalTypeDefinition<ClassType> dbClsDef = TypesUtil
+                .createClassTypeDef(DATABASE_TYPE, null, attrDef("name", DataTypes.STRING_TYPE),
+                        attrDef("description", DataTypes.STRING_TYPE), attrDef("locationUri", DataTypes.STRING_TYPE),
+                        attrDef("owner", DataTypes.STRING_TYPE), attrDef("createTime", DataTypes.INT_TYPE));
+
+        HierarchicalTypeDefinition<ClassType> storageDescClsDef = TypesUtil
+                .createClassTypeDef(STORAGE_DESC_TYPE, null, attrDef("location", DataTypes.STRING_TYPE),
+                        attrDef("inputFormat", DataTypes.STRING_TYPE), attrDef("outputFormat", DataTypes.STRING_TYPE),
+                        attrDef("compressed", DataTypes.STRING_TYPE, Multiplicity.REQUIRED, false, null));
+
+        HierarchicalTypeDefinition<ClassType> columnClsDef = TypesUtil
+                .createClassTypeDef(COLUMN_TYPE, null, attrDef("name", DataTypes.STRING_TYPE),
+                        attrDef("dataType", DataTypes.STRING_TYPE), attrDef("comment", DataTypes.STRING_TYPE));
+
+        
+        HierarchicalTypeDefinition<ClassType> tblClsDef = TypesUtil
+                .createClassTypeDef(TABLE_TYPE, ImmutableList.of("DataSet"),
+                        new AttributeDefinition("db", DATABASE_TYPE, Multiplicity.REQUIRED, false, null),
+                        new AttributeDefinition("sd", STORAGE_DESC_TYPE, Multiplicity.REQUIRED, true, null),
+                        attrDef("owner", DataTypes.STRING_TYPE), attrDef("createTime", DataTypes.INT_TYPE),
+                        attrDef("lastAccessTime", DataTypes.INT_TYPE), attrDef("retention", DataTypes.INT_TYPE),
+                        attrDef("viewOriginalText", DataTypes.STRING_TYPE),
+                        attrDef("viewExpandedText", DataTypes.STRING_TYPE), attrDef("tableType", DataTypes.STRING_TYPE),
+                        attrDef("temporary", DataTypes.BOOLEAN_TYPE),
+                        new AttributeDefinition("columns", DataTypes.arrayTypeName(COLUMN_TYPE),
+                                Multiplicity.COLLECTION, true, null));
+
+        
+        HierarchicalTypeDefinition<ClassType> loadProcessClsDef = TypesUtil
+                .createClassTypeDef(LOAD_PROCESS_TYPE, ImmutableList.of("Process"),
+                        attrDef("userName", DataTypes.STRING_TYPE), attrDef("startTime", DataTypes.INT_TYPE),
+                        attrDef("endTime", DataTypes.INT_TYPE),
+                        attrDef("queryText", DataTypes.STRING_TYPE, Multiplicity.REQUIRED),
+                        attrDef("queryPlan", DataTypes.STRING_TYPE, Multiplicity.REQUIRED),
+                        attrDef("queryId", DataTypes.STRING_TYPE, Multiplicity.REQUIRED),
+                        attrDef("queryGraph", DataTypes.STRING_TYPE, Multiplicity.REQUIRED));
+
+        
+        HierarchicalTypeDefinition<ClassType> viewClsDef = TypesUtil
+                .createClassTypeDef(VIEW_TYPE, null, attrDef("name", DataTypes.STRING_TYPE),
+                        new AttributeDefinition("db", DATABASE_TYPE, Multiplicity.REQUIRED, false, null),
+                        new AttributeDefinition("inputTables", DataTypes.arrayTypeName(TABLE_TYPE),
+                                Multiplicity.COLLECTION, false, null));
+
+        
+        HierarchicalTypeDefinition<TraitType> dimTraitDef = TypesUtil.createTraitTypeDef("Dimension", null);
+
+        
+        HierarchicalTypeDefinition<TraitType> factTraitDef = TypesUtil.createTraitTypeDef("Fact", null);
+
+        
+        HierarchicalTypeDefinition<TraitType> piiTraitDef = TypesUtil.createTraitTypeDef("PII", null);
+
+        
+        HierarchicalTypeDefinition<TraitType> metricTraitDef = TypesUtil.createTraitTypeDef("Metric", null);
+
+        
+        HierarchicalTypeDefinition<TraitType> etlTraitDef = TypesUtil.createTraitTypeDef("ETL", null);
+
+        HierarchicalTypeDefinition<TraitType> jdbcTraitDef = TypesUtil.createTraitTypeDef("JdbcAccess", null);
+
+        return TypeUtils.getTypesDef(ImmutableList.<EnumTypeDefinition>of(), ImmutableList.<StructTypeDefinition>of(),
+                ImmutableList.of(dimTraitDef, factTraitDef, piiTraitDef, metricTraitDef, etlTraitDef, jdbcTraitDef),
+                ImmutableList.of(dbClsDef, storageDescClsDef, tblClsDef, loadProcessClsDef, viewClsDef));
+    }
+
+    AttributeDefinition attrDef(String name, IDataType dT) {
+        return attrDef(name, dT, Multiplicity.OPTIONAL, false, null);
+    }
+
+    AttributeDefinition attrDef(String name, IDataType dT, Multiplicity m) {
+        return attrDef(name, dT, m, false, null);
+    }
+
+    AttributeDefinition attrDef(String name, IDataType dT, Multiplicity m, boolean isComposite,
+            String reverseAttributeName) {
+        Preconditions.checkNotNull(name);
+        Preconditions.checkNotNull(dT);
+        return new AttributeDefinition(name, dT.getName(), m, isComposite, reverseAttributeName);
+    }
+
+    
+    
+    void createMysqlEntities(String databasename, String tablename, String tablename2) throws Exception {
+        
+    	Id sourceDB = database(databasename, "MySQL Database", "Oracle", "mysql -u root");
+
+
+        Referenceable sd =
+                rawStorageDescriptor("hdfs://host:8000/apps/warehouse/products", "TextInputFormat", "TextOutputFormat",
+                        true);
+
+        List<Referenceable> driversColumns = ImmutableList
+                .of(rawColumn("model_id", "String", "model_id"), rawColumn("model_name", "String", "model name"),
+                        rawColumn("max_speed", "String", "maximum speed", "Red"),
+                        rawColumn("torque", "String", "torque"),
+                        rawColumn("engine_type", "String", "engine diesel/gas"),
+                        rawColumn("tow_capacity", "String", "towing capacity"),
+                        rawColumn("model_year", "String", "model_year"));
+
+        
+        Id trucks = table(tablename, "mysql table", sourceDB, sd, "Ford Jr", "Managed", driversColumns, "Trucks");
+
+        Id Motorcycle = table(tablename2, "mysql table", sourceDB, sd, "Ford Jr", "Managed", driversColumns, "Trucks");
+       
+       ;
+
+    }
+
+    private Id createInstance(Referenceable referenceable) throws Exception {
+        String typeName = referenceable.getTypeName();
+
+        String entityJSON = InstanceSerialization.toJson(referenceable, true);
+        System.out.println("Submitting new entity= " + entityJSON);
+        JSONObject jsonObject = metadataServiceClient.createEntity(entityJSON);
+        String guid = jsonObject.getString(AtlasClient.GUID);
+        System.out.println("created instance for type " + typeName + ", guid: " + guid);
+
+        // return the Id for created instance with guid
+        return new Id(guid, referenceable.getId().getVersion(), referenceable.getTypeName());
+    }
+
+    
+    
+    Id database(String name, String description, String owner, String locationUri, String... traitNames)
+    throws Exception {
+        Referenceable referenceable = new Referenceable(DATABASE_TYPE, traitNames);
+        referenceable.set("name", name);
+        referenceable.set("description", description);
+        referenceable.set("owner", owner);
+        referenceable.set("locationUri", locationUri);
+        referenceable.set("createTime", System.currentTimeMillis());
+
+        return createInstance(referenceable);
+    }
+
+    
+    
+    Referenceable rawStorageDescriptor(String location, String inputFormat, String outputFormat, boolean compressed)
+    throws Exception {
+        Referenceable referenceable = new Referenceable(STORAGE_DESC_TYPE);
+        referenceable.set("location", location);
+        referenceable.set("inputFormat", inputFormat);
+        referenceable.set("outputFormat", outputFormat);
+        referenceable.set("compressed", compressed);
+
+        return referenceable;
+    }
+
+    
+    
+    Referenceable rawColumn(String name, String dataType, String comment, String... traitNames) throws Exception {
+        Referenceable referenceable = new Referenceable(COLUMN_TYPE, traitNames);
+        referenceable.set("name", name);
+        referenceable.set("dataType", dataType);
+        referenceable.set("comment", comment);
+
+        return referenceable;
+    }
+
+    Id table(String name, String description, Id dbId, Referenceable sd, String owner, String tableType,
+            List<Referenceable> columns, String... traitNames) throws Exception {
+        Referenceable referenceable = new Referenceable(TABLE_TYPE, traitNames);
+        referenceable.set("name", name);
+        referenceable.set("description", description);
+        referenceable.set("owner", owner);
+        referenceable.set("tableType", tableType);
+        referenceable.set("createTime", System.currentTimeMillis());
+        referenceable.set("lastAccessTime", System.currentTimeMillis());
+        referenceable.set("retention", System.currentTimeMillis());
+        referenceable.set("db", dbId);
+        referenceable.set("sd", sd);
+        referenceable.set("columns", columns);
+
+        return createInstance(referenceable);
+    }
+
+    Id loadProcess(String name, String description, String user, List<Id> inputTables, List<Id> outputTables,
+            String queryText, String queryPlan, String queryId, String queryGraph, String... traitNames)
+    throws Exception {
+        Referenceable referenceable = new Referenceable(LOAD_PROCESS_TYPE, traitNames);
+        // super type attributes
+        referenceable.set("name", name);
+        referenceable.set("description", description);
+        referenceable.set("inputs", inputTables);
+        referenceable.set("outputs", outputTables);
+
+        referenceable.set("user", user);
+        referenceable.set("startTime", System.currentTimeMillis());
+        referenceable.set("endTime", System.currentTimeMillis() + 10000);
+
+        referenceable.set("queryText", queryText);
+        referenceable.set("queryPlan", queryPlan);
+        referenceable.set("queryId", queryId);
+        referenceable.set("queryGraph", queryGraph);
+
+        return createInstance(referenceable);
+    }
+
+    Id view(String name, Id dbId, List<Id> inputTables, String... traitNames) throws Exception {
+        Referenceable referenceable = new Referenceable(VIEW_TYPE, traitNames);
+        referenceable.set("name", name);
+        referenceable.set("db", dbId);
+
+        referenceable.set("inputTables", inputTables);
+
+        return createInstance(referenceable);
+    }
+
+    private void verifyTypesCreated() throws Exception {
+        List<String> types = metadataServiceClient.listTypes();
+        for (String type : TYPES) {
+            assert types.contains(type);
+        }
+    }
+
+    private String[] getDSLQueries() {
+        return new String[]{"from DB", "DB", "DB where name=\"Reporting\"", "DB where DB.name=\"Reporting\"",
+                "DB name = \"Reporting\"", "DB DB.name = \"Reporting\"",
+                "DB where name=\"Reporting\" select name, owner", "DB where DB.name=\"Reporting\" select name, owner",
+                "DB has name", "DB where DB has name", "DB, Table", "DB is JdbcAccess",
+            /*
+            "DB, hive_process has name",
+            "DB as db1, Table where db1.name = \"Reporting\"",
+            "DB where DB.name=\"Reporting\" and DB.createTime < " + System.currentTimeMillis()},
+            */
+                "from Table", "Table", "Table is Dimension", "Column where Column isa PII", "View is Dimension",
+            /*"Column where Column isa PII select Column.name",*/
+                "Column select Column.name", "Column select name", "Column where Column.name=\"customer_id\"",
+                "from Table select Table.name", "DB where (name = \"Reporting\")",
+                "DB where (name = \"Reporting\") select name as _col_0, owner as _col_1", "DB where DB is JdbcAccess",
+                "DB where DB has name", "DB Table", "DB where DB has name",
+                "DB as db1 Table where (db1.name = \"Reporting\")",
+                "DB where (name = \"Reporting\") select name as _col_0, (createTime + 1) as _col_1 ",
+            /*
+            todo: does not work
+            "DB where (name = \"Reporting\") and ((createTime + 1) > 0)",
+            "DB as db1 Table as tab where ((db1.createTime + 1) > 0) and (db1.name = \"Reporting\") select db1.name
+            as dbName, tab.name as tabName",
+            "DB as db1 Table as tab where ((db1.createTime + 1) > 0) or (db1.name = \"Reporting\") select db1.name as
+             dbName, tab.name as tabName",
+            "DB as db1 Table as tab where ((db1.createTime + 1) > 0) and (db1.name = \"Reporting\") or db1 has owner
+            select db1.name as dbName, tab.name as tabName",
+            "DB as db1 Table as tab where ((db1.createTime + 1) > 0) and (db1.name = \"Reporting\") or db1 has owner
+            select db1.name as dbName, tab.name as tabName",
+            */
+                // trait searches
+                "Dimension",
+            /*"Fact", - todo: does not work*/
+                "JdbcAccess", "ETL", "Metric", "PII",
+            /*
+            // Lineage - todo - fix this, its not working
+            "Table hive_process outputTables",
+            "Table loop (hive_process outputTables)",
+            "Table as _loop0 loop (hive_process outputTables) withPath",
+            "Table as src loop (hive_process outputTables) as dest select src.name as srcTable, dest.name as
+            destTable withPath",
+            */
+                "Table where name=\"sales_fact\", columns",
+                "Table where name=\"sales_fact\", columns as column select column.name, column.dataType, column"
+                        + ".comment",
+                "from DataSet", "from Process",};
+    }
+
+    private void search() throws Exception {
+        for (String dslQuery : getDSLQueries()) {
+            JSONObject response = metadataServiceClient.searchEntity(dslQuery);
+            JSONObject results = response.getJSONObject(AtlasClient.RESULTS);
+            if (!results.isNull("rows")) {
+                JSONArray rows = results.getJSONArray("rows");
+                System.out.println("query [" + dslQuery + "] returned [" + rows.length() + "] rows");
+            } else {
+                System.out.println("query [" + dslQuery + "] failed, results:" + results.toString());
+            }
+        }
+    }
+}
diff --git a/codesamples/atlas/src/main/java/com/hortonworks/atlas/trash/mysqlTypeCreator.java b/codesamples/atlas/src/main/java/com/hortonworks/atlas/trash/mysqlTypeCreator.java
new file mode 100644
index 0000000..94d4a10
--- /dev/null
+++ b/codesamples/atlas/src/main/java/com/hortonworks/atlas/trash/mysqlTypeCreator.java
@@ -0,0 +1,391 @@
+package com.hortonworks.atlas.trash;
+
+
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.ImmutableList;
+
+import org.apache.atlas.AtlasClient;
+import org.apache.atlas.typesystem.Referenceable;
+import org.apache.atlas.typesystem.TypesDef;
+import org.apache.atlas.typesystem.json.InstanceSerialization;
+import org.apache.atlas.typesystem.json.TypesSerialization;
+import org.apache.atlas.typesystem.persistence.Id;
+import org.apache.atlas.typesystem.types.AttributeDefinition;
+import org.apache.atlas.typesystem.types.ClassType;
+import org.apache.atlas.typesystem.types.DataTypes;
+import org.apache.atlas.typesystem.types.EnumTypeDefinition;
+import org.apache.atlas.typesystem.types.HierarchicalTypeDefinition;
+import org.apache.atlas.typesystem.types.IDataType;
+import org.apache.atlas.typesystem.types.Multiplicity;
+import org.apache.atlas.typesystem.types.StructTypeDefinition;
+import org.apache.atlas.typesystem.types.TraitType;
+import org.apache.atlas.typesystem.types.TypeUtils;
+import org.apache.atlas.typesystem.types.utils.TypesUtil;
+import org.codehaus.jettison.json.JSONArray;
+import org.codehaus.jettison.json.JSONObject;
+
+import java.util.List;
+
+/**
+ * A driver that sets up sample types and data for testing purposes.
+ * Please take a look at QueryDSL in docs for the Meta Model.
+ * todo - move this to examples module.
+ */
+public class mysqlTypeCreator {
+
+	{
+		System.setProperty("atlas.conf", "/Users/sdutta/Applications/conf");
+	}
+	
+
+    private static final String DATABASE_TYPE = "DB";
+    private static final String COLUMN_TYPE = "Column";
+    private static final String TABLE_TYPE = "Table";
+    private static final String VIEW_TYPE = "View";
+    private static final String LOAD_PROCESS_TYPE = "LoadProcess";
+    private static final String STORAGE_DESC_TYPE = "StorageDesc";
+    
+
+    private static final String[] TYPES =
+            {DATABASE_TYPE, TABLE_TYPE, STORAGE_DESC_TYPE, COLUMN_TYPE, LOAD_PROCESS_TYPE, VIEW_TYPE, "JdbcAccess",
+                    "ETL", "Metric", "PII", "Fact", "Dimension"};
+
+    private final AtlasClient metadataServiceClient;
+
+
+	/**
+	 * 
+	 * @param args
+	 * @throws Exception
+	 */
+    public static void main(String[] args) throws Exception {
+        
+    	
+    	String baseUrl = args[0];
+    	String databasename = args[1];
+    	String tablename = args[2];
+    	String tablename2 = args[3];
+    	String flag = args[4];
+    	
+        mysqlTypeCreator quickStart = new mysqlTypeCreator(baseUrl);
+
+        // Shows how to create types in Atlas for your meta model
+        
+        if("createtype".equalsIgnoreCase(flag))
+        		quickStart.createTypes();
+
+        // Shows how to create entities (instances) for the added types in Atlas
+        quickStart.createMysqlEntities(databasename, tablename, tablename2);
+
+        // Shows some search queries using DSL based on types
+        if("search".equalsIgnoreCase(flag))
+        	quickStart.search();
+    }
+
+    
+    mysqlTypeCreator(String baseUrl) {
+    	
+    	
+        metadataServiceClient = new AtlasClient(baseUrl);
+    }
+
+
+    void createTypes() throws Exception {
+        TypesDef typesDef = createTypeDefinitions();
+
+        String typesAsJSON = TypesSerialization.toJson(typesDef);
+        System.out.println("typesAsJSON = " + typesAsJSON);
+        metadataServiceClient.createType(typesAsJSON);
+
+        // verify types created
+        verifyTypesCreated();
+    }
+
+    
+    
+    
+    TypesDef createTypeDefinitions() throws Exception {
+    
+    	HierarchicalTypeDefinition<ClassType> dbClsDef = TypesUtil
+                .createClassTypeDef(DATABASE_TYPE, null, attrDef("name", DataTypes.STRING_TYPE),
+                        attrDef("description", DataTypes.STRING_TYPE), attrDef("locationUri", DataTypes.STRING_TYPE),
+                        attrDef("owner", DataTypes.STRING_TYPE), attrDef("createTime", DataTypes.INT_TYPE));
+
+        HierarchicalTypeDefinition<ClassType> storageDescClsDef = TypesUtil
+                .createClassTypeDef(STORAGE_DESC_TYPE, null, attrDef("location", DataTypes.STRING_TYPE),
+                        attrDef("inputFormat", DataTypes.STRING_TYPE), attrDef("outputFormat", DataTypes.STRING_TYPE),
+                        attrDef("compressed", DataTypes.STRING_TYPE, Multiplicity.REQUIRED, false, null));
+
+        HierarchicalTypeDefinition<ClassType> columnClsDef = TypesUtil
+                .createClassTypeDef(COLUMN_TYPE, null, attrDef("name", DataTypes.STRING_TYPE),
+                        attrDef("dataType", DataTypes.STRING_TYPE), attrDef("comment", DataTypes.STRING_TYPE));
+
+        
+        HierarchicalTypeDefinition<ClassType> tblClsDef = TypesUtil
+                .createClassTypeDef(TABLE_TYPE, ImmutableList.of("DataSet"),
+                        new AttributeDefinition("db", DATABASE_TYPE, Multiplicity.REQUIRED, false, null),
+                        new AttributeDefinition("sd", STORAGE_DESC_TYPE, Multiplicity.REQUIRED, true, null),
+                        attrDef("owner", DataTypes.STRING_TYPE), attrDef("createTime", DataTypes.INT_TYPE),
+                        attrDef("lastAccessTime", DataTypes.INT_TYPE), attrDef("retention", DataTypes.INT_TYPE),
+                        attrDef("viewOriginalText", DataTypes.STRING_TYPE),
+                        attrDef("viewExpandedText", DataTypes.STRING_TYPE), attrDef("tableType", DataTypes.STRING_TYPE),
+                        attrDef("temporary", DataTypes.BOOLEAN_TYPE),
+                        new AttributeDefinition("columns", DataTypes.arrayTypeName(COLUMN_TYPE),
+                                Multiplicity.COLLECTION, true, null));
+
+        
+        HierarchicalTypeDefinition<ClassType> loadProcessClsDef = TypesUtil
+                .createClassTypeDef(LOAD_PROCESS_TYPE, ImmutableList.of("Process"),
+                        attrDef("userName", DataTypes.STRING_TYPE), attrDef("startTime", DataTypes.INT_TYPE),
+                        attrDef("endTime", DataTypes.INT_TYPE),
+                        attrDef("queryText", DataTypes.STRING_TYPE, Multiplicity.REQUIRED),
+                        attrDef("queryPlan", DataTypes.STRING_TYPE, Multiplicity.REQUIRED),
+                        attrDef("queryId", DataTypes.STRING_TYPE, Multiplicity.REQUIRED),
+                        attrDef("queryGraph", DataTypes.STRING_TYPE, Multiplicity.REQUIRED));
+
+        
+        HierarchicalTypeDefinition<ClassType> viewClsDef = TypesUtil
+                .createClassTypeDef(VIEW_TYPE, null, attrDef("name", DataTypes.STRING_TYPE),
+                        new AttributeDefinition("db", DATABASE_TYPE, Multiplicity.REQUIRED, false, null),
+                        new AttributeDefinition("inputTables", DataTypes.arrayTypeName(TABLE_TYPE),
+                                Multiplicity.COLLECTION, false, null));
+
+        
+        HierarchicalTypeDefinition<TraitType> dimTraitDef = TypesUtil.createTraitTypeDef("Dimension", null);
+
+        
+        HierarchicalTypeDefinition<TraitType> factTraitDef = TypesUtil.createTraitTypeDef("Fact", null);
+
+        
+        HierarchicalTypeDefinition<TraitType> piiTraitDef = TypesUtil.createTraitTypeDef("PII", null);
+
+        
+        HierarchicalTypeDefinition<TraitType> metricTraitDef = TypesUtil.createTraitTypeDef("Metric", null);
+
+        
+        HierarchicalTypeDefinition<TraitType> etlTraitDef = TypesUtil.createTraitTypeDef("ETL", null);
+
+        HierarchicalTypeDefinition<TraitType> jdbcTraitDef = TypesUtil.createTraitTypeDef("JdbcAccess", null);
+
+        return TypeUtils.getTypesDef(ImmutableList.<EnumTypeDefinition>of(), ImmutableList.<StructTypeDefinition>of(),
+                ImmutableList.of(dimTraitDef, factTraitDef, piiTraitDef, metricTraitDef, etlTraitDef, jdbcTraitDef),
+                ImmutableList.of(dbClsDef, storageDescClsDef, tblClsDef, loadProcessClsDef, viewClsDef));
+    }
+
+    AttributeDefinition attrDef(String name, IDataType dT) {
+        return attrDef(name, dT, Multiplicity.OPTIONAL, false, null);
+    }
+
+    AttributeDefinition attrDef(String name, IDataType dT, Multiplicity m) {
+        return attrDef(name, dT, m, false, null);
+    }
+
+    AttributeDefinition attrDef(String name, IDataType dT, Multiplicity m, boolean isComposite,
+            String reverseAttributeName) {
+        Preconditions.checkNotNull(name);
+        Preconditions.checkNotNull(dT);
+        return new AttributeDefinition(name, dT.getName(), m, isComposite, reverseAttributeName);
+    }
+
+    
+    
+    void createMysqlEntities(String databasename, String tablename, String tablename2) throws Exception {
+        
+    	Id sourceDB = database(databasename, "MySQL Database", "Oracle", "mysql -u root");
+
+
+        Referenceable sd =
+                rawStorageDescriptor("hdfs://host:8000/apps/warehouse/sales", "TextInputFormat", "TextOutputFormat",
+                        true);
+
+        List<Referenceable> driversColumns = ImmutableList
+                .of(rawColumn("driver_id", "String", "driver id"), rawColumn("driver_name", "String", "driver name"),
+                        rawColumn("certified", "String", "Certified", "PII"),
+                        rawColumn("wageplan", "String", "product id", "PII"));
+
+        
+        Id salesFact = table(tablename, "mysql table", sourceDB, sd, "Andrew", "Managed", driversColumns, "Fact");
+
+       
+
+        List<Referenceable> timesheetColumns = ImmutableList.of(rawColumn("driver_id", "int", "driver id", "PII"),
+                rawColumn("driver_week", "int", "week of the year"),
+                rawColumn("hours_logged", "string", "hours logged", "PII"),
+                rawColumn("Miles_logged", "string", "miles logged", "PII"));
+
+       
+        Id timesheetDim =
+                table(tablename2, "demo table", sourceDB, sd, "fetl", "Oracle", timesheetColumns,
+                        "Dimension");
+
+    }
+
+    private Id createInstance(Referenceable referenceable) throws Exception {
+        String typeName = referenceable.getTypeName();
+
+        String entityJSON = InstanceSerialization.toJson(referenceable, true);
+        System.out.println("Submitting new entity= " + entityJSON);
+        JSONObject jsonObject = metadataServiceClient.createEntity(entityJSON);
+        String guid = jsonObject.getString(AtlasClient.GUID);
+        System.out.println("created instance for type " + typeName + ", guid: " + guid);
+
+        // return the Id for created instance with guid
+        return new Id(guid, referenceable.getId().getVersion(), referenceable.getTypeName());
+    }
+
+    
+    
+    Id database(String name, String description, String owner, String locationUri, String... traitNames)
+    throws Exception {
+        Referenceable referenceable = new Referenceable(DATABASE_TYPE, traitNames);
+        referenceable.set("name", name);
+        referenceable.set("description", description);
+        referenceable.set("owner", owner);
+        referenceable.set("locationUri", locationUri);
+        referenceable.set("createTime", System.currentTimeMillis());
+
+        return createInstance(referenceable);
+    }
+
+    
+    
+    Referenceable rawStorageDescriptor(String location, String inputFormat, String outputFormat, boolean compressed)
+    throws Exception {
+        Referenceable referenceable = new Referenceable(STORAGE_DESC_TYPE);
+        referenceable.set("location", location);
+        referenceable.set("inputFormat", inputFormat);
+        referenceable.set("outputFormat", outputFormat);
+        referenceable.set("compressed", compressed);
+
+        return referenceable;
+    }
+
+    
+    
+    Referenceable rawColumn(String name, String dataType, String comment, String... traitNames) throws Exception {
+        Referenceable referenceable = new Referenceable(COLUMN_TYPE, traitNames);
+        referenceable.set("name", name);
+        referenceable.set("dataType", dataType);
+        referenceable.set("comment", comment);
+
+        return referenceable;
+    }
+
+    Id table(String name, String description, Id dbId, Referenceable sd, String owner, String tableType,
+            List<Referenceable> columns, String... traitNames) throws Exception {
+        Referenceable referenceable = new Referenceable(TABLE_TYPE, traitNames);
+        referenceable.set("name", name);
+        referenceable.set("description", description);
+        referenceable.set("owner", owner);
+        referenceable.set("tableType", tableType);
+        referenceable.set("createTime", System.currentTimeMillis());
+        referenceable.set("lastAccessTime", System.currentTimeMillis());
+        referenceable.set("retention", System.currentTimeMillis());
+        referenceable.set("db", dbId);
+        referenceable.set("sd", sd);
+        referenceable.set("columns", columns);
+
+        return createInstance(referenceable);
+    }
+
+    Id loadProcess(String name, String description, String user, List<Id> inputTables, List<Id> outputTables,
+            String queryText, String queryPlan, String queryId, String queryGraph, String... traitNames)
+    throws Exception {
+        Referenceable referenceable = new Referenceable(LOAD_PROCESS_TYPE, traitNames);
+        // super type attributes
+        referenceable.set("name", name);
+        referenceable.set("description", description);
+        referenceable.set("inputs", inputTables);
+        referenceable.set("outputs", outputTables);
+
+        referenceable.set("user", user);
+        referenceable.set("startTime", System.currentTimeMillis());
+        referenceable.set("endTime", System.currentTimeMillis() + 10000);
+
+        referenceable.set("queryText", queryText);
+        referenceable.set("queryPlan", queryPlan);
+        referenceable.set("queryId", queryId);
+        referenceable.set("queryGraph", queryGraph);
+
+        return createInstance(referenceable);
+    }
+
+    Id view(String name, Id dbId, List<Id> inputTables, String... traitNames) throws Exception {
+        Referenceable referenceable = new Referenceable(VIEW_TYPE, traitNames);
+        referenceable.set("name", name);
+        referenceable.set("db", dbId);
+
+        referenceable.set("inputTables", inputTables);
+
+        return createInstance(referenceable);
+    }
+
+    private void verifyTypesCreated() throws Exception {
+        List<String> types = metadataServiceClient.listTypes();
+        for (String type : TYPES) {
+            assert types.contains(type);
+        }
+    }
+
+    private String[] getDSLQueries() {
+        return new String[]{"from DB", "DB", "DB where name=\"Reporting\"", "DB where DB.name=\"Reporting\"",
+                "DB name = \"Reporting\"", "DB DB.name = \"Reporting\"",
+                "DB where name=\"Reporting\" select name, owner", "DB where DB.name=\"Reporting\" select name, owner",
+                "DB has name", "DB where DB has name", "DB, Table", "DB is JdbcAccess",
+            /*
+            "DB, hive_process has name",
+            "DB as db1, Table where db1.name = \"Reporting\"",
+            "DB where DB.name=\"Reporting\" and DB.createTime < " + System.currentTimeMillis()},
+            */
+                "from Table", "Table", "Table is Dimension", "Column where Column isa PII", "View is Dimension",
+            /*"Column where Column isa PII select Column.name",*/
+                "Column select Column.name", "Column select name", "Column where Column.name=\"customer_id\"",
+                "from Table select Table.name", "DB where (name = \"Reporting\")",
+                "DB where (name = \"Reporting\") select name as _col_0, owner as _col_1", "DB where DB is JdbcAccess",
+                "DB where DB has name", "DB Table", "DB where DB has name",
+                "DB as db1 Table where (db1.name = \"Reporting\")",
+                "DB where (name = \"Reporting\") select name as _col_0, (createTime + 1) as _col_1 ",
+            /*
+            todo: does not work
+            "DB where (name = \"Reporting\") and ((createTime + 1) > 0)",
+            "DB as db1 Table as tab where ((db1.createTime + 1) > 0) and (db1.name = \"Reporting\") select db1.name
+            as dbName, tab.name as tabName",
+            "DB as db1 Table as tab where ((db1.createTime + 1) > 0) or (db1.name = \"Reporting\") select db1.name as
+             dbName, tab.name as tabName",
+            "DB as db1 Table as tab where ((db1.createTime + 1) > 0) and (db1.name = \"Reporting\") or db1 has owner
+            select db1.name as dbName, tab.name as tabName",
+            "DB as db1 Table as tab where ((db1.createTime + 1) > 0) and (db1.name = \"Reporting\") or db1 has owner
+            select db1.name as dbName, tab.name as tabName",
+            */
+                // trait searches
+                "Dimension",
+            /*"Fact", - todo: does not work*/
+                "JdbcAccess", "ETL", "Metric", "PII",
+            /*
+            // Lineage - todo - fix this, its not working
+            "Table hive_process outputTables",
+            "Table loop (hive_process outputTables)",
+            "Table as _loop0 loop (hive_process outputTables) withPath",
+            "Table as src loop (hive_process outputTables) as dest select src.name as srcTable, dest.name as
+            destTable withPath",
+            */
+                "Table where name=\"sales_fact\", columns",
+                "Table where name=\"sales_fact\", columns as column select column.name, column.dataType, column"
+                        + ".comment",
+                "from DataSet", "from Process",};
+    }
+
+    private void search() throws Exception {
+        for (String dslQuery : getDSLQueries()) {
+            JSONObject response = metadataServiceClient.searchEntity(dslQuery);
+            JSONObject results = response.getJSONObject(AtlasClient.RESULTS);
+            if (!results.isNull("rows")) {
+                JSONArray rows = results.getJSONArray("rows");
+                System.out.println("query [" + dslQuery + "] returned [" + rows.length() + "] rows");
+            } else {
+                System.out.println("query [" + dslQuery + "] failed, results:" + results.toString());
+            }
+        }
+    }
+}
